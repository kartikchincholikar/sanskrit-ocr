<!DOCTYPE html>
<html>

<head>
    <script src="distill.bundle.js" type="module" fetchpriority="high" blocking></script>
    <script src="main.bundle.js" type="module" fetchpriority="low" defer></script>
    <script src="https://cdn.plot.ly/plotly-3.0.0.min.js" charset="utf-8"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf8">
    <base target="_blank">
    <title>The Ultra-Scale Playbook: Training LLMs on GPU Clusters</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <d-front-matter>
        <script id='distill-front-matter' type="text/json">{
    "title": "The Ultra-Scale Playbook: Training LLMs on GPU Clusters",
    "description": "This blog covers everything about scaling LLMs in 2025.",
    "published": "Feb 19, 2025",
    "affiliation": {"name": "Hugging Face"},
    "authors": [
      {
        "author":"Nouamane Tazi",
        "authorURL":"https://huggingface.co/nouamanetazi"
      },
      {
        "author":"Ferdinand Mom",
        "authorURL":"https://huggingface.co/3outeille"
      },
      {
        "author":"Haojun Zhao",
        "authorURL":"https://huggingface.co/zzhhjjj"
      },
      {
        "author":"Phuc Nguyen",
        "authorURL":"https://huggingface.co/neuralink"
      },
      {
        "author":"Mohamed Mekkouri",
        "authorURL":"https://huggingface.co/medmekk"
      },
      {
        "author":"Leandro Werra",
        "authorURL":"https://huggingface.co/lvwerra"
      },
      {
        "author":"Thomas Wolf",
        "authorURL":"https://huggingface.co/thomwolf"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }
    </script>
    </d-front-matter>
    <d-title>
        <h1 class="l-page" style="text-align: center;">The Ultra-Scale Playbook:<br>Training LLMs on GPU Clusters</h1>
        <div id="title-plot" class="main-plot-container l-screen" style="overflow-x: hidden; width: 100%; text-align: center;">
            <div style="display: flex; justify-content: center; position: relative;">
                <div>                            <div id=be38db4f-289a-4968-9a50-192a9445f9f7 class=plotly-graph-div style="height:675px; width:1200px;"></div>            <script>window.PLOTLYENV=window.PLOTLYENV||{},document.getElementById("be38db4f-289a-4968-9a50-192a9445f9f7")&&Plotly.newPlot("be38db4f-289a-4968-9a50-192a9445f9f7",[{customdata:[[1.34,1,4,16,4,2,1,1,44.32406064372896,38894.51,311156.08,41.76,"GPU GOES BRRRRRR!","meh..."],[3.57,1,2,32,4,2,1,1,46.6837423975016,16426.09,131408.72,56.87,"GPU GOES BRRRRRR!","meh..."],[8.86,1,1,128,2,4,1,1,45.218794551981425,7503.28,60026.24,63.07,"GPU GOES BRRRRRR!","meh..."],[80,16,1,256,1,32,4,0,34.29284351266944,662.14,84753.92,61.18,"GPU GOES BRRRRRR!","moar?"],[1.34,1,2,32,4,2,1,1,43.64555608185969,38299.12,306392.96,26.41,"GPU GOES BRRRRRR!","meh..."],[3.57,1,2,64,2,4,1,1,45.74811165185967,16096.88,128775.04,52.19,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,64,4,2,1,0,43.29871978396266,37994.77,303958.16,26.78,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,64,4,2,1,1,43.07934739248845,37802.27,302418.16,20.57,"GPU GOES BRRRRRR!","meh..."],[8.86,2,4,16,4,1,4,1,43.93628721271018,7290.47,116647.52,58.34,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,64,4,4,1,1,43.93285208661937,7289.9,116638.4,64.34,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,16,16,2,1,1,43.30693595365235,7186.04,229953.28,61.1,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,128,2,4,1,1,42.27617093645149,37097.48,296779.84,18.08,"GPU GOES BRRRRRR!","meh..."],[1.34,1,2,64,2,4,1,1,42.2541083416518,37078.12,296624.96,26.16,"GPU GOES BRRRRRR!","meh..."],[8.86,1,1,64,4,1,2,1,43.03525966562854,7140.96,57127.68,65.23,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,128,2,4,1,0,42.06221162996171,36909.73,295277.84,20.84,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,64,4,2,1,1,44.242053376805536,15566.96,124535.68,41.46,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,64,4,2,1,0,44.2357440294123,15564.74,124517.92,59.66,"GPU GOES BRRRRRR!","meh..."],[80,16,1,128,2,16,4,1,32.401956817034744,625.63,80080.64,64.63,"GPU GOES BRRRRRR!","moar?"],[8.86,2,2,32,4,2,2,1,42.65426199639904,7077.74,113243.84,64.02,"GPU GOES BRRRRRR!","meh..."],[1.34,1,4,32,2,4,1,1,41.67003077384775,36565.59,292524.72,42.58,"GPU GOES BRRRRRR!","meh..."],[1.34,2,1,32,8,2,1,1,41.5919683903881,36497.09,583953.44,19.45,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,16,8,2,1,1,43.773598543100114,15402.13,246434.08,54.31,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,128,2,4,1,1,43.773115394876314,15401.96,123215.68,36.07,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,32,8,1,2,1,42.39596462051838,7034.88,112558.08,62.2,"GPU GOES BRRRRRR!","meh..."],[1.34,2,2,16,8,2,1,1,41.47515971440831,36394.59,582313.44,24.68,"GPU GOES BRRRRRR!","meh..."],[8.86,1,2,64,2,2,2,1,42.2828465210018,7016.11,56128.88,62.65,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,128,2,4,1,0,43.41947931553417,15277.53,122220.24,42.4,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,32,4,4,1,1,43.172761096071895,15190.72,243051.52,49,"GPU GOES BRRRRRR!","meh..."],[3.57,2,1,32,8,2,1,1,43.086533348364426,15160.38,242566.08,38.92,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,32,8,4,1,1,41.69736845131428,6918.96,221406.72,61.47,"GPU GOES BRRRRRR!","meh..."],[8.86,2,2,32,4,1,4,0,41.36669236604661,6864.09,109825.44,61.91,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,8,16,1,1,1,42.49207000737391,16385.25,262164,64.98,"GPU GOES BRRRRRR!","meh..."],[8.86,2,2,32,4,1,4,1,41.15865630875774,6829.57,109273.12,42.68,"GPU GOES BRRRRRR!","meh..."],[1.34,1,8,16,2,2,2,1,40.25595064042076,35324.73,282597.84,47.77,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,32,8,1,1,1,42.3760711491673,16340.52,130724.16,50.79,"GPU GOES BRRRRRR!","meh..."],[8.86,1,1,256,1,8,1,0,41.02143206123551,6806.8,54454.4,62.45,"GPU GOES BRRRRRR!","meh..."],[1.34,2,4,8,8,2,1,1,40.15729574208349,35238.16,563810.56,41.07,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,16,16,1,2,1,40.88059189151243,6783.43,217069.76,58.09,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,256,1,8,1,0,42.15630249463562,14833.07,118664.56,32.66,"GPU GOES BRRRRRR!","meh..."],[8.86,1,1,128,2,2,2,0,40.77573014768779,6766.03,54128.24,63.61,"GPU GOES BRRRRRR!","meh..."],[1.34,1,4,32,2,2,2,1,39.957262308964175,35062.63,280501.04,27.7,"GPU GOES BRRRRRR!","meh..."],[1.34,2,1,64,4,4,1,1,39.92227668601218,35031.93,560510.88,16.79,"GPU GOES BRRRRRR!","meh..."],[8.86,4,2,16,8,2,2,1,40.71944229209458,6756.69,216214.08,62.15,"GPU GOES BRRRRRR!","meh..."],[3.57,2,1,64,4,4,1,1,42.01809368214339,14784.44,236551.04,33.62,"GPU GOES BRRRRRR!","meh..."],[8.86,1,1,128,2,2,2,1,40.69846994332965,6753.21,54025.68,53.71,"GPU GOES BRRRRRR!","meh..."],[8.86,4,4,8,8,1,4,1,40.62621176468268,6741.22,215719.04,54.55,"GPU GOES BRRRRRR!","meh..."],[80,32,1,64,4,16,4,1,30.749826288665886,593.73,151994.88,64.21,"GPU GOES BRRRRRR!","MOARRR!!!!"],[1.34,2,2,32,4,4,1,1,39.67677333902366,34816.5,557064,23.38,"GPU GOES BRRRRRR!","meh..."],[80,8,1,256,1,16,4,0,30.648189207912317,664.99,42559.36,63.07,"GPU GOES BRRRRRR!","meh..."],[3.57,1,4,32,2,2,2,1,41.68153831353697,14666.02,117328.16,56.55,"GPU GOES BRRRRRR!","meh..."],[80,32,1,128,2,32,4,1,30.602740059807967,590.89,151267.84,56.9,"GPU GOES BRRRRRR!","MOARRR!!!!"],[8.86,1,1,256,1,4,2,0,40.31222918690893,6689.12,53512.96,40.91,"GPU GOES BRRRRRR!","meh..."],[3.57,1,2,16,8,1,1,1,41.48474971912904,15996.82,127974.56,69.78,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,64,2,8,1,1,41.45735753769105,14587.14,233394.24,46.04,"GPU GOES BRRRRRR!","meh..."],[1.34,1,8,32,1,4,2,0,39.28067226978414,34468.92,275751.36,49.76,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,128,2,8,1,1,40.06538223133094,6648.16,106370.56,54.37,"GPU GOES BRRRRRR!","meh..."],[3.57,2,1,64,4,4,1,0,41.34847866443221,14548.83,232781.28,42.36,"GPU GOES BRRRRRR!","meh..."],[1.34,1,2,16,8,1,1,0,39.17811145739493,40149.94,321199.52,50.18,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,128,2,8,1,0,39.7876191409357,6602.07,105633.12,65.87,"GPU GOES BRRRRRR!","meh..."],[8.86,4,2,32,4,4,2,1,39.77146802177191,6599.39,211180.48,50.59,"GPU GOES BRRRRRR!","meh..."],[3.57,2,1,16,16,1,1,1,40.96250849253287,15795.44,252727.04,49.15,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,8,32,2,1,1,39.504191105759126,6555.04,419522.56,59.09,"GPU GOES BRRRRRR!","moar?"],[8.86,8,2,4,32,1,2,1,39.40854996354666,6539.17,418506.88,63.01,"GPU GOES BRRRRRR!","moar?"],[3.57,2,1,128,2,8,1,1,40.565835382834216,14273.45,228375.2,29.78,"GPU GOES BRRRRRR!","meh..."],[8.86,2,4,32,2,2,4,1,39.185628359864296,6502.18,104034.88,54.74,"GPU GOES BRRRRRR!","meh..."],[1.34,4,1,16,16,2,1,1,38.3782825129589,33677.07,1077666.24,16.39,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,16,16,4,1,1,39.134704122202336,6493.73,415598.72,54.19,"GPU GOES BRRRRRR!","moar?"],[3.57,4,2,8,16,2,1,1,40.38650213035098,14210.35,454731.2,53.33,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,32,8,1,1,0,38.32279784956014,39273.41,314187.28,36.67,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,64,4,2,2,1,39.03821926480968,6477.72,103643.52,47.71,"GPU GOES BRRRRRR!","meh..."],[3.57,4,1,16,16,2,1,1,40.302576441827526,14180.82,453786.24,37.9,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,128,2,4,2,1,38.9084679231692,6456.19,103299.04,37.36,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,32,8,1,1,1,38.061499459424034,39005.63,312045.04,24.24,"GPU GOES BRRRRRR!","meh..."],[3.57,2,1,128,2,8,1,0,40.02243573347138,14082.25,225316,34.29,"GPU GOES BRRRRRR!","meh..."],[1.34,2,4,16,4,4,1,1,37.97599343288454,33324.06,533184.96,39.2,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,128,2,4,2,0,38.74171364434003,6428.52,102856.32,42.71,"GPU GOES BRRRRRR!","meh..."],[3.57,2,4,16,4,2,2,1,39.97940712106891,14067.11,225073.76,54.01,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,32,8,2,2,1,38.70862795620226,6423.03,205536.96,41.31,"GPU GOES BRRRRRR!","meh..."],[8.86,2,8,16,2,1,8,0,38.67066077309334,6416.73,102667.68,66.54,"GPU GOES BRRRRRR!","meh..."],[1.34,2,2,8,16,1,1,1,37.899702887355666,38839.82,621437.12,35.03,"GPU GOES BRRRRRR!","meh..."],[3.57,4,2,4,32,1,1,1,39.90301076124939,15386.89,492380.48,62.9,"GPU GOES BRRRRRR!","meh..."],[3.57,1,8,32,1,2,4,0,39.76278619390136,13990.89,111927.12,56.76,"GPU GOES BRRRRRR!","meh..."],[80,16,1,256,1,32,4,0,29.122669932762467,631.89,80881.92,51.66,"GPU goes brr","moar?"],[8.86,2,8,16,2,1,8,1,38.266701997888454,6349.7,101595.2,57.43,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,8,32,1,2,1,38.18877887446015,6336.77,405553.28,55.48,"GPU GOES BRRRRRR!","moar?"],[1.34,4,2,8,16,2,1,1,37.42904393008948,32844.11,1051011.52,24.46,"GPU GOES BRRRRRR!","meh..."],[1.34,1,16,16,1,2,4,0,37.39468508539884,32813.96,262511.68,56.08,"GPU GOES BRRRRRR!","meh..."],[1.34,2,4,16,4,2,2,1,37.34701574963071,32772.13,524354.08,25.49,"GPU GOES BRRRRRR!","meh..."],[3.57,1,2,64,2,2,2,1,39.21773815648084,13799.11,110392.88,34.01,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,64,4,8,1,1,37.98490112770226,6302.94,201694.08,54.19,"GPU GOES BRRRRRR!","meh..."],[1.34,1,8,32,1,2,4,0,37.19698782579912,32640.48,261123.84,30.43,"GPU GOES BRRRRRR!","meh..."],[8.86,4,4,16,4,2,4,1,37.86635914488442,6283.27,201064.64,48.35,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,64,4,4,2,1,37.79922352268865,6272.13,200708.16,33.06,"GPU GOES BRRRRRR!","meh..."],[3.57,4,1,8,32,1,1,1,39.014697573453915,15044.35,481419.2,48.1,"GPU GOES BRRRRRR!","meh..."],[8.86,1,1,128,2,1,4,0,37.77210410618227,6267.63,50141.04,51.35,"GPU GOES BRRRRRR!","meh..."],[1.34,2,8,8,4,2,2,1,36.936202485201925,32411.64,518586.24,46.38,"GPU GOES BRRRRRR!","meh..."],[3.57,4,1,32,8,4,1,1,38.83815417547195,13665.55,437297.6,29.68,"GPU GOES BRRRRRR!","meh..."],[3.57,2,4,32,2,4,2,1,38.83786997063442,13665.45,218647.2,49.36,"GPU GOES BRRRRRR!","meh..."],[8.86,8,2,8,16,2,2,1,37.60462664291294,6239.84,399349.76,60.25,"GPU GOES BRRRRRR!","moar?"],[8.86,1,1,128,2,1,4,1,37.594562326120574,6238.17,49905.36,36.08,"GPU GOES BRRRRRR!","meh..."],[1.34,2,1,16,16,1,1,1,36.785549892832165,37698.03,603168.48,23.96,"GPU GOES BRRRRRR!","meh..."],[1.34,1,8,8,4,1,2,0,36.71736133691794,37628.15,301025.2,68.32,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,64,4,1,4,0,37.45456587157769,6214.94,99439.04,51.47,"GPU GOES BRRRRRR!","meh..."],[1.34,2,1,16,16,1,1,0,36.69934816087076,37609.69,601755.04,36.66,"GPU GOES BRRRRRR!","meh..."],[3.57,4,2,16,8,4,1,1,38.543035872177825,13561.71,433974.72,45.07,"GPU GOES BRRRRRR!","meh..."],[1.34,4,1,32,8,4,1,1,36.58729212105823,32105.47,1027375.04,15.09,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,64,4,1,4,1,37.30125077026168,6189.5,99032,33.15,"GPU GOES BRRRRRR!","meh..."],[80,32,1,32,8,8,4,1,28.251432035107463,545.49,139645.44,62.84,"GPU goes brr","MOAR!"],[1.34,2,4,32,2,4,2,1,36.39129286074836,31933.48,510935.68,25.62,"GPU GOES BRRRRRR!","meh..."],[1.34,2,8,4,8,1,2,1,36.259830568499765,37159.27,594548.32,58.73,"GPU GOES BRRRRRR!","meh..."],[8.86,4,4,32,2,4,4,1,36.95345931990999,6131.79,196217.28,45.26,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,32,4,2,2,1,38.0411869700623,13385.13,214162.08,34.46,"GPU GOES BRRRRRR!","meh..."],[1.34,4,2,4,32,1,1,1,36.09462847181577,36989.97,1183679.04,34.86,"GPU GOES BRRRRRR!","meh..."],[3.57,2,8,8,4,1,4,0,37.99928615090087,14652.8,234444.8,66.53,"GPU GOES BRRRRRR!","meh..."],[8.86,8,2,16,8,4,2,1,36.68642346537726,6087.48,389598.72,48.61,"GPU GOES BRRRRRR!","meh..."],[3.57,4,1,64,4,8,1,1,37.7696008273159,13289.57,425266.24,28.21,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,128,2,2,2,0,37.76388831008149,13287.56,106300.48,32.28,"GPU GOES BRRRRRR!","meh..."],[3.57,1,2,32,4,1,2,1,37.68474330467061,14531.51,116252.08,39.84,"GPU GOES BRRRRRR!","meh..."],[8.86,2,4,32,2,1,8,0,36.50062532962362,6056.65,96906.4,44.64,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,64,2,4,2,1,37.51120178903102,13198.65,211178.4,30.92,"GPU GOES BRRRRRR!","meh..."],[8.86,16,8,2,16,1,8,0,36.32953194415343,6028.26,771617.28,66.88,"GPU GOES BRRRRRR!","moar?"],[3.57,2,8,8,4,1,4,1,37.467345936384,14447.68,231162.88,58.48,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,32,8,1,4,1,36.267518878408865,6017.97,192575.04,31.74,"GPU GOES BRRRRRR!","meh..."],[8.86,8,4,4,16,1,4,1,36.20610846636444,6007.78,384497.92,54.12,"GPU GOES BRRRRRR!","meh..."],[3.57,4,2,32,4,8,1,1,37.25337116042121,13107.93,419453.76,42.1,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,256,1,4,2,0,37.23145896744742,13100.22,104801.76,22.21,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,32,8,1,4,0,36.05375761096865,5982.5,191440,51.24,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,32,8,4,2,1,36.05128673079808,5982.09,382853.76,32.3,"GPU GOES BRRRRRR!","meh..."],[8.86,2,4,32,2,1,8,1,35.93039439854968,5962.03,95392.48,35.96,"GPU GOES BRRRRRR!","meh..."],[1.34,2,8,16,2,4,2,1,35.197439816424705,30885.87,494173.92,45.88,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,16,16,2,2,1,35.87193698963595,5952.33,380949.12,38.26,"GPU GOES BRRRRRR!","meh..."],[1.34,4,1,8,32,1,1,1,35.08068962572373,35950.88,1150428.16,23.79,"GPU GOES BRRRRRR!","meh..."],[1.34,4,4,4,16,2,1,1,34.98287652361034,30697.59,982322.88,41.08,"GPU GOES BRRRRRR!","meh..."],[3.57,4,4,8,8,2,2,1,36.806999042592096,12950.87,414427.84,52.17,"GPU GOES BRRRRRR!","meh..."],[8.86,4,2,64,2,8,2,1,35.64232592988201,5914.23,189255.36,46.9,"GPU GOES BRRRRRR!","meh..."],[1.34,4,2,16,8,4,1,1,34.93677983147978,30657.14,981028.48,22.06,"GPU GOES BRRRRRR!","meh..."],[8.86,1,8,32,1,1,8,0,35.609300507114256,5908.75,47270,60.09,"GPU GOES BRRRRRR!","meh..."],[8.86,2,1,256,1,8,2,0,35.49377179279712,5889.58,94233.28,33.55,"GPU GOES BRRRRRR!","meh..."],[8.86,1,1,256,1,2,4,0,35.45604567116826,5883.32,47066.56,30.75,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,128,2,2,2,1,36.57253005162749,12868.37,102946.96,24.96,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,16,8,1,2,0,36.43258840423513,14048.67,224778.72,57.72,"GPU GOES BRRRRRR!","meh..."],[1.34,4,1,8,32,1,1,0,34.51153766510424,35367.61,1131763.52,36.66,"GPU GOES BRRRRRR!","meh..."],[80,32,1,128,2,32,4,0,26.654186175148393,578.33,148052.48,53.2,"GPU goes brr","MOAR!"],[8.86,8,4,8,8,2,4,1,35.107410505636366,5825.47,372830.08,50.01,"GPU GOES BRRRRRR!","meh..."],[3.57,2,4,16,4,1,4,0,36.10292654742261,13921.55,222744.8,43.3,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,64,4,1,2,1,35.98327111890365,13875.41,111003.28,27.27,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,32,8,8,1,1,34.84670251495513,5782.21,370061.44,49.36,"GPU GOES BRRRRRR!","meh..."],[3.57,4,2,16,8,2,2,1,35.94540365712747,12647.71,404726.72,31.69,"GPU GOES BRRRRRR!","meh..."],[3.57,2,4,16,4,1,4,1,35.91270709181217,13848.2,221571.2,33.1,"GPU GOES BRRRRRR!","meh..."],[3.57,1,1,64,4,1,2,0,35.877023114863306,13834.44,110675.52,45.53,"GPU GOES BRRRRRR!","meh..."],[1.34,2,16,4,4,1,4,0,33.96597067008932,34808.51,556936.16,66.96,"GPU GOES BRRRRRR!","meh..."],[3.57,2,1,32,8,1,2,0,35.56092428126609,13712.55,219400.8,45.39,"GPU GOES BRRRRRR!","meh..."],[3.57,8,1,8,32,2,1,1,35.52782148934044,12500.78,800049.92,36.73,"GPU GOES BRRRRRR!","moar?"],[1.34,2,16,4,4,1,4,1,33.67612051770293,34511.47,552183.52,65.11,"GPU GOES BRRRRRR!","meh..."],[8.86,16,2,4,32,1,4,0,34.338785976475826,5697.93,729335.04,61.18,"GPU GOES BRRRRRR!","moar?"],[1.34,4,4,8,8,2,2,1,33.634927194740484,29514.76,944472.32,25.14,"GPU GOES BRRRRRR!","meh..."],[3.57,2,1,32,8,1,2,1,35.35192924476104,13631.96,218111.36,25.68,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,16,16,1,4,1,34.18932785884072,5673.13,363080.32,29.25,"GPU GOES BRRRRRR!","meh..."],[3.57,4,2,8,16,1,2,1,35.0992368963673,13534.52,433104.64,36.58,"GPU GOES BRRRRRR!","meh..."],[3.57,2,8,16,2,2,4,1,35.09315861081768,12347.84,197565.44,51,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,128,2,8,2,0,33.96514068238806,5635.93,180349.76,35.27,"GPU GOES BRRRRRR!","meh..."],[3.57,4,2,32,4,4,2,1,34.97515676227404,12306.32,393802.24,27.22,"GPU GOES BRRRRRR!","meh..."],[1.34,2,8,16,2,2,4,1,33.16991470465708,29106.71,465707.36,26,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,16,16,1,4,0,33.80109834520953,5608.71,358957.44,51.12,"GPU GOES BRRRRRR!","meh..."],[3.57,4,4,16,4,4,2,1,34.85385813761501,12263.64,392436.48,46.74,"GPU GOES BRRRRRR!","meh..."],[1.34,2,8,8,4,1,4,0,33.04200012473128,33861.62,541785.92,39.25,"GPU GOES BRRRRRR!","meh..."],[8.86,4,8,8,4,1,8,0,33.70497508003695,5592.76,178968.32,57.04,"GPU GOES BRRRRRR!","meh..."],[1.34,2,8,8,4,1,4,1,33.015868334127624,33834.84,541357.44,33.72,"GPU GOES BRRRRRR!","meh..."],[3.57,8,1,16,16,4,1,1,34.75310752270961,12228.19,782604.16,28.72,"GPU GOES BRRRRRR!","moar?"],[8.86,8,4,16,4,4,4,1,33.653026331084746,5584.14,357384.96,43.64,"GPU GOES BRRRRRR!","meh..."],[1.34,2,16,8,2,2,4,1,32.96699809153553,28928.65,462858.4,49.64,"GPU GOES BRRRRRR!","meh..."],[8.86,2,2,64,2,1,8,0,33.62518373013821,5579.52,89272.32,31.87,"GPU GOES BRRRRRR!","meh..."],[3.57,1,8,16,2,1,4,0,34.66459775853078,13366.92,106935.36,68.68,"GPU GOES BRRRRRR!","meh..."],[3.57,16,8,1,32,1,4,0,34.58630566085009,13336.73,1707101.44,66.2,"GPU GOES BRRRRRR!","MOAR!"],[1.34,1,16,8,2,1,4,0,32.783804762754976,33597.02,268776.16,66.71,"GPU GOES BRRRRRR!","meh..."],[3.57,8,1,4,64,1,1,1,34.52289917564661,13312.28,851985.92,47.67,"GPU GOES BRRRRRR!","moar?"],[3.57,8,2,4,32,2,1,1,34.52202056931164,12146.88,777400.32,49.85,"GPU GOES BRRRRRR!","moar?"],[3.57,8,2,2,64,1,1,1,34.517427286739476,13310.17,851850.88,64.35,"GPU GOES BRRRRRR!","moar?"],[8.86,2,2,64,2,1,8,1,33.3241582069175,5529.57,88473.12,24.64,"GPU GOES BRRRRRR!","meh..."],[8.86,16,4,2,32,1,4,1,33.19922809487816,5508.84,705131.52,52.9,"GPU GOES BRRRRRR!","moar?"],[1.34,8,1,8,32,2,1,1,32.47180907715999,28494.12,1823623.68,16.27,"GPU GOES BRRRRRR!","moar?"],[1.34,2,8,32,1,4,4,0,32.31512590778966,28356.63,453706.08,25.54,"GPU GOES BRRRRRR!","meh..."],[3.57,2,8,32,1,4,4,0,33.9560550558487,11947.74,191163.84,49.26,"GPU GOES BRRRRRR!","meh..."],[8.86,16,2,2,64,1,2,1,32.78538579899095,5440.17,696341.76,67.71,"GPU GOES BRRRRRR!","moar?"],[3.57,4,1,16,16,1,2,1,33.846848597826785,13051.59,417650.88,25.19,"GPU GOES BRRRRRR!","meh..."],[3.57,4,1,16,16,1,2,0,33.832507580928,13046.06,417473.92,45.35,"GPU GOES BRRRRRR!","meh..."],[8.86,32,8,1,32,1,8,0,32.71162098609362,5427.93,1389550.08,63.39,"GPU GOES BRRRRRR!","MOARRR!!!!"],[8.86,2,1,256,1,16,1,0,32.69932685061073,5425.89,86814.24,60.35,"GPU GOES BRRRRRR!","meh..."],[1.34,4,4,16,4,4,2,1,31.97571978128178,28058.8,897881.6,24.53,"GPU GOES BRRRRRR!","meh..."],[8.86,8,2,32,4,8,2,1,32.55764296570745,5402.38,345752.32,46.7,"GPU GOES BRRRRRR!","meh..."],[8.86,16,1,8,32,4,1,1,32.52570231959995,5397.08,690826.24,49.7,"GPU GOES BRRRRRR!","moar?"],[1.34,4,8,4,8,2,2,1,31.857406837474752,27954.98,894559.36,46.34,"GPU GOES BRRRRRR!","meh..."],[1.34,2,4,16,4,1,4,0,31.827535402274428,32617.03,521872.48,21.59,"GPU GOES BRRRRRR!","meh..."],[8.86,16,1,4,64,2,1,1,32.39842185813006,5375.96,688122.88,58.31,"GPU GOES BRRRRRR!","moar?"],[8.86,16,1,4,64,1,2,1,32.376244201964845,5372.28,687651.84,54.19,"GPU GOES BRRRRRR!","moar?"],[1.34,2,4,16,4,1,4,1,31.67291092359429,32458.57,519337.12,18.01,"GPU GOES BRRRRRR!","meh..."],[8.86,2,8,32,1,2,8,0,32.27963881383215,5356.25,85700,51.88,"GPU GOES BRRRRRR!","meh..."],[3.57,8,1,32,8,8,1,1,33.243411425735054,11696.99,748607.36,25.22,"GPU GOES BRRRRRR!","moar?"],[80,32,1,64,4,16,4,0,24.412456340003725,529.69,135600.64,65.4,"GPU goes brr","MOAR!"],[8.86,16,4,4,16,1,8,0,32.18713147086042,5340.9,683635.2,44.3,"GPU GOES BRRRRRR!","moar?"],[80,16,2,32,4,4,8,1,24.404402077513772,471.21,60314.88,60.22,"GPU goes brr","moar?"],[3.57,4,8,4,8,1,4,0,33.20215116545113,12802.99,409695.68,64.68,"GPU GOES BRRRRRR!","meh..."],[1.34,4,16,2,8,1,4,1,31.329333497472387,32106.47,1027407.04,64.97,"GPU GOES BRRRRRR!","meh..."],[8.86,8,1,64,4,8,2,1,31.91943269725754,5296.48,338974.72,27.7,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,256,1,8,1,0,31.266799379784217,27436.72,219493.76,18.38,"GPU GOES BRRRRRR!","meh..."],[1.34,2,16,16,1,4,4,0,31.226104376669703,27401.01,438416.16,49.89,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,128,2,8,2,1,31.800288060739557,5276.71,168854.72,29.27,"GPU GOES BRRRRRR!","meh..."],[8.86,4,1,128,2,16,1,1,31.758283097839698,5269.74,168631.68,57.07,"GPU GOES BRRRRRR!","meh..."],[8.86,16,2,4,32,1,4,1,31.74677241216699,5267.83,674282.24,35.89,"GPU GOES BRRRRRR!","moar?"],[8.86,16,1,16,16,4,2,1,31.669693003918884,5255.04,672645.12,31.38,"GPU GOES BRRRRRR!","moar?"],[1.34,4,4,8,8,4,1,1,31.036931301424403,27235.01,871520.32,38.7,"GPU GOES BRRRRRR!","meh..."],[3.57,2,2,32,4,1,4,0,32.63214112668939,12583.19,201331.04,30.49,"GPU GOES BRRRRRR!","meh..."],[3.57,4,8,8,4,2,4,1,32.536565574307076,11448.28,366344.96,48.16,"GPU GOES BRRRRRR!","meh..."],[3.57,4,2,64,2,8,2,1,32.52050800098646,11442.63,366164.16,24.7,"GPU GOES BRRRRRR!","meh..."],[3.57,8,2,8,16,4,1,1,32.40463768872432,11401.86,729719.04,43.19,"GPU GOES BRRRRRR!","moar?"],[3.57,2,2,32,4,1,4,1,32.40328131813287,12494.94,199919.04,21.16,"GPU GOES BRRRRRR!","meh..."],[8.86,16,2,4,32,2,2,1,31.32943472481896,5198.58,665418.24,51.03,"GPU GOES BRRRRRR!","moar?"],[1.34,8,2,4,32,2,1,1,30.652750415119012,26897.89,1721464.96,24.32,"GPU GOES BRRRRRR!","moar?"],[1.34,8,1,4,64,1,1,1,30.53941767179803,31296.96,2003005.44,23.76,"GPU GOES BRRRRRR!","moar?"],[8.86,4,1,128,2,16,1,0,31.154845947887953,5169.61,165427.52,62.84,"GPU GOES BRRRRRR!","meh..."],[1.34,4,8,8,4,2,4,1,30.495884910254464,26760.24,856327.68,24.25,"GPU GOES BRRRRRR!","meh..."],[8.86,4,8,16,2,2,8,1,31.05558888347464,5153.14,164900.48,48.17,"GPU GOES BRRRRRR!","meh..."],[3.57,8,2,4,32,1,2,1,32.04384823636591,12356.34,790805.76,36.4,"GPU GOES BRRRRRR!","moar?"],[3.57,8,2,8,16,2,2,1,31.992171208012877,11256.73,720430.72,30.75,"GPU GOES BRRRRRR!","moar?"],[8.86,16,2,8,16,4,2,1,30.986042646478307,5141.6,658124.8,43.52,"GPU GOES BRRRRRR!","moar?"],[1.34,8,1,4,64,1,1,0,30.35915905797384,31112.23,1991182.72,36.66,"GPU GOES BRRRRRR!","moar?"],[1.34,8,2,2,64,1,1,1,30.095372390539776,30841.9,1973881.6,34.81,"GPU GOES BRRRRRR!","moar?"],[3.57,8,1,8,32,2,1,0,31.55009058323051,11101.18,710475.52,59.62,"GPU GOES BRRRRRR!","moar?"],[8.86,16,8,2,16,1,8,1,30.53766829357298,5067.2,648601.6,51.87,"GPU GOES BRRRRRR!","moar?"],[3.57,8,4,4,16,2,2,1,31.515133388213965,11088.88,709688.32,49.93,"GPU GOES BRRRRRR!","moar?"],[3.57,8,1,8,32,1,2,1,31.38063455432348,12100.6,774438.4,25.02,"GPU GOES BRRRRRR!","moar?"],[80,16,2,16,8,1,16,1,23.03090672916449,444.69,56920.32,58.68,"GPU goes brr","moar?"],[3.57,8,1,8,32,1,2,0,31.331517219773215,12081.66,773226.24,45.46,"GPU GOES BRRRRRR!","moar?"],[3.57,16,4,2,32,1,4,0,31.305350698885565,12071.57,1545160.96,42.68,"GPU GOES BRRRRRR!","MOAR!"],[8.86,16,1,16,16,8,1,1,30.319387123381706,5030.98,643965.44,45.66,"GPU GOES BRRRRRR!","moar?"],[8.86,16,1,8,32,1,4,0,30.25731379226713,5020.68,642647.04,51.36,"GPU GOES BRRRRRR!","moar?"],[3.57,8,2,16,8,4,2,1,31.207282708198377,10980.56,702755.84,26.01,"GPU GOES BRRRRRR!","moar?"],[3.57,16,2,2,64,1,2,0,31.19365673906087,12028.5,1539648,59.76,"GPU GOES BRRRRRR!","MOAR!"],[8.86,8,4,32,2,8,4,1,30.20789618885552,5012.48,320798.72,44.7,"GPU GOES BRRRRRR!","meh..."],[8.86,16,1,8,32,2,2,1,30.160768669504453,5004.66,640596.48,42.09,"GPU GOES BRRRRRR!","moar?"],[8.86,16,1,8,32,1,4,1,30.14895665698168,5002.7,640345.6,27.57,"GPU GOES BRRRRRR!","moar?"],[1.34,8,8,1,32,1,2,0,29.55013716583125,30283.14,1938120.96,62.1,"GPU goes brr","moar?"],[1.34,16,4,1,64,1,2,0,29.455465533106604,30186.12,3863823.36,38.84,"GPU goes brr","MOAR!"],[8.86,8,1,64,4,16,1,1,29.98895209959409,4976.15,318473.6,54.28,"GPU goes brr","meh..."],[8.86,4,8,32,1,4,8,0,29.95393791961587,4970.34,159050.88,46.57,"GPU goes brr","meh..."],[1.34,16,8,1,32,1,4,0,29.3595351248077,30087.81,3851239.68,31.3,"GPU goes brr","MOAR!"],[3.57,4,8,16,2,4,4,1,30.90122251865924,10872.87,347931.84,47.62,"GPU GOES BRRRRRR!","meh..."],[3.57,1,16,16,1,1,8,0,30.74490998243061,11855.46,94843.68,67.13,"GPU GOES BRRRRRR!","meh..."],[3.57,8,2,16,8,8,1,1,30.693497202906467,10799.78,691185.92,40.13,"GPU GOES BRRRRRR!","moar?"],[3.57,2,16,8,2,1,8,0,30.609046352172523,11803.07,188849.12,68.72,"GPU GOES BRRRRRR!","meh..."],[1.34,4,16,4,4,2,4,1,29.040289263989,25482.95,815454.4,48.91,"GPU goes brr","meh..."],[1.34,4,8,8,4,4,2,1,29.00806146538697,25454.67,814549.44,44.98,"GPU goes brr","meh..."],[3.57,2,16,8,2,1,8,1,30.346888413963132,11701.98,187231.68,67.88,"GPU GOES BRRRRRR!","meh..."],[1.34,4,8,16,2,4,4,1,28.694341851626255,25179.38,805740.16,25.81,"GPU goes brr","meh..."],[8.86,16,1,32,8,8,2,1,29.233284624986595,4850.76,620897.28,27.1,"GPU goes brr","moar?"],[8.86,16,4,4,16,1,8,1,29.224847473184617,4849.36,620718.08,30.91,"GPU goes brr","moar?"],[3.57,4,8,32,1,8,4,0,30.103914267444544,10592.33,338954.56,47.45,"GPU GOES BRRRRRR!","meh..."],[1.34,1,1,64,4,1,2,0,28.565179431797777,29273.75,234190,19.52,"GPU goes brr","meh..."],[1.34,8,2,8,16,4,1,1,28.50023432206884,25009.05,1600579.2,21.94,"GPU goes brr","moar?"],[1.34,8,4,4,16,2,2,1,28.474946668215264,24986.86,1599159.04,24.41,"GPU goes brr","moar?"],[3.57,8,2,32,4,8,2,1,29.972270586699324,10546.01,674944.64,24.83,"GPU goes brr","moar?"],[8.86,8,8,8,4,2,8,1,28.97480645299588,4807.87,307703.68,46.46,"GPU goes brr","meh..."],[3.57,8,8,2,16,1,4,0,29.88400810541635,11523.49,737503.36,64.65,"GPU goes brr","moar?"],[8.86,16,2,8,16,1,8,0,28.791720258892877,4777.49,611518.72,32.05,"GPU goes brr","moar?"],[3.57,2,8,16,2,1,8,0,29.55170107008,11395.35,182325.6,38.29,"GPU goes brr","meh..."],[8.86,16,2,16,8,8,2,1,28.51329424942748,4731.29,605605.12,43.02,"GPU goes brr","moar?"],[8.86,8,2,64,2,16,2,1,28.511606819067083,4731.01,302784.64,51.38,"GPU goes brr","meh..."],[3.57,8,4,8,8,4,2,1,29.41457543400859,10349.78,662385.92,44.59,"GPU goes brr","moar?"],[8.86,4,1,256,1,16,2,0,28.45604214791403,4721.79,151097.28,31.86,"GPU goes brr","meh..."],[1.34,8,16,1,16,1,4,0,27.878473445262625,28570.01,1828480.64,58.68,"GPU goes brr","moar?"],[1.34,16,4,2,32,1,4,0,27.82758573503185,28517.86,3650286.08,21.57,"GPU goes brr","MOAR!"],[3.57,2,8,16,2,1,8,1,29.298956855440697,11297.89,180766.24,36.43,"GPU goes brr","meh..."],[80,32,2,8,16,1,16,1,21.4647491373816,414.45,106099.2,55.26,"GPU goes brr","MOAR!"],[8.86,32,4,2,32,1,8,0,28.30013563568741,4695.92,1202155.52,43.3,"GPU goes brr","MOAR!"],[1.34,2,1,32,8,1,2,1,27.717398961508177,28404.94,454479.04,12.26,"GPU goes brr","meh..."],[1.34,2,1,32,8,1,2,0,27.50885205008877,28191.22,451059.52,19.55,"GPU goes brr","meh..."],[3.57,16,8,1,32,1,4,1,28.96968399546991,11170.92,1429877.76,51.85,"GPU goes brr","MOAR!"],[3.57,8,8,4,8,2,4,1,28.88206082992327,10162.41,650394.24,47.23,"GPU goes brr","meh..."],[1.34,1,16,16,1,1,8,0,27.420649938338816,28100.83,224806.64,38.27,"GPU goes brr","meh..."],[8.86,32,2,2,64,1,4,0,27.875746900047726,4625.5,1184128,60.38,"GPU goes brr","MOAR!"],[3.57,16,16,1,16,1,8,0,28.73089180088765,11078.84,1418091.52,66.02,"GPU goes brr","MOAR!"],[1.34,8,1,8,32,2,1,0,27.227988411202265,23892.65,1529129.6,26.79,"GPU goes brr","moar?"],[1.34,1,1,64,4,1,2,1,27.20239386648228,27877.16,223017.28,12.54,"GPU goes brr","meh..."],[1.34,2,16,8,2,1,8,0,27.176652393886155,27850.78,445612.48,40.23,"GPU goes brr","meh..."],[8.86,8,8,16,2,4,8,1,27.690671948734252,4594.79,294066.56,45.15,"GPU goes brr","meh..."],[1.34,8,4,2,32,2,1,1,27.09819972722825,23778.76,1521840.64,40.49,"GPU goes brr","moar?"],[1.34,1,2,64,2,2,2,1,27.08267841828342,23765.14,190121.12,14.41,"GPU goes brr","meh..."],[1.34,2,16,8,2,1,8,1,26.998159969046004,27667.86,442685.76,37.14,"GPU goes brr","meh..."],[1.34,1,1,256,1,4,2,0,26.825608163380807,23539.56,188316.48,11.05,"GPU goes brr","meh..."],[3.57,16,4,1,64,1,2,1,28.182613719017738,10867.42,1391029.76,55.83,"GPU goes brr","MOAR!"],[8.86,16,2,8,16,1,8,1,27.237355835487772,4519.57,578504.96,19.77,"GPU goes brr","moar?"],[1.34,1,1,128,2,2,2,0,26.597973694825093,23339.81,186718.48,14.72,"GPU goes brr","meh..."],[1.34,8,8,4,8,2,4,1,26.440960042371493,23202.03,1484929.92,23.89,"GPU goes brr","meh..."],[3.57,16,8,2,16,1,8,0,27.807620763581216,10722.82,1372520.96,40.13,"GPU goes brr","MOAR!"],[1.34,2,2,32,4,2,2,1,26.377142619397176,23146.03,370336.48,14.2,"GPU goes brr","meh..."],[3.57,8,4,16,4,8,2,1,27.7623506110113,9768.43,625179.52,43.11,"GPU goes brr","meh..."],[1.34,8,4,8,8,4,2,1,26.358362063493303,23129.55,1480291.2,23.45,"GPU goes brr","meh..."],[8.86,2,16,16,1,1,16,0,26.852802509427445,4455.76,71292.16,67.5,"GPU goes brr","meh..."],[1.34,4,1,16,16,1,2,0,26.311473498411228,26964.14,862852.48,19.51,"GPU goes brr","meh..."],[3.57,16,4,2,32,1,4,1,27.696030536247655,10679.79,1367013.12,30.68,"GPU goes brr","MOAR!"],[3.57,2,16,16,1,2,8,0,27.690929935339287,9743.3,155892.8,55.85,"GPU goes brr","meh..."],[1.34,2,2,64,2,4,2,1,26.177508045171447,22970.85,367533.6,13.45,"GPU goes brr","meh..."],[1.34,2,1,128,2,8,1,1,26.15283577361084,22949.2,367187.2,17.39,"GPU goes brr","meh..."],[1.34,4,16,8,2,4,4,1,26.137177713045354,22935.46,733934.72,48.82,"GPU goes brr","meh..."],[1.34,4,1,16,16,1,2,1,26.106029615151627,26753.6,856115.2,12.09,"GPU goes brr","meh..."],[3.57,16,2,2,64,1,2,1,27.49014747447652,10600.4,1356851.2,34.35,"GPU goes brr","MOAR!"],[8.86,2,2,64,2,2,4,1,26.62632524891425,4418.18,70690.88,29.84,"GPU goes brr","meh..."],[3.57,2,4,32,2,1,8,0,27.44032994561113,10581.19,169299.04,23.14,"GPU goes brr","meh..."],[1.34,2,8,16,2,1,8,0,26.02233567638856,26667.83,426685.28,21.02,"GPU goes brr","meh..."],[3.57,2,4,32,2,1,8,1,27.366005615716173,10552.53,168840.48,19.67,"GPU goes brr","meh..."],[3.57,16,1,4,64,2,1,1,27.360570232191098,9627.06,1232263.68,34.38,"GPU goes brr","moar?"],[1.34,8,4,2,32,1,2,0,25.93151843398821,26574.76,1700784.64,38.84,"GPU goes brr","moar?"],[3.57,16,1,8,32,4,1,1,27.249275617813225,9587.9,1227251.2,27.66,"GPU goes brr","moar?"],[3.57,4,16,4,4,1,8,0,27.18540734865809,10482.89,335452.48,63.8,"GPU goes brr","meh..."],[1.34,16,2,1,128,1,1,0,25.806304417174907,26446.44,3385144.32,48.55,"GPU goes brr","MOAR!"],[1.34,2,8,16,2,1,8,1,25.794097221489633,26433.93,422942.88,21.41,"GPU goes brr","meh..."],[1.34,4,2,32,4,4,2,1,25.736848739533784,22584.17,722693.44,13.02,"GPU goes brr","meh..."],[1.34,2,32,8,1,2,8,0,25.73594845803111,22583.38,361334.08,59.35,"GPU goes brr","meh..."],[1.34,4,2,16,8,2,2,1,25.715526882679328,22565.46,722094.72,13.58,"GPU goes brr","meh..."],[3.57,16,1,4,64,1,2,0,27.049828982750608,10430.61,1335118.08,45.53,"GPU goes brr","MOAR!"],[8.86,16,1,32,8,16,1,1,26.20042987902421,4347.51,556481.28,50.81,"GPU goes brr","moar?"],[3.57,16,1,16,16,8,1,1,27.011708794119507,9504.31,1216551.68,24.37,"GPU goes brr","moar?"],[1.34,16,2,2,64,1,2,0,25.621849884769844,26257.41,3360948.48,25.19,"GPU goes brr","MOAR!"],[1.34,2,2,64,2,8,1,1,25.511413692877007,22386.35,358181.6,27.2,"GPU goes brr","meh..."],[8.86,16,4,16,4,8,4,1,25.961055829327982,4307.79,551397.12,42.35,"GPU goes brr","moar?"],[3.57,32,4,1,64,1,4,0,26.76959565796174,10322.55,2642572.8,38.98,"GPU goes brr","MOARRR!!!!"],[3.57,2,4,32,2,2,4,1,26.729067083193076,9404.86,150477.76,24.34,"GPU goes brr","meh..."],[1.34,2,2,32,4,1,4,1,25.31517702625922,25943.13,415090.08,10.16,"GPU goes brr","meh..."],[1.34,16,16,1,16,1,8,0,25.283414898397055,25910.58,3316554.24,34.16,"GPU goes brr","MOAR!"],[8.86,8,1,128,2,16,2,1,25.69528554756555,4263.69,272876.16,29.94,"GPU goes brr","meh..."],[8.86,4,2,32,4,2,4,1,25.65243686948549,4256.58,136210.56,27.87,"GPU goes brr","meh..."],[1.34,16,8,1,32,1,4,1,25.095945159383877,25718.46,3291962.88,29.46,"GPU goes brr","MOAR!"],[3.57,4,4,32,2,4,4,1,26.410132414513697,9292.64,297364.48,25.75,"GPU goes brr","meh..."],[3.57,16,2,4,32,1,4,0,26.40077478574748,10180.33,1303082.24,30.37,"GPU goes brr","moar?"],[3.57,8,8,4,8,2,4,0,26.342292719794603,9268.77,593201.28,54.31,"GPU goes brr","meh..."],[1.34,8,8,2,16,2,2,1,24.95061808849019,21894.25,1401232,40.67,"GPU goes brr","meh..."],[3.57,16,2,1,128,1,1,1,26.23288374885287,10115.59,1294795.52,64.3,"GPU goes brr","moar?"],[3.57,8,8,8,4,4,4,1,26.22511506527981,9227.54,590562.56,46.34,"GPU goes brr","meh..."],[8.86,16,8,2,16,2,4,1,25.273427957466463,4193.69,536792.32,41.74,"GPU goes brr","moar?"],[3.57,32,8,1,32,1,8,0,26.091626029056005,10061.12,2575646.72,34.43,"GPU goes brr","MOARRR!!!!"],[1.34,4,1,64,4,8,1,1,24.762801133483094,21729.44,695342.08,15.86,"GPU goes brr","meh..."],[3.57,4,16,8,2,2,8,1,26.0793464041092,9176.25,293640,54.37,"GPU goes brr","meh..."],[1.34,2,4,32,2,8,1,1,24.693126182757208,21668.3,346692.8,46.26,"GPU goes brr","meh..."],[1.34,1,2,4,2,4,1,1,24.671849909776306,21649.63,173197.04,29.04,"GPU goes brr","meh..."],[3.57,16,1,4,64,1,2,1,25.96325707128209,10011.62,1281487.36,24.97,"GPU goes brr","moar?"],[1.34,16,4,2,32,1,4,1,24.601090231496347,25211.33,3227050.24,17.74,"GPU goes brr","MOAR!"],[3.57,4,4,16,4,2,4,1,25.905526725474104,9115.09,291682.88,23.4,"GPU goes brr","meh..."],[8.86,16,1,8,32,2,2,0,24.951430085480833,4140.26,529953.28,63.03,"GPU goes brr","moar?"],[8.86,16,1,64,4,16,2,1,24.90122903225904,4131.93,528887.04,28.55,"GPU goes brr","moar?"],[3.57,16,1,2,128,1,1,1,25.70252545482574,9911.08,1268618.24,47.64,"GPU goes brr","moar?"],[1.34,16,4,1,64,1,2,1,24.397285683868343,25002.47,3200316.16,27.17,"GPU goes brr","MOAR!"],[1.34,2,2,32,4,1,4,0,24.31256716033553,24915.65,398650.4,13.65,"GPU goes brr","meh..."],[1.34,8,8,8,4,4,4,1,24.27911065121133,21305,1363520,25.47,"GPU goes brr","meh..."],[8.86,32,1,16,16,8,2,1,24.762678946596495,4108.94,1051888.64,25.45,"GPU goes brr","MOAR!"],[8.86,16,2,32,4,16,2,1,24.75821930921545,4108.2,525849.6,50.03,"GPU goes brr","moar?"],[3.57,16,2,4,32,2,2,1,25.55356745467341,8991.25,1150880,28.38,"GPU goes brr","moar?"],[1.34,8,1,8,32,1,2,1,24.24719865202566,24848.66,1590314.24,12.06,"GPU goes brr","moar?"],[3.57,2,1,256,1,16,1,0,25.51431876661012,8977.44,143639.04,28.75,"GPU goes brr","meh..."],[3.57,16,2,4,32,1,4,1,25.374834515344695,9784.72,1252444.16,19.57,"GPU goes brr","moar?"],[8.86,8,8,32,1,8,8,0,24.54391565344511,4072.64,260648.96,46.96,"GPU goes brr","meh..."],[1.34,4,4,32,2,8,2,1,24.02454750935413,21081.62,674611.84,27.85,"GPU goes brr","meh..."],[3.57,16,2,8,16,4,2,1,25.24145370209482,8881.43,1136823.04,25.09,"GPU goes brr","moar?"],[3.57,16,4,4,16,1,8,0,25.236740636527305,9731.47,1245628.16,23.66,"GPU goes brr","moar?"],[1.34,8,16,2,8,2,4,1,23.959909576655853,21024.9,1345593.6,40.8,"GPU goes brr","meh..."],[8.86,32,2,4,32,1,8,0,24.422179606016517,4052.44,1037424.64,31.9,"GPU goes brr","MOAR!"],[1.34,2,4,32,2,1,8,0,23.92544000252428,24518.92,392302.72,12.88,"GPU goes brr","meh..."],[1.34,16,8,2,16,1,8,0,23.919858455016705,24513.2,3137689.6,22.8,"GPU goes brr","MOAR!"],[1.34,16,1,2,128,1,1,0,23.900947547552228,24493.82,3135208.96,36.67,"GPU goes brr","MOAR!"],[1.34,1,1,128,2,2,2,1,23.876673423547924,20951.86,167614.88,11.35,"GPU goes brr","meh..."],[1.34,2,8,32,1,8,2,0,23.87430306212316,20949.78,335196.48,53.13,"GPU goes brr","meh..."],[3.57,16,2,2,64,2,1,1,25.043874499041973,8811.91,1127924.48,49.01,"GPU goes brr","moar?"],[1.34,2,4,32,2,1,8,1,23.727138938802344,24315.7,389051.2,11.34,"GPU goes brr","meh..."],[8.86,32,8,1,32,1,8,1,24.197811633453817,4015.21,1027893.76,48.73,"GPU goes brr","MOAR!"],[1.34,4,2,32,4,8,1,1,23.715363491011946,20810.31,665929.92,25,"GPU goes brr","meh..."],[3.57,16,16,1,16,1,8,1,24.81747984050087,9569.8,1224934.4,60.61,"GPU goes brr","moar?"],[3.57,8,4,32,2,8,4,1,24.798776247153633,8725.67,558442.88,24.39,"GPU goes brr","meh..."],[1.34,16,1,4,64,2,1,1,23.493996805069784,20616.06,2638855.68,16.24,"GPU goes brr","moar?"],[3.57,8,8,16,2,8,4,1,24.717237879265458,8696.98,556606.72,45.96,"GPU goes brr","meh..."],[1.34,4,32,4,2,2,8,1,23.357735211051228,20496.49,655887.68,58.31,"GPU goes brr","meh..."],[8.86,16,8,8,4,4,8,1,23.81157087703309,3951.12,505743.36,43.24,"GPU goes brr","moar?"],[3.57,8,16,2,8,1,8,0,24.564631893370436,9472.3,606227.2,65.3,"GPU goes brr","meh..."],[3.57,4,16,16,1,4,8,0,24.47378801543481,8611.32,275562.24,53.49,"GPU goes brr","meh..."],[8.86,16,8,4,8,2,8,1,23.70550382580818,3933.52,503490.56,44.96,"GPU goes brr","moar?"],[8.86,32,1,4,64,1,4,0,23.687062622583845,3930.46,1006197.76,51.54,"GPU goes brr","MOAR!"],[1.34,32,4,1,64,1,4,0,23.199243591578018,23774.71,6086325.76,19.57,"GPU goes brr","MOARRR!!!!"],[1.34,16,2,2,64,1,2,1,23.195681939759368,23771.06,3042695.68,17.54,"GPU goes brr","moar?"],[1.34,4,8,32,1,8,4,0,23.188026449813098,20347.57,651122.24,29.39,"GPU goes brr","meh..."],[1.34,8,2,16,8,4,2,1,23.1719923222908,20333.5,1301344,12.04,"GPU goes brr","meh..."],[3.57,16,8,2,16,1,8,1,24.251307903910956,9351.48,1196989.44,34.11,"GPU goes brr","moar?"],[8.86,32,4,1,64,1,4,1,23.481979568425675,3896.43,997486.08,50.74,"GPU goes brr","MOAR!"],[80,64,2,4,32,1,16,1,17.78759341593367,343.45,175846.4,53.46,"GPU sipping coffee","MOARRR!!!!"],[8.86,4,2,64,2,4,4,1,23.425269855242355,3887.02,124384.64,28.63,"GPU goes brr","meh..."],[8.86,32,4,2,32,1,8,1,23.312935205535968,3868.38,990305.28,29.58,"GPU goes brr","MOAR!"],[3.57,4,1,128,2,16,1,1,24.065300402448827,8467.59,270962.88,25.61,"GPU goes brr","meh..."],[3.57,4,2,64,2,16,1,1,24.03304315338885,8456.24,270599.68,42.41,"GPU goes brr","meh..."],[3.57,4,1,128,2,16,1,0,24.032361061778765,8456,270592,29.95,"GPU goes brr","meh..."],[1.34,2,16,16,1,2,8,0,22.73249540542874,19947.84,319165.44,21.6,"GPU goes brr","meh..."],[1.34,16,1,8,32,4,1,1,22.663378857153877,19887.19,2545560.32,13.76,"GPU goes brr","moar?"],[1.34,8,2,8,16,2,2,1,22.638227954920964,19865.12,1271367.68,13.46,"GPU goes brr","meh..."],[1.34,8,1,8,32,1,2,0,22.50423358943282,23062.46,1475997.44,19.57,"GPU goes brr","meh..."],[8.86,8,8,4,8,1,8,0,22.94573830603972,3807.45,243676.8,58.62,"GPU goes brr","meh..."],[8.86,16,16,2,8,1,16,1,22.92705604133533,3804.35,486956.8,63.25,"GPU goes brr","moar?"],[3.57,32,2,1,128,1,2,0,23.56026798112278,9085.01,2325762.56,55.72,"GPU goes brr","MOARRR!!!!"],[3.57,16,4,2,32,2,2,1,23.50828734136817,8271.6,1058764.8,43.29,"GPU goes brr","moar?"],[8.86,16,4,32,2,16,4,1,22.74161949780177,3773.58,483018.24,48.99,"GPU goes brr","moar?"],[1.34,16,2,1,128,1,1,1,22.242398512036367,22794.13,2917648.64,30.8,"GPU goes brr","moar?"],[8.86,32,2,2,64,1,4,1,22.649714808530184,3758.33,962132.48,33.81,"GPU goes brr","MOAR!"],[3.57,8,16,4,4,2,8,1,23.355413559259244,8217.81,525939.84,53.18,"GPU goes brr","meh..."],[8.86,32,2,1,128,1,2,1,22.60704692656016,3751.25,960320,62.68,"GPU goes brr","MOAR!"],[1.34,16,16,1,16,1,8,1,22.139403347801306,22688.58,2904138.24,29.25,"GPU goes brr","moar?"],[3.57,16,2,4,32,4,1,1,23.30900290929013,8201.48,1049789.44,42.31,"GPU goes brr","moar?"],[1.34,8,8,4,8,4,2,1,22.095928007228313,19389.25,1240912,40.05,"GPU goes brr","meh..."],[3.57,8,4,8,8,2,4,1,23.26819109462041,8187.12,523975.68,22.97,"GPU goes brr","meh..."],[1.34,4,8,1,16,2,1,1,22.03695387081901,19337.5,618800,41.34,"GPU goes brr","meh..."],[1.34,32,8,1,32,1,8,0,21.876563201535483,22419.22,5739320.32,16.19,"GPU goes brr","MOARRR!!!!"],[3.57,32,4,2,32,1,8,0,23.034110852986437,8882.12,2273822.72,23.37,"GPU goes brr","MOARRR!!!!"],[3.57,8,2,64,2,16,2,1,22.986572521109096,8088.03,517633.92,24.44,"GPU goes brr","meh..."],[1.34,4,4,16,4,8,1,1,21.78248189670892,19114.2,611654.4,44.82,"GPU goes brr","meh..."],[3.57,8,4,16,4,4,4,1,22.917482325104856,8063.72,516078.08,24.13,"GPU goes brr","meh..."],[1.34,8,1,32,8,8,1,1,21.75423129107439,19089.41,1221722.24,14.59,"GPU goes brr","meh..."],[1.34,4,16,4,4,1,8,0,21.739893351201392,22279.16,712933.12,25.67,"GPU goes brr","meh..."],[3.57,8,1,64,4,16,1,1,22.825257855325443,8031.27,514001.28,23.86,"GPU goes brr","meh..."],[3.57,16,4,4,16,1,8,1,22.815494627929045,8797.82,1126120.96,18.04,"GPU goes brr","moar?"],[8.86,32,2,4,32,1,8,1,22.094007831629657,3666.12,938526.72,19.45,"GPU goes brr","MOAR!"],[1.34,16,4,4,16,1,8,0,21.642655377577285,22179.51,2838977.28,10.93,"GPU goes brr","moar?"],[8.86,16,8,4,8,2,8,0,22.057306221291032,3660.03,468483.84,50.21,"GPU goes brr","moar?"],[1.34,16,8,2,16,1,8,1,21.54447088278494,22078.89,2826097.92,20.02,"GPU goes brr","moar?"],[1.34,16,1,2,128,1,1,1,21.50803469678269,22041.55,2821318.4,23.73,"GPU goes brr","moar?"],[8.86,32,1,32,8,16,2,1,21.878800195309104,3630.41,929384.96,28.39,"GPU goes brr","MOAR!"],[1.34,4,8,16,2,8,2,1,21.42556016678827,18801,601632,51.57,"GPU goes brr","meh..."],[3.57,16,8,2,16,2,4,1,22.51681034515103,7922.74,1014110.72,40.94,"GPU goes brr","moar?"],[8.86,1,1,256,1,1,8,0,21.79292204303893,3616.16,28929.28,24.39,"GPU goes brr","meh..."],[1.34,8,8,1,32,2,1,0,21.301868325901975,18692.46,1196317.44,56.79,"GPU goes brr","meh..."],[8.86,16,2,16,8,4,4,1,21.707827340578945,3602.04,461061.12,23.18,"GPU goes brr","moar?"],[8.86,2,1,128,2,1,8,0,21.66455680490878,3594.86,57517.76,26.38,"GPU goes brr","meh..."],[8.86,16,16,2,8,2,8,1,21.5812097981792,3581.03,458371.84,67.8,"GPU goes brr","moar?"],[1.34,4,16,8,2,2,8,1,21.019989047818093,18445.11,590243.52,21.03,"GPU goes brr","meh..."],[8.86,4,1,64,4,1,8,0,21.43627158329516,3556.98,113823.36,26.08,"GPU goes brr","meh..."],[8.86,32,1,16,16,16,1,1,21.406500490508165,3552.04,909322.24,48.13,"GPU goes brr","MOAR!"],[8.86,8,2,16,8,2,4,1,21.364857119828383,3545.13,226888.32,27,"GPU goes brr","meh..."],[3.57,1,1,128,2,1,4,0,22.01180121977322,8487.91,67903.28,23.44,"GPU goes brr","meh..."],[8.86,16,16,2,8,1,16,0,21.29199628748127,3533.04,452229.12,52.94,"GPU goes brr","moar?"],[8.86,32,1,2,128,1,2,1,21.271144469456377,3529.58,903572.48,51.53,"GPU goes brr","MOAR!"],[1.34,4,32,8,1,4,8,0,20.839795995662755,18286.99,585183.68,58.46,"GPU goes brr","meh..."],[1.34,16,1,4,64,1,2,0,20.79056189330456,21306.28,2727203.84,18.53,"GPU goes brr","moar?"],[1.34,16,2,4,32,1,4,0,20.78473639704228,21300.31,2726439.68,12.66,"GPU goes brr","moar?"],[3.57,2,1,64,4,1,4,0,21.811649378137044,8410.73,134571.68,23.42,"GPU goes brr","meh..."],[3.57,16,4,16,4,8,4,1,21.78634707172407,7665.72,981212.16,23.78,"GPU goes brr","moar?"],[3.57,2,1,64,4,2,2,1,21.768385325991993,7659.4,122550.4,20.6,"GPU goes brr","meh..."],[8.86,32,1,4,64,1,4,1,21.062324962357323,3494.93,894702.08,26.78,"GPU goes brr","MOAR!"],[80,64,2,4,32,2,8,1,15.96610656067559,308.28,157839.36,66.31,"GPU sipping coffee","MOARRR!!!!"],[80,8,1,64,4,1,16,1,15.960927468110173,308.18,19723.52,64.55,"GPU sipping coffee","meh..."],[1.34,16,2,2,64,2,1,1,20.60993931327133,18085.29,2314917.12,24.31,"GPU goes brr","moar?"],[1.34,4,2,64,2,8,2,1,20.570167383596264,18050.39,577612.48,13.78,"GPU goes brr","meh..."],[8.86,4,1,64,4,1,8,1,20.906056857910617,3469,111008,17.53,"GPU goes brr","meh..."],[8.86,32,16,1,16,1,16,1,20.846333876226588,3459.09,885527.04,63.16,"GPU goes brr","MOAR!"],[1.34,4,16,16,1,8,4,0,20.393928732471657,17895.74,572663.68,55.83,"GPU goes brr","meh..."],[8.86,2,1,128,2,1,8,1,20.78727381361272,3449.29,55188.64,19.21,"GPU goes brr","meh..."],[3.57,16,1,8,32,4,1,0,21.457436813247288,7549.99,966398.72,42.35,"GPU goes brr","moar?"],[8.86,16,2,8,16,2,4,1,20.775702862569997,3447.37,441263.36,25.97,"GPU goes brr","moar?"],[1.34,16,2,8,16,4,2,1,20.3399574261848,17848.38,2284592.64,11.96,"GPU goes brr","moar?"],[1.34,4,16,16,1,4,8,0,20.329256611868217,17838.99,570847.68,24.75,"GPU goes brr","meh..."],[8.86,8,1,128,2,32,1,1,20.70525264502345,3435.68,219883.52,64.04,"GPU goes brr","meh..."],[3.57,8,2,32,4,16,1,1,21.358874575590892,7515.31,480979.84,40.8,"GPU goes brr","meh..."],[1.34,16,8,2,16,2,4,1,20.24643071362226,17766.31,2274087.68,18.68,"GPU goes brr","moar?"],[80,16,1,32,8,1,16,1,15.627393906897147,301.74,38622.72,65.71,"GPU sipping coffee","meh..."],[3.57,1,1,128,2,1,4,1,21.231473555244524,8187.01,65496.08,17.21,"GPU goes brr","meh..."],[3.57,4,1,32,8,2,2,1,21.227429838231945,7469.06,239009.92,19.47,"GPU goes brr","meh..."],[3.57,8,16,8,2,4,8,1,21.197702012226006,7458.6,477350.4,52.43,"GPU goes brr","meh..."],[3.57,32,2,2,64,1,4,0,21.164877295938783,8161.33,2089300.48,30.33,"GPU goes brr","MOAR!"],[3.57,8,4,32,2,16,2,1,21.140633680849408,7438.52,476065.28,44.26,"GPU goes brr","meh..."],[1.34,16,1,4,64,1,2,1,20.060345206880807,20557.95,2631417.6,12.02,"GPU goes brr","moar?"],[3.57,1,1,256,1,2,4,0,21.11809623723305,7430.59,59444.72,15.89,"GPU goes brr","meh..."],[1.34,8,2,32,4,8,2,1,20.01331478427179,17561.75,1123952,14.49,"GPU goes brr","meh..."],[3.57,2,1,128,2,4,2,0,21.030674829207943,7399.83,118397.28,23.1,"GPU goes brr","meh..."],[80,8,1,64,4,2,8,1,15.42489138758924,297.83,19061.12,65.63,"GPU sipping coffee","meh..."],[1.34,8,2,16,8,8,1,1,19.9312068320343,17489.7,1119340.8,24.35,"GPU sipping coffee","meh..."],[3.57,2,1,64,4,1,4,1,20.98925818845496,8093.61,129497.76,14.39,"GPU goes brr","meh..."],[1.34,8,16,4,4,4,4,1,19.90970263968563,17470.83,1118133.12,40.8,"GPU sipping coffee","meh..."],[1.34,8,4,16,4,8,2,1,19.89987931493494,17462.21,1117581.44,26.65,"GPU sipping coffee","meh..."],[8.86,8,1,32,8,1,8,0,20.28845734600555,3366.52,215457.28,26.32,"GPU goes brr","meh..."],[3.57,16,4,8,8,8,2,1,20.9364609255658,7366.68,942935.04,42.9,"GPU goes brr","moar?"],[1.34,16,4,2,32,2,2,1,19.85293932114365,17421.02,2229890.56,22.25,"GPU sipping coffee","moar?"],[3.57,4,1,32,8,1,4,0,20.800957783930436,8021,256672,23.38,"GPU goes brr","meh..."],[1.34,8,32,2,4,2,8,1,19.723458328316116,17307.4,1107673.6,42.58,"GPU sipping coffee","meh..."],[8.86,16,8,4,8,4,4,1,20.02931625494468,3323.52,425410.56,62.04,"GPU goes brr","moar?"],[8.86,8,1,32,8,1,8,1,20.00629488359927,3319.7,212460.8,16.79,"GPU goes brr","meh..."],[1.34,8,16,1,16,2,2,1,19.586900439119468,17187.57,1100004.48,43.18,"GPU sipping coffee","meh..."],[3.57,16,8,4,8,4,4,1,20.58237011848361,7242.09,926987.52,42.67,"GPU goes brr","moar?"],[1.34,8,8,16,2,8,4,1,19.51087793400131,17120.86,1095735.04,28.64,"GPU sipping coffee","meh..."],[3.57,16,4,1,64,2,1,1,20.470251310076893,7202.64,921937.92,49.37,"GPU goes brr","moar?"],[3.57,4,1,64,4,4,2,1,20.46613033993267,7201.19,230438.08,17.77,"GPU goes brr","meh..."],[3.57,2,1,128,2,4,2,1,20.412671409992736,7182.38,114918.08,18.98,"GPU goes brr","meh..."],[8.86,16,8,16,2,8,8,1,19.735281514645617,3274.73,419165.44,45.7,"GPU sipping coffee","moar?"],[3.57,8,8,32,1,16,4,0,20.328717300985527,7152.84,457781.76,46.11,"GPU goes brr","meh..."],[3.57,4,1,128,2,8,2,1,20.28816127066959,7138.57,228434.24,14.44,"GPU goes brr","meh..."],[1.34,16,2,4,32,1,4,1,19.260788526420644,19738.56,2526535.68,8.88,"GPU sipping coffee","moar?"],[3.57,4,1,32,8,1,4,1,20.255143348724868,7810.53,249936.96,13.44,"GPU goes brr","meh..."],[80,4,1,256,1,4,8,0,14.760931720702446,285.01,9120.32,65.97,"GPU sipping coffee","meh..."],[3.57,2,1,256,1,8,2,0,20.045365077965265,7053.14,112850.24,16.76,"GPU goes brr","meh..."],[80,32,1,16,16,1,16,1,14.710176613561332,284.03,72711.68,64.06,"GPU sipping coffee","moar?"],[1.34,8,32,1,8,2,4,1,18.98268619517334,16657.37,1066071.68,49.81,"GPU sipping coffee","meh..."],[8.86,16,1,128,2,32,2,1,19.32583859076944,3206.79,410469.12,34.17,"GPU sipping coffee","moar?"],[3.57,2,2,64,2,1,8,1,19.90590198421148,7675.86,122813.76,12.73,"GPU sipping coffee","meh..."],[1.34,16,8,4,8,4,4,1,18.870857557379235,16559.24,2119582.72,21.39,"GPU sipping coffee","moar?"],[3.57,16,8,1,32,2,2,1,19.86682759902712,6990.32,894760.96,46.83,"GPU sipping coffee","moar?"],[3.57,8,1,16,16,2,2,1,19.8320409269131,6978.08,446597.12,18.69,"GPU sipping coffee","meh..."],[8.86,16,1,16,16,1,8,0,19.1983773331895,3185.64,407761.92,26.31,"GPU sipping coffee","moar?"],[1.34,16,2,4,32,2,2,1,18.777159905290873,16477.02,2109058.56,13.43,"GPU sipping coffee","moar?"],[3.57,8,1,16,16,1,4,0,19.74327537124174,7613.15,487241.6,22.93,"GPU sipping coffee","meh..."],[3.57,8,16,16,1,8,8,0,19.73035243604384,6942.3,444307.2,53.09,"GPU sipping coffee","meh..."],[8.86,8,2,64,2,8,4,1,19.008963275235008,3154.21,201869.44,25.7,"GPU sipping coffee","meh..."],[80,16,1,32,8,2,8,1,14.40564597071466,278.15,35603.2,57,"GPU sipping coffee","meh..."],[3.57,32,4,1,64,1,4,1,19.604688763091477,7559.71,1935285.76,26.48,"GPU sipping coffee","MOAR!"],[8.86,16,8,4,8,1,16,0,18.954905238332312,3145.24,402590.72,40.58,"GPU sipping coffee","moar?"],[3.57,4,1,256,1,16,2,0,19.56332525302578,6883.53,220272.96,15.88,"GPU sipping coffee","meh..."],[3.57,8,1,16,16,1,4,1,19.552952183140174,7539.76,482544.64,13.33,"GPU sipping coffee","meh..."],[8.86,16,8,4,8,1,16,1,18.93339050123726,3141.67,402133.76,34.61,"GPU sipping coffee","moar?"],[1.34,32,2,1,128,1,2,0,18.505240201308663,18964.27,4854853.12,24.44,"GPU sipping coffee","MOAR!"],[8.86,16,1,16,16,1,8,1,18.822381689671182,3123.25,399776,14.65,"GPU sipping coffee","moar?"],[3.57,32,8,1,32,1,8,1,19.42128971877287,7488.99,1917181.44,28.42,"GPU sipping coffee","MOAR!"],[8.86,16,2,32,4,8,4,1,18.79707023426524,3119.05,399238.4,23.8,"GPU sipping coffee","moar?"],[8.86,16,1,64,4,32,1,1,18.69257008266069,3101.71,397018.88,61.63,"GPU sipping coffee","moar?"],[8.86,32,1,8,32,4,2,0,18.687688587689543,3100.9,793830.4,42.04,"GPU sipping coffee","MOAR!"],[3.57,2,2,64,2,1,8,0,19.29144257300035,7438.92,119022.72,16.49,"GPU sipping coffee","meh..."],[8.86,64,2,1,128,1,4,0,18.68557929973905,3100.55,1587481.6,57.76,"GPU sipping coffee","MOARRR!!!!"],[8.86,16,2,64,2,32,2,1,18.65647112602221,3095.72,396252.16,60.94,"GPU sipping coffee","meh..."],[3.57,16,1,32,8,16,1,1,19.19610418244957,6754.32,864552.96,22.97,"GPU sipping coffee","moar?"],[1.34,8,16,8,2,4,8,1,18.213458328965046,15982.37,1022871.68,24.26,"GPU sipping coffee","meh..."],[1.34,8,8,1,16,2,2,1,18.20500252042095,15974.95,1022396.8,25.75,"GPU sipping coffee","meh..."],[8.86,16,8,32,1,16,8,0,18.55606901957863,3079.06,394119.68,52.12,"GPU sipping coffee","meh..."],[1.34,16,8,1,32,2,2,0,18.12104842130456,15901.28,2035363.84,29.22,"GPU sipping coffee","moar?"],[8.86,32,2,1,128,2,1,1,18.44698869985301,3060.96,783605.76,65.7,"GPU sipping coffee","moar?"],[1.34,16,1,16,16,8,1,1,18.03198892835022,15823.13,2025360.64,14.49,"GPU sipping coffee","moar?"],[3.57,16,4,32,2,16,4,1,18.94404291204167,6665.63,853200.64,24,"GPU sipping coffee","moar?"],[80,32,1,32,8,4,8,1,13.889290441942324,268.18,68654.08,48.58,"GPU sipping coffee","moar?"],[1.34,16,1,8,32,4,1,0,17.943749945119837,15745.7,2015449.6,20.83,"GPU sipping coffee","moar?"],[1.34,16,4,1,64,2,1,1,17.900012218445653,15707.32,2010536.96,24.13,"GPU sipping coffee","moar?"],[3.57,16,2,32,4,16,2,1,18.846049084060343,6631.15,848787.2,23.36,"GPU sipping coffee","moar?"],[3.57,8,1,64,4,8,2,1,18.838801860703256,6628.6,424230.4,15.12,"GPU sipping coffee","meh..."],[3.57,8,1,32,8,4,2,1,18.80563515616317,6616.93,423483.52,15.57,"GPU sipping coffee","meh..."],[3.57,16,8,8,4,8,4,1,18.79767742071225,6614.13,846608.64,44.57,"GPU sipping coffee","moar?"],[3.57,32,2,1,128,1,2,1,18.759346759746784,7233.74,1851837.44,32.52,"GPU sipping coffee","MOAR!"],[3.57,32,1,2,128,1,2,0,18.753511807120695,7231.49,1851261.44,44.34,"GPU sipping coffee","MOAR!"],[1.34,32,4,2,32,1,8,0,17.737182184055587,18177.16,4653352.96,10.13,"GPU sipping coffee","MOAR!"],[1.34,2,4,32,2,2,4,1,17.725346210958627,15554.05,248864.8,11.37,"GPU sipping coffee","meh..."],[1.34,16,8,1,32,2,2,1,17.701209549912267,15532.87,1988207.36,24.75,"GPU sipping coffee","moar?"],[80,8,1,256,1,8,8,0,13.662964096833488,263.81,16883.84,46.82,"GPU sipping coffee","meh..."],[3.57,32,4,2,32,1,8,1,18.58206793240487,7165.38,1834337.28,17.54,"GPU sipping coffee","MOAR!"],[1.34,16,2,16,8,8,2,1,17.515705976487993,15370.09,1967371.52,13.62,"GPU sipping coffee","moar?"],[1.34,4,4,16,4,2,4,1,17.467193339059122,15327.52,490480.64,10.68,"GPU sipping coffee","meh..."],[80,16,4,16,4,2,16,1,13.503448045818564,260.73,33373.44,62.44,"GPU sipping coffee","meh..."],[3.57,32,2,8,16,8,2,1,18.351902133041957,6457.28,1653063.68,23.51,"GPU sipping coffee","MOAR!"],[1.34,8,4,8,8,8,1,1,17.397313260902244,15266.2,977036.8,43.89,"GPU sipping coffee","meh..."],[3.57,32,2,2,64,1,4,1,18.149736842496,6998.67,1791659.52,17.6,"GPU sipping coffee","MOAR!"],[80,32,1,64,4,8,8,1,13.323733533798505,257.26,65858.56,37.3,"GPU sipping coffee","moar?"],[3.57,16,1,64,4,16,2,1,18.119593098842596,6375.54,816069.12,13.64,"GPU sipping coffee","moar?"],[3.57,16,1,8,32,1,4,0,18.081895793296695,6972.51,892481.28,22.99,"GPU sipping coffee","moar?"],[8.86,16,1,256,1,32,4,0,17.48207986055636,2900.85,371308.8,19.66,"GPU sipping coffee","meh..."],[8.86,16,16,1,16,1,8,1,17.451404787219158,2895.76,370657.28,52.92,"GPU sipping coffee","meh..."],[3.57,16,2,8,16,1,8,0,17.929175633229914,6913.62,884943.36,14.62,"GPU sipping coffee","moar?"],[3.57,16,16,1,16,2,4,1,17.907320505689306,6300.85,806508.8,46.56,"GPU sipping coffee","moar?"],[8.86,32,8,2,16,1,16,1,17.29121943372153,2869.18,734510.08,33.73,"GPU sipping coffee","moar?"],[1.34,32,8,1,32,1,8,1,16.891343545853413,17310.34,4431447.04,15.28,"GPU sipping coffee","MOAR!"],[3.57,16,1,32,8,8,2,1,17.684390231128525,6222.41,796468.48,14.21,"GPU sipping coffee","moar?"],[80,4,1,256,1,4,8,0,12.982583825285824,281.69,9014.08,60.18,"GPU sipping coffee","meh..."],[8.86,32,1,64,4,32,2,1,17.116992249010604,2840.27,727109.12,33.44,"GPU sipping coffee","moar?"],[80,16,1,64,4,4,8,1,12.971037330093427,250.45,32057.6,50.46,"GPU sipping coffee","meh..."],[3.57,16,8,4,8,4,4,0,17.590545793775174,6189.39,792241.92,43.74,"GPU sipping coffee","moar?"],[1.34,8,32,4,2,4,8,1,16.677646461214543,14634.69,936620.16,46.13,"GPU sipping coffee","meh..."],[3.57,16,1,8,32,1,4,1,17.550292709376002,6767.52,866242.56,12.3,"GPU sipping coffee","moar?"],[3.57,16,16,2,8,2,8,0,17.477602791339613,6149.65,787155.2,43.82,"GPU sipping coffee","moar?"],[8.86,32,8,2,16,1,16,0,16.903833635270537,2804.9,718054.4,40.37,"GPU sipping coffee","moar?"],[80,4,1,256,1,4,8,0,12.817885526413589,251.32,8042.24,66.31,"GPU sipping coffee","meh..."],[3.57,32,1,2,128,1,2,1,17.420212165498434,6717.36,1719644.16,23.92,"GPU sipping coffee","MOAR!"],[80,64,4,4,16,1,32,1,12.782000451455612,246.8,126361.6,54.1,"GPU sipping coffee","MOAR!"],[1.34,32,4,1,64,1,4,1,16.514559815241512,16924.21,4332597.76,15.7,"GPU sipping coffee","MOAR!"],[1.34,8,8,8,4,8,2,1,16.47748767244295,14459.05,925379.2,49.23,"GPU sipping coffee","meh..."],[80,8,1,128,2,4,8,1,12.746264712754217,246.11,15751.04,61.77,"GPU sipping coffee","meh..."],[1.34,2,32,8,1,1,16,0,16.462706067942115,16871.07,269937.12,48.82,"GPU sipping coffee","meh..."],[1.34,8,16,2,8,1,8,0,16.37170342448819,16777.81,1073779.84,25.64,"GPU sipping coffee","meh..."],[80,8,1,256,1,8,8,0,12.64890528896906,274.45,17564.8,41.67,"GPU sipping coffee","meh..."],[1.34,8,16,16,1,8,8,0,16.324040958094884,14324.4,916761.6,32.26,"GPU sipping coffee","meh..."],[3.57,16,1,4,64,2,1,0,17.132293913737186,6028.15,771603.2,59.64,"GPU sipping coffee","moar?"],[3.57,2,8,32,1,2,8,0,17.061327965805233,6003.18,96050.88,22.71,"GPU sipping coffee","meh..."],[8.86,16,2,64,2,16,4,1,16.513856425908934,2740.19,350744.32,26.71,"GPU sipping coffee","meh..."],[1.34,32,2,2,64,1,4,0,16.166932839791563,16567.96,4241397.76,12.3,"GPU sipping coffee","MOAR!"],[1.34,16,16,1,16,2,4,1,16.157545859942246,14178.3,1814822.4,21.32,"GPU sipping coffee","moar?"],[80,16,4,32,2,4,16,1,12.475916080839312,240.89,30833.92,60,"GPU sipping coffee","meh..."],[80,16,1,256,1,16,8,0,12.413249060797735,239.68,30679.04,36.72,"GPU sipping coffee","meh..."],[3.57,16,1,16,16,4,2,1,16.85417105972753,5930.29,759077.12,14.87,"GPU sipping coffee","moar?"],[3.57,16,2,8,16,1,8,1,16.848153410036872,6496.77,831586.56,10.58,"GPU sipping coffee","moar?"],[8.86,32,16,1,16,1,16,0,16.317632381142843,2707.63,693153.28,53.5,"GPU sipping coffee","moar?"],[8.86,32,1,8,32,8,1,0,16.29973356624864,2704.66,692392.96,65.88,"GPU sipping coffee","moar?"],[3.57,8,8,8,4,2,8,1,16.815007633115503,5916.51,378656.64,20.04,"GPU sipping coffee","meh..."],[3.57,16,16,4,4,4,8,1,16.80372470106545,5912.54,756805.12,47.34,"GPU sipping coffee","moar?"],[3.57,8,1,128,2,16,2,1,16.787013456618517,5906.66,378026.24,14.56,"GPU sipping coffee","meh..."],[8.86,16,16,4,4,4,8,1,16.23844368494424,2694.49,344894.72,57.49,"GPU sipping coffee","meh..."],[3.57,32,4,8,8,8,4,1,16.757171948677566,5896.16,1509416.96,22.71,"GPU sipping coffee","MOAR!"],[1.34,8,16,8,2,8,4,1,15.889774790723042,13943.33,892373.12,53.42,"GPU sipping coffee","meh..."],[8.86,8,4,16,4,2,8,1,16.167812671287653,2682.77,171697.28,21.01,"GPU sipping coffee","meh..."],[8.86,2,1,32,8,2,1,1,16.158592069675485,2681.24,42899.84,64.84,"GPU sipping coffee","meh..."],[3.57,16,2,16,8,16,1,1,16.63379862870455,5852.75,749152,38.64,"GPU sipping coffee","moar?"],[80,32,1,128,2,16,8,1,12.218515180337958,235.92,60395.52,35.39,"GPU sipping coffee","moar?"],[1.34,16,16,2,8,2,8,1,15.709296839357997,13784.96,1764474.88,19.51,"GPU sipping coffee","moar?"],[8.86,4,16,8,2,1,16,1,16.014015447011527,2657.25,85032,36.46,"GPU sipping coffee","meh..."],[1.34,8,2,2,64,1,1,0,15.667989330775232,16056.64,1027624.96,50.16,"GPU sipping coffee","meh..."],[80,8,1,256,1,8,8,0,12.113034428311426,237.5,15200,45.41,"GPU sipping coffee","meh..."],[3.57,2,16,16,1,1,16,0,16.397272716665398,6308.15,100930.4,45.07,"GPU sipping coffee","meh..."],[8.86,32,1,8,32,1,8,0,15.82815704588787,2626.41,672360.96,25.4,"GPU sipping coffee","moar?"],[80,32,1,16,16,2,8,1,11.99995747407725,231.7,59315.2,52.74,"GPU sipping coffee","moar?"],[8.86,4,4,32,2,2,8,1,15.80073630253143,2621.86,83899.52,22.42,"GPU sipping coffee","meh..."],[3.57,4,8,32,1,4,8,0,16.269959595694978,5724.73,183191.36,27.94,"GPU sipping coffee","meh..."],[1.34,16,16,4,4,4,8,1,15.369696981387518,13486.96,1726330.88,23.34,"GPU sipping coffee","moar?"],[1.34,32,1,1,256,1,1,0,15.320352597375097,15700.38,4019297.28,36.65,"GPU sipping coffee","MOAR!"],[80,16,1,128,2,8,8,1,11.819725052800647,228.22,29212.16,40.54,"GPU sipping coffee","meh..."],[80,8,1,128,2,4,8,0,11.777838391678058,255.55,16355.2,62.93,"GPU sipping coffee","meh..."],[3.57,32,2,4,32,1,8,0,15.969954140126609,6158.13,1576481.28,15.35,"GPU sipping coffee","MOAR!"],[1.34,8,4,16,4,4,4,1,15.139179332829574,13284.68,850219.52,11.5,"GPU sipping coffee","meh..."],[1.34,32,1,2,128,1,2,0,15.08526406144414,15459.46,3957621.76,19.52,"GPU sipping coffee","MOAR!"],[3.57,16,4,16,4,16,2,1,15.857208910146008,5579.5,714176,42.54,"GPU sipping coffee","moar?"],[1.34,4,4,32,2,4,4,1,14.913425199057926,13086.58,418770.56,13.78,"GPU sipping coffee","meh..."],[3.57,16,32,1,8,2,8,1,15.68972699938789,5520.57,706632.96,60.06,"GPU sipping coffee","moar?"],[1.34,16,32,1,8,2,8,1,14.887852645994643,13064.14,1672209.92,28.54,"GPU sipping coffee","moar?"],[1.34,16,32,1,8,2,8,0,14.886997948365524,13063.39,1672113.92,32.05,"GPU sipping coffee","moar?"],[1.34,8,4,8,8,2,4,1,14.719306273532116,12916.24,826639.36,11.38,"GPU sipping coffee","meh..."],[1.34,32,4,2,32,1,8,1,14.704489329205323,15069.24,3857725.44,9.06,"GPU sipping coffee","MOAR!"],[8.86,32,1,32,8,32,1,1,14.987214072710342,2486.87,636638.72,59.71,"GPU sipping coffee","moar?"],[8.86,32,1,8,32,1,8,1,14.902963085430551,2472.89,633059.84,14.2,"GPU sipping coffee","moar?"],[3.57,16,8,16,2,16,4,1,15.335579351338192,5395.96,690682.88,44.58,"GPU sipping coffee","moar?"],[3.57,16,16,8,2,8,8,1,15.301446350350496,5383.95,689145.6,52.47,"GPU sipping coffee","moar?"],[3.57,32,1,32,8,16,2,1,15.19881998351738,5347.84,1369047.04,13.04,"GPU sipping coffee","MOAR!"],[1.34,32,2,2,64,1,4,1,14.381618273382504,14738.36,3773020.16,8.83,"GPU sipping coffee","MOAR!"],[1.34,8,32,8,1,8,8,0,14.260504586220328,12513.64,800872.96,63.87,"GPU sipping coffee","meh..."],[3.57,32,1,16,16,16,1,1,14.826568487316838,5216.86,1335516.16,22.05,"GPU sipping coffee","MOAR!"],[1.34,1,1,128,2,1,4,0,14.020027671213429,14367.8,114942.4,10.39,"GPU sipping coffee","meh..."],[3.57,2,2,64,2,2,4,1,14.764810776120946,5195.13,83122.08,15.35,"GPU sipping coffee","meh..."],[3.57,32,1,4,64,1,4,0,14.684552976283824,5662.47,1449592.32,22.9,"GPU sipping coffee","MOAR!"],[1.34,2,1,64,4,1,4,0,13.91532486569545,14260.5,228168,10.34,"GPU sipping coffee","meh..."],[1.34,16,4,8,8,4,4,1,13.874238237699885,12174.69,1558360.32,11.42,"GPU sipping coffee","moar?"],[3.57,16,16,2,8,1,16,1,14.578721165845224,5608.54,717893.12,43.3,"GPU sipping coffee","moar?"],[3.57,16,1,8,32,2,2,0,14.551003476844972,5119.9,655347.2,29.7,"GPU sipping coffee","meh..."],[1.34,2,1,64,4,2,2,1,13.714842827853136,12034.82,192557.12,10.19,"GPU sipping coffee","meh..."],[3.57,32,2,4,32,1,8,1,14.444152928089046,5569.77,1425861.12,8.97,"GPU sipping coffee","MOAR!"],[1.34,1,1,128,2,1,4,1,13.707060934831498,14047.07,112376.56,8.24,"GPU sipping coffee","meh..."],[3.57,32,2,1,128,2,1,1,14.423367084308966,5074.99,1299197.44,33.44,"GPU sipping coffee","moar?"],[1.34,1,1,256,1,2,4,0,13.689931240956376,12012.96,96103.68,7.63,"GPU sipping coffee","meh..."],[8.86,16,4,8,8,1,16,0,13.93130452469232,2311.66,295892.48,25.88,"GPU sipping coffee","meh..."],[8.86,16,4,8,8,1,16,1,13.9269654180513,2310.94,295800.32,19.74,"GPU sipping coffee","meh..."],[3.57,16,16,2,8,1,16,0,14.351535440167083,5521.14,706705.92,45.71,"GPU sipping coffee","moar?"],[1.34,32,2,1,128,1,2,1,13.614584840899171,13952.3,3571788.8,17.48,"GPU sipping coffee","MOAR!"],[3.57,32,1,1,256,1,1,1,14.310441747021914,5518.21,1412661.76,45.58,"GPU sipping coffee","MOAR!"],[8.86,8,16,4,4,1,16,0,13.84614955686232,2297.53,147041.92,41.5,"GPU sipping coffee","meh..."],[80,16,1,64,4,4,8,0,10.428836905767604,226.28,28963.84,63,"GPU sipping coffee","meh..."],[1.34,16,16,8,2,8,8,1,13.461875121570298,11812.84,1512043.52,32.16,"GPU sipping coffee","meh..."],[3.57,16,8,32,1,16,8,0,14.178354093871889,4988.78,638563.84,27.36,"GPU sipping coffee","meh..."],[1.34,4,1,32,8,1,4,1,13.446865193066644,13780.42,440973.44,6.15,"GPU sipping coffee","meh..."],[8.86,2,1,128,2,2,4,0,13.712420700800903,2275.34,36405.44,30.59,"GPU sipping coffee","meh..."],[3.57,32,4,16,4,16,4,1,14.13765596113718,4974.46,1273461.76,23.23,"GPU sipping coffee","moar?"],[1.34,2,1,64,4,1,4,1,13.42082122401469,13753.73,220059.68,6.85,"GPU sipping coffee","meh..."],[3.57,16,8,16,2,8,8,1,14.086982238605072,4956.63,634448.64,26.91,"GPU sipping coffee","meh..."],[80,32,1,32,8,4,8,0,10.291954964764733,223.31,57167.36,62.71,"GPU sipping coffee","moar?"],[8.86,4,16,16,1,2,16,0,13.481724864386718,2237.06,71585.92,37.46,"GPU sipping coffee","meh..."],[8.86,4,1,64,4,2,4,1,13.474010897024908,2235.78,71544.96,20.92,"GPU sipping coffee","meh..."],[1.34,2,16,16,1,1,16,0,13.178384951592122,13505.28,216084.48,24.67,"GPU sipping coffee","meh..."],[1.34,4,1,32,8,2,2,1,13.16116970370899,11548.97,369567.04,9.26,"GPU sipping coffee","meh..."],[1.34,8,1,16,16,1,4,0,13.143490521719588,13469.52,862049.28,10.39,"GPU sipping coffee","meh..."],[3.57,16,1,256,1,32,4,0,13.802748980588458,4856.62,621647.36,3.36,"GPU sipping coffee","meh..."],[1.34,8,4,32,2,8,4,1,13.09724971701918,11492.88,735544.32,14.99,"GPU sipping coffee","meh..."],[1.34,4,1,32,8,1,4,0,13.095393585242595,13420.23,429447.36,10.42,"GPU sipping coffee","meh..."],[3.57,8,8,32,1,8,8,0,13.775380054734043,4846.99,310207.36,27.32,"GPU sipping coffee","meh..."],[80,16,1,128,2,8,8,0,10.103915126619372,219.23,28061.44,44.26,"GPU sipping coffee","meh..."],[1.34,32,2,8,16,8,2,1,13.042298357450935,11444.66,2929832.96,13.58,"GPU sipping coffee","moar?"],[3.57,16,16,16,1,16,8,0,13.734056671356765,4832.45,618553.6,53.5,"GPU sipping coffee","meh..."],[8.86,2,1,256,1,4,4,0,13.292491602542269,2205.66,35290.56,20.69,"GPU sipping coffee","meh..."],[1.34,2,2,64,2,1,8,0,12.902195929469023,13222.24,211555.84,7.93,"GPU sipping coffee","meh..."],[3.57,8,2,32,4,4,4,1,13.585730166648364,4780.26,305936.64,13.65,"GPU sipping coffee","meh..."],[3.57,32,1,4,64,1,4,1,13.57339646441739,5234,1339904,12.24,"GPU sipping coffee","MOAR!"],[1.34,16,32,1,8,1,16,0,12.861153990978003,13180.18,1687063.04,41,"GPU sipping coffee","moar?"],[1.34,16,4,16,4,8,4,1,12.829638191357413,11258.05,1441030.4,14.19,"GPU sipping coffee","meh..."],[8.86,32,1,4,64,2,2,0,13.046970485104598,2164.92,554219.52,62.76,"GPU sipping coffee","moar?"],[8.86,32,4,4,16,1,16,1,13.035459799431896,2163.01,553730.56,19.28,"GPU sipping coffee","moar?"],[1.34,32,1,2,128,2,1,1,12.775450361671185,11210.5,2869888,16.21,"GPU sipping coffee","moar?"],[1.34,16,4,4,16,2,4,1,12.768168337871078,11204.11,1434126.08,10.85,"GPU sipping coffee","meh..."],[80,32,1,64,4,8,8,0,9.874856794368776,214.26,54850.56,43.52,"GPU taking a nap","moar?"],[8.86,64,1,2,128,1,4,1,12.996588635772763,2156.56,1104158.72,26.08,"GPU sipping coffee","MOAR!"],[8.86,16,32,2,4,2,16,1,12.987850157120713,2155.11,275854.08,65.48,"GPU sipping coffee","meh..."],[1.34,32,1,2,128,1,2,1,12.730202299454124,13045.98,3339770.88,11.96,"GPU sipping coffee","MOAR!"],[1.34,2,2,64,2,1,8,1,12.728182403765192,13043.91,208702.56,6.83,"GPU sipping coffee","meh..."],[8.86,4,1,128,2,4,4,1,12.982908396779552,2154.29,68937.28,18.82,"GPU sipping coffee","meh..."],[8.86,8,1,32,8,2,4,1,12.97826796328846,2153.52,137825.28,19.75,"GPU sipping coffee","meh..."],[3.57,4,8,16,2,2,8,1,13.37985218233956,4707.82,150650.24,20.92,"GPU sipping coffee","meh..."],[1.34,2,1,128,2,4,2,1,12.694425026430576,11139.4,178230.4,9.27,"GPU sipping coffee","meh..."],[1.34,16,1,4,64,2,1,0,12.665490662692756,11114.01,1422593.28,26.8,"GPU sipping coffee","meh..."],[8.86,16,4,8,8,2,8,1,12.875937365004424,2136.54,273477.12,20.09,"GPU sipping coffee","meh..."],[3.57,8,2,16,8,2,4,1,13.291976046574396,4676.9,299321.6,13.29,"GPU sipping coffee","meh..."],[3.57,4,2,64,2,4,4,1,13.27822053243781,4672.06,149505.92,14.19,"GPU sipping coffee","meh..."],[8.86,16,4,4,16,2,4,1,12.835921159315026,2129.9,272627.2,41.57,"GPU sipping coffee","meh..."],[8.86,32,4,4,16,1,16,0,12.835439036354911,2129.82,545233.92,25.6,"GPU sipping coffee","moar?"],[8.86,8,16,16,1,4,16,0,12.81723889461064,2126.8,136115.2,49.71,"GPU sipping coffee","meh..."],[1.34,8,1,32,8,4,2,1,12.533297429388814,10998.01,703872.64,7.49,"GPU sipping coffee","meh..."],[3.57,32,1,8,32,8,1,0,13.187729712167345,4640.22,1187896.32,33.53,"GPU sipping coffee","moar?"],[1.34,8,1,16,16,1,4,1,12.494547802412264,12804.48,819486.72,6.63,"GPU sipping coffee","meh..."],[8.86,16,16,4,4,2,16,1,12.615470435803244,2093.32,267944.96,35,"GPU sipping coffee","meh..."],[8.86,8,16,8,2,2,16,1,12.605647180490935,2091.69,133868.16,37.31,"GPU sipping coffee","meh..."],[3.57,32,16,1,16,1,16,1,12.985769719505669,4995.72,1278904.32,38.13,"GPU sipping coffee","moar?"],[3.57,8,8,16,2,4,8,1,12.94112517464008,4553.45,291420.8,26.31,"GPU sipping coffee","meh..."],[8.86,16,1,16,16,2,4,1,12.48680387082302,2071.97,265212.16,18.8,"GPU sipping coffee","meh..."],[80,32,1,256,1,32,8,0,9.440705991591992,204.84,52439.04,27.44,"GPU taking a nap","moar?"],[8.86,32,8,1,32,2,4,1,12.430877607449885,2062.69,528048.64,41.15,"GPU sipping coffee","moar?"],[3.57,16,8,4,8,2,8,1,12.802546895859056,4504.69,576600.32,19,"GPU sipping coffee","meh..."],[3.57,16,8,4,8,2,8,0,12.755766779601146,4488.23,574493.44,23,"GPU sipping coffee","meh..."],[8.86,8,4,32,2,4,8,1,12.336140445787631,2046.97,131006.08,25.31,"GPU sipping coffee","meh..."],[8.86,16,4,16,4,4,8,1,12.271234642282389,2036.2,260633.6,23.24,"GPU sipping coffee","meh..."],[1.34,16,2,8,16,1,8,0,12.015315352775833,12313.36,1576110.08,6.99,"GPU sipping coffee","moar?"],[1.34,16,1,8,32,1,4,0,11.982938474051815,12280.18,1571863.04,9.28,"GPU sipping coffee","moar?"],[1.34,16,1,8,32,1,4,1,11.972097391392872,12269.07,1570440.96,6.09,"GPU sipping coffee","moar?"],[3.57,32,16,1,16,1,16,0,12.537740752939486,4823.36,1234780.16,42.4,"GPU sipping coffee","moar?"],[3.57,16,2,32,4,8,4,1,12.459085349700594,4383.84,561131.52,12.63,"GPU sipping coffee","meh..."],[3.57,4,32,8,1,2,16,0,12.443340401701274,4378.3,140105.6,50.6,"GPU sipping coffee","meh..."],[8.86,4,16,16,1,1,32,0,12.019540660703129,1992.43,63757.76,46.29,"GPU sipping coffee","meh..."],[80,32,4,8,8,2,16,1,9.094486544876846,175.6,44953.6,58.42,"GPU taking a nap","moar?"],[1.34,16,1,16,16,4,2,1,11.743203545061595,10304.7,1319001.6,7.67,"GPU sipping coffee","meh..."],[1.34,8,1,16,16,2,2,1,11.73422352197164,10296.82,658996.48,8.92,"GPU sipping coffee","meh..."],[3.57,32,16,1,16,2,8,1,12.269492302582425,4317.13,1105185.28,25.45,"GPU sipping coffee","moar?"],[8.86,32,1,64,64,1,4,0,11.882523005691082,1971.7,504755.2,49.54,"GPU sipping coffee","moar?"],[80,32,1,128,2,16,8,0,9.00563342490273,195.4,50022.4,33.4,"GPU taking a nap","moar?"],[1.34,4,1,64,4,4,2,1,11.60208726850964,10180.87,325787.84,8.88,"GPU sipping coffee","meh..."],[1.34,16,2,8,16,1,8,1,11.578383617204675,11865.59,1518795.52,5.07,"GPU sipping coffee","meh..."],[80,16,1,256,1,16,8,0,8.851698851519028,192.06,24583.68,32.36,"GPU taking a nap","meh..."],[3.57,16,8,8,4,4,8,1,12.020358342001115,4229.47,541372.16,26.69,"GPU sipping coffee","meh..."],[8.86,32,1,2,128,2,1,1,11.522075827636415,1911.89,489443.84,65.85,"GPU sipping coffee","moar?"],[3.57,32,8,2,16,2,8,1,11.870838176975075,4176.86,1069276.16,19.34,"GPU sipping coffee","moar?"],[8.86,16,16,8,2,4,16,1,11.497427291300626,1907.8,244198.4,49.16,"GPU sipping coffee","meh..."],[8.86,16,1,32,8,4,4,1,11.451987202309953,1900.26,243233.28,15.13,"GPU sipping coffee","meh..."],[8.86,4,1,128,2,4,4,0,11.44397190809807,1898.93,60765.76,22.11,"GPU sipping coffee","meh..."],[1.34,16,1,8,32,2,2,1,11.18876689103544,9818.18,1256727.04,8.96,"GPU sipping coffee","meh..."],[3.57,4,2,32,4,2,4,1,11.782279949599838,4145.7,132662.4,14.3,"GPU sipping coffee","meh..."],[1.34,2,8,32,1,2,8,0,11.18235096083284,9812.55,157000.8,10.84,"GPU sipping coffee","meh..."],[8.86,32,1,16,16,4,4,1,11.382260169203576,1888.69,483504.64,14.43,"GPU sipping coffee","moar?"],[3.57,32,8,8,4,8,8,1,11.750676371666184,4134.58,1058452.48,26.79,"GPU sipping coffee","moar?"],[8.86,16,4,32,2,8,8,1,11.367856745770192,1886.3,241446.4,24.16,"GPU sipping coffee","meh..."],[8.86,32,2,2,64,2,2,1,11.363517639129173,1885.58,482708.48,46.83,"GPU sipping coffee","moar?"],[1.34,16,16,2,8,1,16,1,11.123877813344809,11399.81,1459175.68,25.33,"GPU sipping coffee","meh..."],[1.34,32,2,1,128,2,1,1,11.10984980994666,9748.93,2495726.08,16.14,"GPU sipping coffee","moar?"],[3.57,32,8,4,8,4,8,1,11.676896795842651,4108.62,1051806.72,24.62,"GPU sipping coffee","moar?"],[8.86,32,16,1,16,2,8,1,11.309941725186588,1876.69,480432.64,39.36,"GPU sipping coffee","moar?"],[1.34,32,1,1,256,1,1,1,11.052586229270778,11326.75,2899648,21.67,"GPU sipping coffee","moar?"],[3.57,32,1,4,64,4,1,0,11.639268075353302,4095.38,1048417.28,42.35,"GPU sipping coffee","moar?"],[8.86,8,1,64,4,4,4,1,11.26847915061685,1869.81,119667.84,16.59,"GPU sipping coffee","meh..."],[3.57,16,2,16,8,4,4,1,11.446605615989776,4027.59,515531.52,12.93,"GPU sipping coffee","meh..."],[3.57,16,2,8,16,2,4,1,11.41733251772389,4017.29,514213.12,13.53,"GPU sipping coffee","meh..."],[80,64,2,8,16,1,32,1,8.333677847016704,160.91,82385.92,39.43,"GPU taking a nap","moar?"],[1.34,32,1,4,64,1,4,0,10.746801344227269,11013.38,2819425.28,9.82,"GPU sipping coffee","moar?"],[1.34,32,2,4,32,1,8,0,10.737219037142584,11003.56,2816911.36,6.98,"GPU sipping coffee","moar?"],[1.34,8,1,64,4,8,2,1,10.718250148220504,9405.3,601939.2,8.38,"GPU sipping coffee","meh..."],[1.34,4,8,32,1,4,8,0,10.700176142356709,9389.44,300462.08,12.38,"GPU sipping coffee","meh..."],[3.57,32,8,16,2,16,8,1,11.262014574012182,3962.64,1014435.84,27.32,"GPU sipping coffee","moar?"],[3.57,8,32,4,2,2,16,1,11.241182359421025,3955.31,253139.84,58.32,"GPU sipping coffee","meh..."],[80,64,2,8,16,1,32,0,8.240454180839151,159.11,81464.32,61.91,"GPU taking a nap","moar?"],[1.34,4,8,16,2,2,8,1,10.64994271370121,9345.36,299051.52,10.29,"GPU sipping coffee","meh..."],[3.57,16,8,4,8,1,16,0,11.185153883870608,4303.01,550785.28,24.99,"GPU sipping coffee","meh..."],[1.34,4,1,128,2,8,2,1,10.518752325615434,9230.24,295367.68,8.3,"GPU sipping coffee","meh..."],[8.86,32,4,2,32,2,4,1,10.58573277372648,1756.52,449669.12,38.15,"GPU sipping coffee","moar?"],[3.57,8,2,64,2,8,4,1,10.915597297543377,3840.75,245808,13.6,"GPU sipping coffee","meh..."],[80,32,2,16,8,4,8,1,8.011020380191061,154.68,39598.08,60.64,"GPU taking a nap","meh..."],[8.86,16,16,16,1,8,16,0,10.514318310259704,1744.67,223317.76,58.22,"GPU sipping coffee","meh..."],[3.57,1,1,256,1,1,8,0,10.74588215430678,4143.69,33149.52,11.71,"GPU sipping coffee","meh..."],[3.57,32,1,8,32,4,2,0,10.745813327601246,3781.01,967938.56,22.15,"GPU sipping coffee","moar?"],[1.34,16,1,32,8,8,2,1,10.16748299601537,8922,1142016,8.08,"GPU sipping coffee","meh..."],[8.86,8,1,128,2,8,4,1,10.34256200571936,1716.17,109834.88,16.48,"GPU sipping coffee","meh..."],[1.34,16,16,2,8,1,16,0,10.12743497193998,10378.65,1328467.2,26.3,"GPU sipping coffee","meh..."],[3.57,4,1,64,4,1,8,0,10.622570155475476,4096.14,131076.48,12.34,"GPU sipping coffee","meh..."],[80,64,4,4,16,2,16,1,7.799713403521942,150.6,77107.2,56.4,"GPU taking a nap","moar?"],[3.57,8,1,32,8,1,8,0,10.501358739589564,4049.4,259161.6,12.36,"GPU sipping coffee","meh..."],[3.57,2,1,128,2,1,8,0,10.496198048155826,4047.41,64758.56,12.49,"GPU sipping coffee","meh..."],[3.57,2,1,128,2,1,8,1,10.493552869632,4046.39,64742.24,9,"GPU sipping coffee","meh..."],[3.57,64,2,1,128,1,4,1,10.490648359880348,4045.27,2071178.24,16.42,"GPU sipping coffee","MOAR!"],[1.34,8,8,16,2,4,8,1,9.90634438039814,8692.85,556342.4,13.2,"GPU taking a nap","meh..."],[3.57,4,1,64,4,1,8,1,10.408984956237912,4013.78,128440.96,7.82,"GPU sipping coffee","meh..."],[1.34,16,8,4,8,2,8,1,9.87068639531124,8661.56,1108679.68,9.77,"GPU taking a nap","meh..."],[1.34,32,2,4,32,1,8,1,9.778676074268214,10021.24,2565437.44,4.55,"GPU taking a nap","moar?"],[3.57,8,1,32,8,1,8,1,10.249833381721045,3952.41,252954.24,7.5,"GPU sipping coffee","meh..."],[1.34,8,64,2,2,2,16,1,9.716931989816688,8526.64,545704.96,49.19,"GPU taking a nap","meh..."],[3.57,8,32,8,1,4,16,0,10.204147327747364,3590.42,229786.88,65.06,"GPU sipping coffee","meh..."],[1.34,4,32,4,2,1,16,0,9.641440262823217,9880.6,316179.2,25.26,"GPU taking a nap","meh..."],[1.34,32,16,1,16,2,8,1,9.622448015909534,8443.73,2161594.88,12.09,"GPU taking a nap","moar?"],[3.57,32,8,2,16,1,16,0,10.11727699802286,3892.19,996400.64,24.58,"GPU sipping coffee","moar?"],[3.57,16,16,2,8,2,8,1,10.115645941339633,3559.28,455587.84,35.55,"GPU sipping coffee","meh..."],[1.34,32,16,1,16,1,16,1,9.53869880529304,9775.31,2502479.36,19.03,"GPU taking a nap","moar?"],[3.57,16,32,1,8,1,16,0,10.00415202340601,3848.67,492629.76,61.91,"GPU sipping coffee","meh..."],[8.86,32,8,2,16,2,8,1,9.62980347456194,1597.9,409062.4,33.28,"GPU taking a nap","moar?"],[3.57,32,2,32,4,16,4,1,9.92338136874865,3491.63,893857.28,12.54,"GPU taking a nap","moar?"],[8.86,64,2,1,128,1,4,1,9.604612549896023,1593.72,815984.64,31.75,"GPU taking a nap","MOAR!"],[3.57,32,2,16,8,8,4,1,9.91212685718235,3487.67,892843.52,12.13,"GPU taking a nap","moar?"],[1.34,32,1,8,32,8,1,0,9.407611219853848,8255.21,2113333.76,19.42,"GPU taking a nap","moar?"],[1.34,32,1,4,64,1,4,1,9.395686287336703,9628.75,2464960,5.55,"GPU taking a nap","moar?"],[3.57,16,1,16,16,1,8,0,9.894757064581563,3815.49,488382.72,12.08,"GPU taking a nap","meh..."],[3.57,16,1,16,16,1,8,1,9.80881469568,3782.35,484140.8,7.4,"GPU taking a nap","meh..."],[1.34,2,2,64,2,2,4,1,9.292876610120736,8154.53,130472.48,7.27,"GPU taking a nap","meh..."],[1.34,16,1,8,32,2,2,0,9.262027723693686,8127.46,1040314.88,13.47,"GPU taking a nap","meh..."],[1.34,8,32,2,4,1,16,1,9.242105909326687,9471.36,606167.04,24.89,"GPU taking a nap","meh..."],[80,64,1,8,32,2,8,1,7.134717918122063,137.76,70533.12,50.21,"GPU taking a nap","moar?"],[80,64,2,8,16,4,8,1,7.084480720237492,136.79,70036.48,59.64,"GPU taking a nap","moar?"],[1.34,4,32,8,1,2,16,0,9.143703383918268,8023.63,256756.16,19.37,"GPU taking a nap","meh..."],[3.57,4,32,8,1,1,32,0,9.602366009469282,3694.1,118211.2,62.92,"GPU taking a nap","meh..."],[8.86,8,1,256,1,16,4,0,9.24374351425126,1533.84,98165.76,16.22,"GPU taking a nap","meh..."],[80,8,2,32,4,1,16,1,6.967951137515551,134.54,8610.56,64.25,"GPU taking a nap","meh..."],[1.34,8,32,4,2,2,16,1,8.983293732884967,7882.87,504503.68,19.92,"GPU taking a nap","meh..."],[1.34,32,16,1,16,1,16,0,8.959466882129277,9181.71,2350517.76,19.52,"GPU taking a nap","moar?"],[8.86,16,2,16,8,1,16,0,9.137073809326203,1516.14,194065.92,18.13,"GPU taking a nap","meh..."],[8.86,32,1,32,8,8,4,1,9.13442213304558,1515.7,388019.2,14.13,"GPU taking a nap","meh..."],[8.86,16,2,16,8,1,16,1,9.12104322090244,1513.48,193725.44,12.21,"GPU taking a nap","meh..."],[80,8,2,64,2,2,16,1,6.908909482269768,133.4,8537.6,48.86,"GPU taking a nap","meh..."],[8.86,2,1,64,4,2,2,0,9.054992375366924,1502.52,24040.32,62.74,"GPU taking a nap","meh..."],[3.57,16,32,1,8,1,16,1,9.289582732914928,3573.77,457442.56,68.15,"GPU taking a nap","meh..."],[3.57,16,32,2,4,2,16,1,9.25777363924669,3257.43,416951.04,51.59,"GPU taking a nap","meh..."],[1.34,4,2,32,4,2,4,1,8.788935492023006,7712.32,246794.24,6.58,"GPU taking a nap","meh..."],[8.86,16,1,128,2,16,4,1,8.907342218832243,1478.02,189186.56,16.01,"GPU taking a nap","meh..."],[3.57,2,1,32,8,2,1,0,9.181180435531584,3230.48,51687.68,59.57,"GPU taking a nap","meh..."],[8.86,32,2,8,16,1,16,1,8.708104905565444,1444.96,369909.76,11.97,"GPU taking a nap","meh..."],[1.34,8,64,4,1,4,16,0,8.488195886256534,7448.42,476698.88,65.43,"GPU taking a nap","meh..."],[1.34,8,8,8,4,2,8,1,8.487113269259648,7447.47,476638.08,9.8,"GPU taking a nap","meh..."],[8.86,32,32,1,8,2,16,1,8.643259367430213,1434.2,367155.2,33.91,"GPU taking a nap","meh..."],[1.34,32,1,4,64,4,1,0,8.461426756512486,7424.93,1900782.08,20.33,"GPU taking a nap","moar?"],[1.34,8,2,16,8,2,4,1,8.45731281192432,7421.32,474964.48,6.41,"GPU taking a nap","meh..."],[8.86,32,1,8,32,2,4,0,8.61162004817278,1428.95,365811.2,30.61,"GPU taking a nap","meh..."],[1.34,4,2,64,2,4,4,1,8.438953906850815,7405.21,236966.72,8.09,"GPU taking a nap","meh..."],[8.86,32,2,8,16,1,16,0,8.601857058230488,1427.33,365396.48,18.08,"GPU taking a nap","meh..."],[3.57,32,1,4,64,2,2,0,8.872022413263336,3121.7,799155.2,30.08,"GPU taking a nap","moar?"],[1.34,8,8,32,1,8,8,0,8.41644686928398,7385.46,472669.44,16.66,"GPU taking a nap","meh..."],[8.86,4,1,32,8,2,2,0,8.574376049504032,1422.77,45528.64,61.99,"GPU taking a nap","meh..."],[1.34,8,2,32,4,4,4,1,8.388993981436634,7361.37,471127.68,6.58,"GPU taking a nap","meh..."],[3.57,32,1,8,32,1,8,0,8.792288148847303,3390.37,867934.72,12.43,"GPU taking a nap","moar?"],[1.34,32,1,16,16,8,2,1,8.336116688115057,7314.97,1872632.32,8.04,"GPU taking a nap","moar?"],[1.34,8,32,8,1,4,16,0,8.325324706051365,7305.5,467552,28.95,"GPU taking a nap","meh..."],[1.34,16,8,4,8,1,16,1,8.267930710251033,8473.02,1084546.56,12.76,"GPU taking a nap","meh..."],[3.57,4,1,16,16,2,1,0,8.680439932282434,3054.29,97737.28,59.59,"GPU taking a nap","meh..."],[80,16,2,32,4,2,16,1,6.351639122230617,122.64,15697.92,44.47,"GPU taking a nap","meh..."],[8.86,8,1,16,16,2,2,0,8.317766103983757,1380.19,88332.16,62.5,"GPU taking a nap","meh..."],[80,16,2,64,2,4,16,1,6.298812378063337,121.62,15567.36,38.45,"GPU taking a nap","meh..."],[8.86,16,32,1,8,1,16,1,8.28112475901515,1374.11,175886.08,59.95,"GPU taking a nap","meh..."],[3.57,4,4,32,2,2,8,1,8.468821010255466,2979.83,95354.56,12.12,"GPU taking a nap","meh..."],[8.86,4,2,64,2,2,8,1,8.193016788054456,1359.49,43503.68,15.25,"GPU taking a nap","meh..."],[80,64,8,2,16,1,32,1,6.202481256346532,119.76,61317.12,51.93,"GPU taking a nap","moar?"],[1.34,4,64,4,1,1,32,0,8.006378613410039,8204.98,262559.36,51.84,"GPU taking a nap","meh..."],[1.34,16,32,2,4,2,16,1,7.981576715587559,7003.86,896494.08,19.38,"GPU taking a nap","meh..."],[3.57,8,4,16,4,2,8,1,8.368667225508887,2944.59,188453.76,11.13,"GPU taking a nap","meh..."],[8.86,64,16,1,16,1,32,1,8.050811315198706,1334.55,683289.6,46.22,"GPU taking a nap","moar?"],[1.34,16,8,4,8,1,16,0,7.841586350633933,8036.1,1028620.8,14.19,"GPU taking a nap","meh..."],[8.86,64,2,1,128,2,2,1,7.996491416438294,1326.88,679362.56,33.31,"GPU taking a nap","moar?"],[3.57,32,8,1,32,2,4,1,8.173731127445096,2876,736256,21.98,"GPU taking a nap","moar?"],[1.34,16,8,8,4,4,8,1,7.732871101481933,6785.62,868559.36,13.44,"GPU taking a nap","meh..."],[8.86,32,1,64,4,16,4,1,7.877467310660337,1307.13,334625.28,15.01,"GPU taking a nap","meh..."],[3.57,32,1,8,32,1,8,1,8.09409068417113,3121.14,799011.84,6.48,"GPU taking a nap","moar?"],[8.86,64,4,1,64,1,8,1,7.830279525939252,1299.3,665241.6,23.36,"GPU taking a nap","moar?"],[8.86,8,2,64,2,4,8,1,7.811115138274749,1296.12,82951.68,14.43,"GPU taking a nap","meh..."],[80,16,8,16,2,2,32,1,5.912452072683033,114.16,14612.48,48.96,"GPU taking a nap","meh..."],[3.57,4,16,8,2,1,16,1,8.048223291783895,3096.21,99078.72,23.61,"GPU taking a nap","meh..."],[8.86,16,2,16,8,2,8,1,7.753139852321133,1286.5,164672,13.02,"GPU taking a nap","meh..."],[3.57,16,4,8,8,1,16,1,7.9545156783891935,3060.16,391700.48,11.91,"GPU taking a nap","meh..."],[1.34,16,2,16,8,4,4,1,7.510433194511272,6590.43,843575.04,6.49,"GPU taking a nap","meh..."],[3.57,16,4,16,4,4,8,1,7.842120923011754,2759.32,353192.96,13.15,"GPU taking a nap","meh..."],[8.86,16,4,4,16,4,2,1,7.582829916661129,1258.24,161054.72,66,"GPU taking a nap","meh..."],[3.57,16,4,8,8,1,16,0,7.827302069525484,3011.22,385436.16,14.81,"GPU taking a nap","meh..."],[1.34,32,1,8,32,4,2,0,7.428416410020886,6518.46,1668725.76,10.61,"GPU taking a nap","moar?"],[80,32,4,16,4,4,16,1,5.7140928274274625,110.33,28244.48,47.82,"GPU taking a nap","meh..."],[8.86,16,64,2,2,2,32,1,7.514271538964189,1245.61,159438.08,63.38,"GPU taking a nap","meh..."],[8.86,64,8,1,32,2,8,1,7.493034515340036,1243.34,636590.08,19.73,"GPU taking a nap","moar?"],[1.34,32,8,2,16,1,16,1,7.287100589147114,7467.86,1911772.16,12.87,"GPU taking a nap","moar?"],[8.86,4,8,32,1,2,16,0,7.394862227586977,1227.05,39265.6,20.53,"GPU taking a nap","meh..."],[8.86,32,1,4,64,2,2,1,7.387570117815264,1225.84,313815.04,34.31,"GPU taking a nap","meh..."],[8.86,64,1,1,256,2,1,1,7.375396513072404,1223.82,626595.84,52.36,"GPU taking a nap","moar?"],[3.57,16,32,4,2,4,16,1,7.598273172408558,2673.52,342210.56,60.13,"GPU taking a nap","meh..."],[1.34,16,2,8,16,2,4,1,7.212132325979891,6328.67,810069.76,6.08,"GPU taking a nap","meh..."],[1.34,32,8,2,16,2,8,1,7.203414410162863,6321.02,1618181.12,9.77,"GPU taking a nap","moar?"],[1.34,16,32,4,2,4,16,1,7.10951163064351,6238.62,798543.36,28.84,"GPU taking a nap","meh..."],[1.34,32,8,4,8,4,8,1,7.075403497257415,6208.69,1589424.64,12.16,"GPU taking a nap","moar?"],[3.57,4,16,16,1,2,16,0,7.428574463917687,2613.81,83641.92,19.86,"GPU taking a nap","meh..."],[3.57,32,4,4,16,1,16,1,7.414364719625836,2852.36,730204.16,11.93,"GPU taking a nap","moar?"],[80,4,1,256,1,1,32,0,5.435975556664428,104.96,3358.72,51.73,"GPU taking a nap","meh..."],[1.34,4,32,8,1,1,32,0,7.016941980240243,7191,230112,37.58,"GPU taking a nap","meh..."],[8.86,8,8,32,1,4,16,0,7.1304177839648615,1183.17,75722.88,25.59,"GPU taking a nap","meh..."],[1.34,4,1,64,4,1,8,0,6.98785353072961,7161.19,229158.08,5.36,"GPU taking a nap","meh..."],[8.86,16,8,8,4,2,16,1,7.117581260151846,1181.04,151173.12,17.71,"GPU taking a nap","meh..."],[80,32,8,8,4,2,32,1,5.383148812497149,103.94,26608.64,47.18,"GPU taking a nap","meh..."],[1.34,8,2,64,2,8,4,1,6.9487145166850075,6097.52,390241.28,8.12,"GPU taking a nap","meh..."],[80,8,1,128,2,1,32,1,5.373308536622852,103.75,6640,40.48,"GPU taking a nap","meh..."],[8.86,64,16,1,16,1,32,0,7.079019179448558,1173.46,600811.52,49.15,"GPU taking a nap","moar?"],[80,8,1,128,2,1,32,0,5.360360805209302,103.5,6624,56.21,"GPU taking a nap","meh..."],[80,16,1,64,4,1,32,0,5.348448892308837,103.27,13218.56,56.12,"GPU taking a nap","meh..."],[3.57,16,4,4,16,2,4,1,7.272688110530912,2558.96,327546.88,22.45,"GPU taking a nap","meh..."],[3.57,8,16,8,2,2,16,1,7.268680822321699,2557.55,163683.2,18.96,"GPU taking a nap","meh..."],[3.57,32,4,4,16,1,16,0,7.261755177811608,2793.65,715174.4,14.92,"GPU taking a nap","moar?"],[80,16,1,64,4,1,32,1,5.3039086962462285,102.41,13108.48,36.21,"GPU taking a nap","meh..."],[3.57,2,1,128,2,2,4,0,7.200500081797565,2533.56,40536.96,15.16,"GPU taking a nap","meh..."],[1.34,2,1,128,2,1,8,0,6.787434991816865,6955.8,111292.8,5.67,"GPU taking a nap","meh..."],[1.34,16,32,8,1,8,16,0,6.786116839721156,5954.84,762219.52,42.76,"GPU taking a nap","meh..."],[1.34,4,1,64,4,1,8,1,6.769265688566682,6937.18,221989.76,3.78,"GPU taking a nap","meh..."],[1.34,1,1,256,1,1,8,0,6.739825952849278,6907.01,55256.08,5.36,"GPU taking a nap","meh..."],[1.34,2,1,128,2,1,8,1,6.738850157830472,6906.01,110496.16,4.42,"GPU taking a nap","meh..."],[1.34,8,1,32,8,1,8,0,6.721803018851915,6888.54,440866.56,5.7,"GPU taking a nap","meh..."],[80,64,4,8,8,4,16,1,5.1977372986551265,100.36,51384.32,46.35,"GPU taking a nap","moar?"],[8.86,8,2,32,4,2,8,1,6.819147147841747,1131.52,72417.28,13.83,"GPU taking a nap","meh..."],[3.57,2,1,256,1,4,4,0,7.015738516917453,2468.55,39496.8,11.42,"GPU taking a nap","meh..."],[3.57,16,16,4,4,2,16,1,6.987403294615426,2458.58,314698.24,19.45,"GPU taking a nap","meh..."],[1.34,32,2,8,16,4,4,1,6.628761308215911,5816.76,1489090.56,6.18,"GPU taking a nap","meh..."],[3.57,4,1,256,1,8,4,0,6.9689015596920365,2452.07,78466.24,8.24,"GPU taking a nap","meh..."],[1.34,32,8,2,16,1,16,0,6.608483943317872,6772.41,1733736.96,13.9,"GPU taking a nap","moar?"],[3.57,32,4,4,16,2,8,1,6.938349539657254,2441.32,624977.92,10.74,"GPU taking a nap","meh..."],[3.57,2,1,128,2,2,4,1,6.928771836632396,2437.95,39007.2,11.26,"GPU taking a nap","meh..."],[1.34,16,1,16,16,1,8,0,6.557986551094617,6720.66,860244.48,5.69,"GPU taking a nap","meh..."],[1.34,16,8,16,2,8,8,1,6.50650535934654,5709.48,730813.44,16.57,"GPU taking a nap","meh..."],[8.86,32,2,4,32,2,4,1,6.626358229166463,1099.53,281479.68,23.9,"GPU taking a nap","meh..."],[3.57,4,1,64,4,2,4,1,6.837939970556893,2405.99,76991.68,10.22,"GPU taking a nap","meh..."],[3.57,4,1,128,2,4,4,1,6.83634842346671,2405.43,76973.76,10.12,"GPU taking a nap","meh..."],[8.86,16,2,32,4,4,8,1,6.615450197193901,1097.72,140508.16,14.38,"GPU taking a nap","meh..."],[1.34,8,1,32,8,1,8,1,6.463451529672606,6623.78,423921.92,3.71,"GPU taking a nap","meh..."],[80,64,1,16,16,1,32,0,4.997306416373387,96.49,49402.88,56.36,"GPU taking a nap","moar?"],[3.57,4,1,128,2,4,4,0,6.797042894435915,2391.6,76531.2,11.97,"GPU taking a nap","meh..."],[3.57,8,16,4,4,1,16,1,6.7955305581590775,2614.29,167314.56,21.66,"GPU taking a nap","meh..."],[3.57,4,16,16,1,1,32,0,6.7772569237023585,2607.26,83432.32,32.75,"GPU taking a nap","meh..."],[3.57,8,4,32,2,4,8,1,6.776750669036069,2384.46,152605.44,14.96,"GPU taking a nap","meh..."],[8.86,32,16,2,8,2,16,1,6.550243066838585,1086.9,278246.4,31.55,"GPU taking a nap","meh..."],[3.57,8,1,128,2,8,4,1,6.704733163205241,2359.12,150983.68,8.66,"GPU taking a nap","meh..."],[3.57,8,16,16,1,4,16,0,6.681087320722509,2350.8,150451.2,30.48,"GPU taking a nap","meh..."],[3.57,16,1,16,16,2,4,1,6.665456054658201,2345.3,300198.4,9.55,"GPU taking a nap","meh..."],[3.57,32,16,2,8,2,16,1,6.622825329028272,2330.3,596556.8,18.68,"GPU taking a nap","meh..."],[1.34,16,1,16,16,1,8,1,6.256417100532352,6411.61,820686.08,3.21,"GPU taking a nap","meh..."],[3.57,32,1,2,128,2,1,1,6.564421234915268,2309.75,591296,33.6,"GPU taking a nap","meh..."],[3.57,16,4,8,8,2,8,1,6.555042475276682,2306.45,295225.6,10.76,"GPU taking a nap","meh..."],[3.57,16,4,32,2,8,8,1,6.548562604980934,2304.17,294933.76,14.41,"GPU taking a nap","meh..."],[80,64,8,4,8,2,32,1,4.798429261861274,92.65,47436.8,43.57,"GPU taking a nap","moar?"],[3.57,8,1,32,8,2,4,1,6.506045561286018,2289.21,146509.44,9.31,"GPU taking a nap","meh..."],[1.34,32,8,8,4,8,8,1,6.177161872925509,5420.48,1387642.88,16.51,"GPU taking a nap","meh..."],[8.86,8,8,16,2,2,16,1,6.30032257738988,1045.43,66907.52,18.64,"GPU taking a nap","meh..."],[3.57,16,16,8,2,4,16,1,6.479841875265487,2279.99,291838.72,30.38,"GPU taking a nap","meh..."],[3.57,16,16,16,1,8,16,0,6.475948268991288,2278.62,291663.36,36.23,"GPU taking a nap","meh..."],[80,32,2,16,8,2,16,1,4.730065239997735,91.33,23380.48,42.63,"GPU taking a nap","meh..."],[8.86,16,8,16,2,4,16,1,6.2343319972243805,1034.48,132413.44,27.08,"GPU taking a nap","meh..."],[80,16,2,64,2,8,8,1,4.722296601149606,91.18,11671.04,53.25,"GPU taking a nap","meh..."],[1.34,16,64,1,4,2,16,1,6.093150793967175,5346.76,684385.28,36.64,"GPU taking a nap","meh..."],[1.34,32,32,1,8,2,16,1,6.087418621867875,5341.73,1367482.88,17.71,"GPU taking a nap","meh..."],[3.57,32,2,2,64,2,2,1,6.388583701933687,2247.88,575457.28,25.35,"GPU taking a nap","meh..."],[3.57,32,4,16,4,8,8,1,6.370423012815336,2241.49,573821.44,14.11,"GPU taking a nap","meh..."],[3.57,32,1,2,128,2,1,0,6.3604758435016855,2237.99,572925.44,60.34,"GPU taking a nap","meh..."],[8.86,16,8,32,1,8,16,0,6.142608104062835,1019.26,130465.28,29.69,"GPU taking a nap","meh..."],[8.86,16,2,64,2,8,8,1,6.138208732051802,1018.53,130371.84,13.94,"GPU taking a nap","meh..."],[3.57,32,4,8,8,4,8,1,6.301759124067398,2217.33,567636.48,13.12,"GPU taking a nap","meh..."],[1.34,32,1,8,32,1,8,0,5.930569869902172,6077.68,1555886.08,5.61,"GPU taking a nap","moar?"],[8.86,64,8,2,16,1,32,1,6.008774950735207,996.05,509977.6,23.77,"GPU taking a nap","moar?"],[8.86,64,1,2,128,2,2,1,5.995681131968324,994.88,509378.56,33.21,"GPU taking a nap","moar?"],[3.57,32,1,64,64,1,4,0,6.150662462842433,2371.74,607165.44,21.91,"GPU taking a nap","meh..."],[3.57,32,4,32,2,16,8,1,6.144366485041697,2161.95,553459.2,14.09,"GPU taking a nap","meh..."],[3.57,8,1,256,1,16,4,0,6.143997018752906,2161.82,138356.48,7.38,"GPU taking a nap","meh..."],[3.57,32,4,2,32,2,4,1,6.074764720329901,2137.46,547189.76,21.27,"GPU taking a nap","meh..."],[8.86,2,1,256,1,1,16,0,5.8087982195544265,963.87,15421.92,13.13,"GPU taking a nap","meh..."],[80,32,2,32,4,8,8,1,4.380476491831911,84.58,21652.48,51.22,"GPU taking a nap","meh..."],[80,64,2,8,16,2,16,1,4.345258662387058,83.9,42956.8,41.21,"GPU taking a nap","meh..."],[8.86,4,1,128,2,1,16,1,5.7253909474548355,950.03,30400.96,10.58,"GPU taking a nap","meh..."],[3.57,32,16,4,4,4,16,1,5.8915378615724645,2072.99,530685.44,30.25,"GPU taking a nap","meh..."],[1.34,32,2,16,8,8,4,1,5.578702593015458,4895.33,1253204.48,7.64,"GPU taking a nap","meh..."],[8.86,32,64,1,4,2,32,1,5.67384479134078,940.53,240775.68,37.35,"GPU taking a nap","meh..."],[3.57,32,1,128,32,2,4,0,5.829154899734002,2051.04,525066.24,15.15,"GPU taking a nap","meh..."],[8.86,8,1,64,4,1,16,0,5.645720128296121,936.81,59955.84,14.08,"GPU taking a nap","meh..."],[1.34,8,16,4,4,2,8,1,5.52089084538176,4844.6,310054.4,19.93,"GPU taking a nap","meh..."],[8.86,64,2,2,64,2,4,1,5.627399455811818,933.77,478090.24,23.75,"GPU taking a nap","moar?"],[8.86,4,1,128,2,1,16,0,5.624446452681124,933.28,29864.96,14.15,"GPU taking a nap","meh..."],[8.86,8,1,64,4,1,16,1,5.612393378678293,931.28,59601.92,9.62,"GPU taking a nap","meh..."],[8.86,32,4,4,16,2,8,1,5.569725496708271,924.2,236595.2,19.13,"GPU taking a nap","meh..."],[8.86,16,1,32,8,1,16,1,5.560926752686204,922.74,118110.72,8.55,"GPU taking a nap","meh..."],[80,16,8,32,1,4,32,0,4.191439613194095,80.93,10359.04,45.33,"GPU taking a nap","meh..."],[1.34,16,4,8,8,1,16,1,5.411193213192024,5545.42,709813.76,7.26,"GPU taking a nap","meh..."],[8.86,16,1,32,8,1,16,0,5.51271445667488,914.74,117086.72,14.08,"GPU taking a nap","meh..."],[3.57,32,32,1,8,2,16,1,5.684977785653581,2000.31,512079.36,42.16,"GPU taking a nap","meh..."],[1.34,4,4,32,2,2,8,1,5.391261704965177,4730.85,151387.2,5.97,"GPU taking a nap","meh..."],[3.57,16,1,32,8,4,4,1,5.674547468116125,1996.64,255569.92,8.14,"GPU taking a nap","meh..."],[1.34,8,4,16,4,2,8,1,5.386498190178879,4726.67,302506.88,5.49,"GPU taking a nap","meh..."],[8.86,32,1,128,2,32,4,1,5.48951228921943,910.89,233187.84,18.01,"GPU taking a nap","meh..."],[1.34,8,4,32,2,4,8,1,5.370646398150795,4712.76,301616.64,6.6,"GPU taking a nap","meh..."],[1.34,32,1,8,32,1,8,1,5.327811528834976,5459.97,1397752.32,3.11,"GPU taking a nap","meh..."],[8.86,2,2,64,2,4,2,1,5.397125476987729,895.56,14328.96,50.41,"GPU taking a nap","meh..."],[1.34,16,4,8,8,1,16,0,5.251357989111458,5381.62,688847.36,8.24,"GPU taking a nap","meh..."],[1.34,64,2,1,128,2,2,1,5.245655416868239,4603.08,2356776.96,8.4,"GPU taking a nap","moar?"],[3.57,32,1,16,16,4,4,1,5.524572575350034,1943.87,497630.72,7.82,"GPU taking a nap","meh..."],[8.86,64,8,2,16,1,32,0,5.344947235681844,886.01,453637.12,27.67,"GPU taking a nap","moar?"],[8.86,32,1,16,16,1,16,1,5.329025608871733,884.26,226370.56,8.52,"GPU taking a nap","meh..."],[80,64,2,16,8,8,8,1,4.033477289948797,77.88,39874.56,49.93,"GPU taking a nap","meh..."],[8.86,32,1,16,16,1,16,0,5.292203467793084,878.15,224806.4,14.31,"GPU taking a nap","meh..."],[1.34,32,1,4,64,2,2,0,5.062009386291252,4441.93,1137134.08,13.45,"GPU taking a nap","meh..."],[3.57,32,16,8,2,8,16,1,5.305678009482224,1866.85,477913.6,36.12,"GPU taking a nap","meh..."],[1.34,32,4,4,16,1,16,1,5.0361854295143536,5161.11,1321244.16,6.73,"GPU taking a nap","meh..."],[80,64,1,16,16,4,8,1,3.894677609195551,75.2,38502.4,45.78,"GPU taking a nap","meh..."],[80,32,8,16,2,4,32,1,3.8563523242114455,74.46,19061.76,42.21,"GPU taking a nap","meh..."],[8.86,8,32,8,1,2,32,0,5.08077951810472,842.22,53902.08,33.36,"GPU taking a nap","meh..."],[8.86,64,8,1,32,1,16,0,5.062230815819066,839.99,430074.88,25.93,"GPU taking a nap","moar?"],[8.86,64,2,2,64,1,8,1,5.04547704295513,837.21,428651.52,16.45,"GPU taking a nap","moar?"],[1.34,8,64,2,2,1,32,1,4.931170369590358,5053.49,323423.36,26.29,"GPU taking a nap","meh..."],[1.34,16,4,16,4,4,8,1,4.883651085046423,4285.42,548533.76,6.52,"GPU taking a nap","meh..."],[3.57,32,1,64,4,16,4,1,5.114038687533816,1799.42,460651.52,7.3,"GPU taking a nap","meh..."],[80,4,4,64,1,1,32,0,3.74655556182455,72.34,2314.88,58.54,"GPU taking a nap","meh..."],[1.34,4,16,16,1,2,16,0,4.824836492194556,4233.81,135481.92,10.75,"GPU taking a nap","meh..."],[1.34,8,128,2,1,2,32,0,4.793167096043551,4206.02,269185.28,44.37,"GPU taking a nap","meh..."],[3.57,16,2,16,8,1,16,1,5.032553730622641,1936.06,247815.68,6.74,"GPU taking a nap","meh..."],[8.86,64,16,1,16,2,16,1,4.8496748557791385,804.72,412016.64,18.12,"GPU taking a nap","moar?"],[80,64,8,8,4,4,32,1,3.643491619772699,70.35,36019.2,39.79,"GPU taking a nap","meh..."],[3.57,16,2,16,8,1,16,0,4.959797112067365,1908.07,244232.96,9.53,"GPU taking a nap","meh..."],[8.86,16,32,4,2,2,32,1,4.775529769088906,791.62,101327.36,32.38,"GPU taking a nap","meh..."],[1.34,4,16,16,1,1,32,0,4.67822478481524,4794.27,153416.64,18.98,"GPU taking a nap","meh..."],[80,32,4,32,2,8,16,1,3.5989514237100906,69.49,17789.44,44.27,"GPU taking a nap","meh..."],[8.86,64,4,1,64,1,8,0,4.724142090039652,783.89,401351.68,37.07,"GPU taking a nap","moar?"],[80,8,4,32,2,1,32,1,3.5730559608829924,68.99,4415.36,46.76,"GPU taking a nap","meh..."],[1.34,8,16,8,2,2,16,1,4.607868650050265,4043.42,258778.88,9.89,"GPU taking a nap","meh..."],[8.86,4,32,8,1,1,32,0,4.700182548432631,779.13,24932.16,48.62,"GPU taking a nap","meh..."],[1.34,32,4,4,16,1,16,0,4.592491434462867,4706.41,1204840.96,7.74,"GPU taking a nap","meh..."],[1.34,32,4,1,64,2,2,1,4.5775781660742405,4016.84,1028311.04,13.3,"GPU taking a nap","meh..."],[8.86,64,4,2,32,2,8,1,4.661767432075001,773.54,396052.48,19.38,"GPU taking a nap","meh..."],[1.34,8,16,16,1,4,16,0,4.568210680059082,4008.62,256551.68,14.67,"GPU taking a nap","meh..."],[3.57,32,2,8,16,1,16,1,4.790395581833448,1842.9,471782.4,6.78,"GPU taking a nap","meh..."],[8.86,16,4,8,8,4,4,1,4.609216029422657,764.82,97896.96,36.32,"GPU taking a nap","meh..."],[8.86,16,64,4,1,4,32,0,4.596542416256906,761.95,97529.6,63.77,"GPU taking a nap","meh..."],[1.34,16,64,1,4,1,32,1,4.49976163382565,4611.38,590256.64,26.28,"GPU taking a nap","meh..."],[3.57,32,2,8,16,1,16,0,4.739031852549695,1823.14,466723.84,9.58,"GPU taking a nap","meh..."],[80,16,4,16,4,1,32,0,3.468438291061517,66.97,8572.16,63.19,"GPU taking a nap","meh..."],[1.34,32,8,1,32,2,4,1,4.479607025840275,3930.87,1006302.72,11.18,"GPU taking a nap","meh..."],[3.57,32,1,8,32,2,4,0,4.7092741579195145,1657,424192,15.13,"GPU taking a nap","meh..."],[1.34,4,1,64,4,2,4,1,4.462080026459114,3915.49,125295.68,4.77,"GPU taking a nap","meh..."],[80,32,1,32,8,2,16,1,3.438399554182083,66.39,16995.84,36.82,"GPU taking a nap","meh..."],[80,4,1,256,1,2,16,0,3.4202727302031146,66.04,2113.28,55.3,"GPU taking a nap","meh..."],[3.57,64,2,1,128,2,2,1,4.636233514673569,1631.3,835225.6,17.14,"GPU taking a nap","moar?"],[80,32,1,64,4,4,16,1,3.4026638154806874,65.7,16819.2,25.04,"GPU taking a nap","meh..."],[1.34,2,1,128,2,2,4,1,4.357510620528339,3823.73,61179.68,5.33,"GPU taking a nap","meh..."],[8.86,2,1,128,2,1,8,1,4.4416783007833045,737.02,11792.32,17.83,"GPU taking a nap","meh..."],[1.34,8,1,64,4,4,4,1,4.328097626118212,3797.92,243066.88,4.6,"GPU taking a nap","meh..."],[80,16,1,64,4,2,16,1,3.3384430676694836,64.46,8250.88,38.85,"GPU taking a nap","meh..."],[1.34,32,4,8,8,4,8,1,4.28217187351348,3757.62,961950.72,6.51,"GPU taking a nap","meh..."],[80,64,4,16,4,8,16,1,3.303743147481173,63.79,32660.48,42.86,"GPU taking a nap","meh..."],[8.86,32,2,4,32,4,2,1,4.350315999841844,721.86,184796.16,38.67,"GPU taking a nap","meh..."],[80,8,1,128,2,2,16,1,3.2939028716068757,63.6,4070.4,43.38,"GPU taking a nap","meh..."],[1.34,2,1,256,1,4,4,0,4.250046638626944,3729.43,59670.88,4.9,"GPU taking a nap","meh..."],[3.57,64,4,1,64,2,4,1,4.4687231834317,1572.36,805048.32,13.2,"GPU taking a nap","moar?"],[1.34,16,4,8,8,2,8,1,4.236884295138491,3717.88,475888.64,5.44,"GPU taking a nap","meh..."],[8.86,32,1,4,64,4,1,1,4.253650346339139,705.82,180689.92,49.51,"GPU taking a nap","meh..."],[3.57,64,1,1,256,1,2,1,4.3875731773440005,1691.88,866242.56,22.82,"GPU taking a nap","moar?"],[80,16,1,256,1,8,16,0,3.2006792054293225,61.8,7910.4,24.17,"GPU taking a nap","meh..."],[3.57,4,1,32,8,2,2,0,4.355922283414908,1532.67,49045.44,30.93,"GPU taking a nap","meh..."],[8.86,2,1,256,1,2,8,0,4.217069266740546,699.75,11196,15.51,"GPU taking a nap","meh..."],[1.34,32,1,64,64,1,4,0,4.106155197089521,4208.01,1077250.56,9.28,"GPU taking a nap","meh..."],[3.57,64,16,1,16,1,32,1,4.304623632008154,1656.02,847882.24,29.35,"GPU taking a nap","moar?"],[80,8,1,256,1,4,16,0,3.158728555649424,60.99,3903.36,35.03,"GPU taking a nap","meh..."],[3.57,4,2,64,2,2,8,1,4.29999077138844,1512.99,48415.68,7.97,"GPU taking a nap","meh..."],[3.57,2,1,64,4,2,2,0,4.29876869058705,1512.56,24200.96,31.06,"GPU taking a nap","meh..."],[1.34,16,16,8,2,4,16,1,4.0590274165027465,3561.81,455911.68,14.59,"GPU taking a nap","meh..."],[8.86,2,1,128,2,2,4,1,4.140713042932611,687.08,10993.28,23.74,"GPU taking a nap","meh..."],[3.57,64,2,4,32,1,16,0,4.271663506131737,1643.34,841390.08,9.56,"GPU taking a nap","moar?"],[1.34,16,16,4,4,2,16,1,4.040828054986678,3545.84,453867.52,9.8,"GPU taking a nap","meh..."],[80,32,1,128,2,8,16,1,3.1224749076914864,60.29,15434.24,22.01,"GPU taking a nap","meh..."],[80,16,1,128,2,4,16,1,3.1198853614087767,60.24,7710.72,27.43,"GPU taking a nap","meh..."],[3.57,64,1,1,256,2,1,1,4.2453381811308715,1493.76,764805.12,25.81,"GPU taking a nap","moar?"],[8.86,32,32,2,4,2,32,1,4.106755318972441,680.76,174274.56,31.64,"GPU taking a nap","meh..."],[3.57,8,2,32,4,2,8,1,4.224534387023466,1486.44,95132.16,6.95,"GPU taking a nap","meh..."],[8.86,32,4,4,16,4,4,1,4.087679517320153,678.28,173639.68,34.08,"GPU taking a nap","meh..."],[3.57,8,1,16,16,2,2,0,4.214160910453517,1482.79,94898.56,31.03,"GPU taking a nap","meh..."],[8.86,64,4,4,16,1,32,1,4.073817302576662,675.3,345753.6,13.02,"GPU taking a nap","meh..."],[1.34,32,1,128,32,2,4,0,3.962583336032809,3477.18,890158.08,6.75,"GPU taking a nap","meh..."],[8.86,8,16,8,2,1,32,1,4.026823319477193,667.51,42720.64,18.48,"GPU taking a nap","meh..."],[80,32,4,16,4,2,32,1,3.05203924880178,58.93,15086.08,30.42,"GPU taking a nap","meh..."],[1.34,16,1,32,8,4,4,1,3.93225866415162,3450.57,441672.96,4,"GPU taking a nap","meh..."],[3.57,16,2,32,4,4,8,1,4.140807641886285,1456.98,186493.44,7.46,"GPU taking a nap","meh..."],[8.86,8,1,64,4,2,8,1,4.00758684057134,664.99,42559.36,10.28,"GPU taking a nap","meh..."],[8.86,4,1,64,4,2,4,0,4.007526575201327,664.98,21279.36,31.77,"GPU taking a nap","meh..."],[8.86,8,1,128,2,4,8,1,3.9971006661888775,663.25,42448,9.4,"GPU taking a nap","meh..."],[8.86,32,8,4,8,4,8,1,3.9945092552782695,662.82,169681.92,31.26,"GPU taking a nap","meh..."],[1.34,32,4,4,16,2,8,1,3.9150393559169423,3435.46,879477.76,5.44,"GPU taking a nap","meh..."],[3.57,16,64,2,2,2,32,1,4.107072527671134,1445.11,184974.08,38.81,"GPU taking a nap","meh..."],[1.34,32,16,2,8,2,16,1,3.886629206724983,3410.53,873095.68,9.55,"GPU taking a nap","meh..."],[8.86,16,1,32,8,2,8,1,3.960941444180384,657.25,84128,9.46,"GPU taking a nap","meh..."],[1.34,4,1,128,2,4,4,1,3.8776491836350258,3402.65,108884.8,5.04,"GPU taking a nap","meh..."],[80,32,2,32,4,4,16,1,2.9992125046344995,57.91,14824.96,31.74,"GPU taking a nap","meh..."],[3.57,16,1,8,32,2,2,1,4.073166890553464,1433.18,183447.04,19.06,"GPU taking a nap","meh..."],[8.86,8,1,32,8,2,4,0,3.939486972455345,653.69,41836.16,31.9,"GPU taking a nap","meh..."],[3.57,8,2,64,2,4,8,1,4.063987074301153,1429.95,91516.8,7.93,"GPU taking a nap","meh..."],[8.86,4,1,256,1,4,8,0,3.923275587921537,651,20832,11.17,"GPU taking a nap","meh..."],[8.86,16,1,64,4,4,8,1,3.919599400350674,650.39,83249.92,8.54,"GPU taking a nap","meh..."],[80,64,1,8,32,1,16,1,2.969173767755066,57.33,29352.96,60.3,"GPU taking a nap","meh..."],[3.57,16,2,16,8,2,8,1,4.037214978605557,1420.53,181827.84,8.05,"GPU taking a nap","meh..."],[8.86,4,1,128,2,2,8,1,3.88416336278235,644.51,20624.32,11.42,"GPU taking a nap","meh..."],[8.86,16,16,4,4,1,32,1,3.8810152871903902,643.34,82347.52,18.44,"GPU taking a nap","meh..."],[80,64,2,16,8,4,16,1,2.939135030875632,56.75,29056,30.81,"GPU taking a nap","meh..."],[1.34,32,1,16,16,4,4,1,3.781729317710949,3318.48,849530.88,3.69,"GPU taking a nap","meh..."],[1.34,16,16,16,1,8,16,0,3.7757920181806592,3313.27,424098.56,21.55,"GPU taking a nap","meh..."],[1.34,8,4,4,16,4,1,1,3.752988685435729,3293.26,210768.64,44.99,"GPU taking a nap","meh..."],[8.86,64,4,4,16,1,32,0,3.817793600042451,632.86,324024.32,16.49,"GPU taking a nap","meh..."],[8.86,16,1,16,16,2,4,0,3.8129296954256184,632.69,80984.32,31.58,"GPU taking a nap","meh..."],[3.57,64,4,1,64,1,8,1,3.9158237408055654,1509.97,773104.64,13.2,"GPU taking a nap","moar?"],[3.57,16,4,4,16,4,2,1,3.903951330286399,1373.64,175825.92,38.97,"GPU taking a nap","meh..."],[1.34,32,1,2,128,2,1,0,3.6931712473656946,3240.77,829637.12,26.77,"GPU taking a nap","meh..."],[3.57,16,2,64,2,8,8,1,3.885392754395502,1367.11,174990.08,8.02,"GPU taking a nap","meh..."],[3.57,8,32,4,2,1,32,1,3.8795211883307745,1492.48,95518.72,22.14,"GPU taking a nap","meh..."],[1.34,16,128,1,2,2,32,1,3.679587253046875,3228.85,413292.8,26.37,"GPU taking a nap","meh..."],[3.57,8,8,16,2,2,16,1,3.8722909113852375,1362.5,87200,10.35,"GPU taking a nap","meh..."],[8.86,32,8,4,8,2,16,1,3.749892118390811,622.23,159290.88,17.11,"GPU taking a nap","meh..."],[80,16,4,32,2,2,32,1,2.840732272132659,54.85,7020.8,32.63,"GPU taking a nap","meh..."],[3.57,16,8,8,4,2,16,1,3.834974816217173,1349.37,172719.36,10.94,"GPU taking a nap","meh..."],[1.34,8,1,32,8,2,4,1,3.6381856998922872,3192.52,204321.28,4.21,"GPU taking a nap","meh..."],[3.57,64,8,1,32,2,8,1,3.828864412210217,1347.22,689776.64,11.81,"GPU taking a nap","moar?"],[8.86,32,1,128,32,1,8,0,3.7049944177302647,614.78,157383.68,24.6,"GPU taking a nap","meh..."],[3.57,4,8,32,1,2,16,0,3.823521361264598,1345.34,43050.88,11.23,"GPU taking a nap","meh..."],[1.34,4,1,256,1,8,4,0,3.5882371704464955,3148.69,100758.08,5.08,"GPU taking a nap","meh..."],[80,8,1,128,2,2,16,0,2.7713324317560364,53.51,3424.64,60.02,"GPU taking a nap","meh..."],[1.34,32,4,16,4,8,8,1,3.556054955718018,3120.45,798835.2,8.65,"GPU taking a nap","meh..."],[8.86,32,1,8,32,2,4,1,3.624962006351466,601.5,153984,18.32,"GPU taking a nap","meh..."],[1.34,16,1,256,1,16,8,0,3.4885794268910657,3061.24,391838.72,3.72,"GPU taking a nap","meh..."],[3.57,32,1,4,64,2,2,1,3.666327665625173,1290.03,330247.68,17.08,"GPU taking a nap","meh..."],[3.57,16,32,2,4,1,32,1,3.665462326664931,1410.13,180496.64,21.5,"GPU taking a nap","meh..."],[3.57,8,8,32,1,4,16,0,3.656778383084069,1286.67,82346.88,17.06,"GPU taking a nap","meh..."],[8.86,8,4,32,2,2,16,1,3.532032805789638,586.08,37509.12,15.13,"GPU taking a nap","meh..."],[80,16,1,64,4,2,16,0,2.67189385449998,51.59,6603.52,59.71,"GPU taking a nap","meh..."],[3.57,16,8,4,8,1,16,1,3.616022137850305,1391.11,178062.08,11.5,"GPU taking a nap","meh..."],[8.86,64,2,2,64,1,8,0,3.489123862339559,578.96,296427.52,28.37,"GPU taking a nap","meh..."],[80,64,4,8,8,2,32,1,2.64030138985092,50.98,26101.76,29.25,"GPU taking a nap","meh..."],[8.86,16,4,16,4,2,16,1,3.4745396427961333,576.54,73797.12,10.67,"GPU taking a nap","meh..."],[3.57,32,2,64,2,16,8,1,3.524481031212521,1240.12,317470.72,7.65,"GPU taking a nap","meh..."],[8.86,16,4,32,2,4,16,1,3.40270332173926,564.62,72271.36,14.8,"GPU taking a nap","meh..."],[8.86,32,1,16,16,2,8,1,3.3990873995384105,564.02,144389.12,8.93,"GPU taking a nap","meh..."],[1.34,16,2,16,8,1,16,1,3.3294711518701883,3412.06,436743.68,4.01,"GPU taking a nap","meh..."],[8.86,32,1,32,8,4,8,1,3.3966767847378443,563.62,144286.72,7.92,"GPU taking a nap","meh..."],[1.34,32,16,8,2,8,16,1,3.302665598604813,2898.1,741913.6,21.47,"GPU taking a nap","meh..."],[80,64,4,4,16,1,32,0,2.5408628125948636,49.06,25118.72,66.81,"GPU taking a nap","meh..."],[3.57,32,8,4,8,2,16,1,3.4137832469934724,1201.17,307499.52,10.14,"GPU taking a nap","meh..."],[3.57,32,2,4,32,2,4,1,3.4132432578021596,1200.98,307450.88,12.89,"GPU taking a nap","meh..."],[80,64,8,16,2,8,32,1,2.506162892406552,48.39,24775.68,39.8,"GPU taking a nap","meh..."],[8.86,64,8,2,16,2,16,1,3.277050025259745,543.77,278410.24,17.32,"GPU taking a nap","meh..."],[1.34,16,2,16,8,1,16,0,3.179911049337658,3258.79,417125.12,4.8,"GPU taking a nap","meh..."],[3.57,16,8,16,2,4,16,1,3.3470803716245094,1177.7,150745.6,17.47,"GPU taking a nap","meh..."],[1.34,32,1,8,32,2,4,0,3.170016526565339,2781.7,712115.2,6.75,"GPU taking a nap","meh..."],[1.34,32,2,8,16,1,16,1,3.161868599439919,3240.3,829516.8,3.86,"GPU taking a nap","meh..."],[8.86,8,1,256,1,8,8,0,3.2059368886430413,531.97,34046.08,9.55,"GPU taking a nap","meh..."],[3.57,32,2,16,8,4,8,1,3.299362379402742,1160.91,297192.96,7.39,"GPU taking a nap","meh..."],[8.86,32,16,4,4,4,16,1,3.189363911889149,529.22,135480.32,30.58,"GPU taking a nap","meh..."],[1.34,16,1,64,4,8,4,1,3.112740389445889,2731.44,349624.32,4.69,"GPU taking a nap","meh..."],[3.57,32,8,2,16,1,16,1,3.277349046333088,1260.82,322769.92,11.46,"GPU taking a nap","meh..."],[3.57,32,2,32,4,8,8,1,3.2642062409999273,1148.54,294026.24,7.82,"GPU taking a nap","meh..."],[3.57,32,8,8,4,4,16,1,3.2557937778089543,1145.58,293268.48,15.68,"GPU taking a nap","meh..."],[3.57,32,2,8,16,2,8,1,3.2539180258812377,1144.92,293099.52,6.18,"GPU taking a nap","meh..."],[8.86,64,1,4,64,2,4,1,3.1467562952891406,522.15,267340.8,17.02,"GPU taking a nap","meh..."],[3.57,32,64,1,4,2,32,1,3.245619244625278,1142,292352,22.38,"GPU taking a nap","meh..."],[3.57,16,8,32,1,8,16,0,3.2451929373689783,1141.85,146156.8,19.19,"GPU taking a nap","meh..."],[8.86,64,4,2,32,1,16,1,3.1397052469974844,520.98,266741.76,12.01,"GPU taking a nap","meh..."],[8.86,16,1,128,2,8,8,1,3.1208421861830535,517.85,66284.8,8.68,"GPU taking a nap","meh..."],[8.86,64,32,1,8,2,32,1,3.1059463592545846,514.86,263608.32,19.41,"GPU taking a nap","meh..."],[80,64,1,16,16,2,16,1,2.348200569161254,45.34,23214.08,35.27,"GPU taking a nap","meh..."],[1.34,32,2,8,16,1,16,0,3.014006380240114,3088.77,790725.12,4.92,"GPU taking a nap","meh..."],[8.86,64,1,4,64,1,8,1,3.074558382012182,510.17,261207.04,13.6,"GPU taking a nap","meh..."],[80,32,2,64,2,16,8,1,2.317643923025278,44.75,11456,50.25,"GPU taking a nap","meh..."],[8.86,64,4,2,32,1,16,0,3.009652578506936,499.4,255692.8,18.71,"GPU taking a nap","meh..."],[3.57,64,4,1,64,1,8,0,3.103027806553043,1196.55,612633.6,18.32,"GPU taking a nap","meh..."],[8.86,32,1,64,4,8,8,1,3.0012756920749686,498.01,127490.56,7.86,"GPU taking a nap","meh..."],[80,64,1,32,8,16,4,1,2.249797810418281,43.44,22241.28,57.15,"GPU taking a nap","meh..."],[3.57,2,1,256,1,1,16,0,3.048785649778411,1172.89,18766.24,6.51,"GPU taking a nap","meh..."],[1.34,8,64,4,1,2,32,0,2.893675689118156,2539.21,162509.44,22.36,"GPU taking a nap","meh..."],[3.57,4,1,128,2,1,16,1,3.042079251926584,1170.31,37449.92,5.22,"GPU taking a nap","meh..."],[1.34,32,2,2,64,2,2,1,2.8833053578848293,2530.11,647708.16,13.58,"GPU taking a nap","meh..."],[3.57,4,1,128,2,1,16,0,3.0311878538648975,1166.12,37315.84,6.89,"GPU taking a nap","meh..."],[3.57,64,1,2,128,2,2,1,3.023768948447126,1063.94,544737.28,17.05,"GPU taking a nap","meh..."],[80,64,1,32,8,8,8,1,2.2119904346907178,42.71,21867.52,34.75,"GPU taking a nap","meh..."],[3.57,8,1,64,4,1,16,1,2.9930029761736865,1151.43,73691.52,4.61,"GPU taking a nap","meh..."],[8.86,32,2,8,16,2,8,1,2.897619255650621,480.81,123087.36,12.65,"GPU taking a nap","meh..."],[8.86,16,1,256,1,16,8,0,2.8633282601125662,475.12,60815.36,8.49,"GPU taking a nap","meh..."],[8.86,8,16,16,1,2,32,0,2.8629530807890315,474.58,30373.12,19.14,"GPU taking a nap","meh..."],[3.57,8,1,64,4,1,16,0,2.9554419494454356,1136.98,72766.72,6.86,"GPU taking a nap","meh..."],[3.57,16,1,32,8,1,16,1,2.9320735321245377,1127.99,144382.72,4.28,"GPU taking a nap","meh..."],[80,64,2,32,4,16,8,1,2.142072685057553,41.36,21176.32,47.85,"GPU taking a nap","meh..."],[3.57,64,8,1,32,1,16,1,2.9143457672604085,1121.17,574039.04,11.42,"GPU taking a nap","meh..."],[3.57,16,1,32,8,1,16,0,2.901374865988853,1116.18,142871.04,6.91,"GPU taking a nap","meh..."],[1.34,8,2,64,2,4,8,1,2.75102095683378,2414.03,154497.92,4.02,"GPU taking a nap","meh..."],[3.57,64,2,2,64,2,4,1,2.8792792090454196,1013.1,518707.2,12.77,"GPU taking a nap","meh..."],[8.86,4,1,64,4,4,2,0,2.7798607226429604,461.27,14760.64,42.41,"GPU taking a nap","meh..."],[80,32,4,32,2,4,32,1,2.0944250334556926,40.44,10352.64,25.38,"GPU taking a nap","meh..."],[3.57,32,1,16,16,1,16,1,2.848867409939672,1095.98,280570.88,4.09,"GPU taking a nap","meh..."],[1.34,8,2,32,4,2,8,1,2.7009698636724933,2370.11,151687.04,3.37,"GPU taking a nap","meh..."],[1.34,64,2,4,32,1,16,1,2.696912030928631,2763.81,1415070.72,3.79,"GPU taking a nap","meh..."],[8.86,16,32,8,1,4,32,0,2.741999375892449,454.53,58179.84,32.44,"GPU taking a nap","meh..."],[1.34,4,2,64,2,2,8,1,2.6687534610388512,2341.84,74938.88,3.71,"GPU taking a nap","meh..."],[3.57,32,1,16,16,1,16,0,2.803820171655895,1078.65,276134.4,6.93,"GPU taking a nap","meh..."],[1.34,16,64,2,2,2,32,1,2.6597620419805055,2333.95,298745.6,21.63,"GPU taking a nap","meh..."],[8.86,64,2,4,32,2,8,1,2.701214414774487,448.22,229488.64,12.48,"GPU taking a nap","meh..."],[3.57,64,8,2,16,1,32,0,2.7506888646437533,1058.21,541803.52,17.9,"GPU taking a nap","meh..."],[8.86,16,16,8,2,2,32,1,2.6481320617682185,438.97,56188.16,17.48,"GPU taking a nap","meh..."],[1.34,16,2,32,4,4,8,1,2.591602755050257,2274.14,291089.92,3.64,"GPU taking a nap","meh..."],[1.34,64,2,4,32,1,16,0,2.5835734394942125,2647.66,1355601.92,4.73,"GPU taking a nap","meh..."],[3.57,64,8,1,32,1,16,0,2.6831829916933954,1032.24,528506.88,14.56,"GPU taking a nap","meh..."],[1.34,4,8,32,1,2,16,0,2.5424291514548676,2230.99,71391.68,5.69,"GPU taking a nap","meh..."],[80,64,4,16,4,4,32,1,1.966501447089828,37.97,19440.64,23.95,"GPU taking a nap","meh..."],[8.86,8,1,32,8,4,2,0,2.555131157860174,423.98,27134.72,42.46,"GPU taking a nap","meh..."],[3.57,4,1,32,8,4,1,0,2.6352040945721984,927.22,29671.04,42.3,"GPU taking a nap","meh..."],[8.86,4,1,64,4,8,1,0,2.5319289904047237,420.13,13444.16,65.76,"GPU taking a nap","meh..."],[3.57,64,16,1,16,2,16,1,2.6066983493676523,917.19,469601.28,10.39,"GPU taking a nap","meh..."],[80,32,2,64,2,8,16,1,1.913156793666006,36.94,9456.64,26.39,"GPU taking a nap","meh..."],[8.86,8,2,32,4,4,4,1,2.5177666284513975,417.78,26737.92,23.44,"GPU taking a nap","meh..."],[80,64,4,32,2,16,16,1,1.9079777011005867,36.84,18862.08,44.38,"GPU taking a nap","meh..."],[3.57,16,32,4,2,2,32,1,2.5947049052237654,912.97,116860.16,19.97,"GPU taking a nap","meh..."],[3.57,8,32,8,1,2,32,0,2.5845872130075955,909.41,58202.24,20.71,"GPU taking a nap","meh..."],[8.86,4,64,4,1,1,32,0,2.4687224266747343,409.23,13095.36,25.61,"GPU taking a nap","meh..."],[8.86,32,1,8,32,4,2,1,2.4490641066352596,406.38,104033.28,26.83,"GPU taking a nap","meh..."],[1.34,64,16,1,16,1,32,1,2.3830573210795873,2442.17,1250391.04,15.98,"GPU taking a nap","meh..."],[1.34,8,8,16,2,2,16,1,2.3677973318730032,2077.75,132976,5.33,"GPU taking a nap","meh..."],[3.57,8,1,16,16,4,1,0,2.4749409866874177,870.83,55733.12,42.27,"GPU taking a nap","meh..."],[80,64,2,32,4,8,16,1,1.814754034923033,35.04,17940.48,26.22,"GPU taking a nap","meh..."],[3.57,64,2,2,64,1,8,1,2.458460039791305,948,485376,8.83,"GPU taking a nap","meh..."],[3.57,64,4,2,32,2,8,1,2.4492204488906935,861.78,441231.36,10.17,"GPU taking a nap","meh..."],[1.34,8,8,32,1,4,16,0,2.3144528038475065,2030.94,129980.16,7.82,"GPU taking a nap","meh..."],[8.86,64,2,8,16,1,32,0,2.3604371896226817,391.28,200335.36,10.96,"GPU taking a nap","meh..."],[8.86,32,32,4,2,4,32,1,2.354525237961901,390.3,99916.8,31.34,"GPU taking a nap","meh..."],[8.86,32,16,4,4,2,32,1,2.333592102999749,386.83,99028.48,16.85,"GPU taking a nap","meh..."],[8.86,64,1,2,128,1,4,0,2.3270267323565945,386.13,197698.56,49.53,"GPU taking a nap","meh..."],[1.34,32,8,4,8,2,16,1,2.2692905811247885,1991.31,509775.36,5.29,"GPU taking a nap","meh..."],[8.86,32,1,128,2,16,8,1,2.2658573817922263,375.98,96250.88,8.74,"GPU taking a nap","meh..."],[3.57,4,4,32,2,8,2,1,2.336732174195187,822.2,26310.4,40.87,"GPU taking a nap","meh..."],[3.57,16,64,4,1,4,32,0,2.336106923552615,821.98,105213.44,37.19,"GPU taking a nap","meh..."],[1.34,32,8,8,4,4,16,1,2.1637639138493996,1898.71,486069.76,7.71,"GPU taking a nap","meh..."],[8.86,32,2,8,16,4,4,1,2.206797319178354,366.18,93742.08,21,"GPU taking a nap","meh..."],[8.86,32,4,8,8,4,8,1,2.1814858637724086,361.98,92666.88,17.63,"GPU taking a nap","meh..."],[8.86,64,1,4,64,4,2,1,2.168227482369294,359.78,184207.36,25.76,"GPU taking a nap","meh..."],[1.34,16,8,8,4,2,16,1,2.120812508994006,1861.02,238210.56,5.32,"GPU taking a nap","meh..."],[3.57,16,4,8,8,4,4,1,2.223192341600809,782.25,100128,20.92,"GPU taking a nap","meh..."],[1.34,16,2,16,8,2,8,1,2.1079578566520363,1849.74,236766.72,3.27,"GPU taking a nap","meh..."],[1.34,16,8,16,2,4,16,1,2.0991715650246796,1842.03,235779.84,9.73,"GPU taking a nap","meh..."],[3.57,32,32,2,4,2,32,1,2.2107157492331164,777.86,199132.16,19.46,"GPU taking a nap","meh..."],[1.34,32,2,8,16,2,8,1,2.0985561827317127,1841.49,471421.44,3.3,"GPU taking a nap","meh..."],[8.86,64,16,2,8,2,32,1,2.1212444515104876,351.63,180034.56,16.4,"GPU taking a nap","meh..."],[1.34,16,2,64,2,8,8,1,2.0785220703051324,1823.91,233460.48,4.69,"GPU taking a nap","meh..."],[3.57,8,1,64,4,2,8,1,2.1530221672139453,757.56,48483.84,4.88,"GPU taking a nap","meh..."],[3.57,2,1,256,1,2,8,0,2.148730674167199,756.05,12096.8,7.12,"GPU taking a nap","meh..."],[1.34,64,1,1,256,2,1,1,2.0349894710619374,1785.71,914283.52,13.02,"GPU taking a nap","meh..."],[3.57,4,1,128,2,2,8,1,2.139579278398641,752.83,24090.56,5.76,"GPU taking a nap","meh..."],[80,16,2,64,2,2,32,1,1.5578710436782204,30.08,3850.24,24.29,"GPU taking a nap","meh..."],[8.86,64,2,4,32,4,4,1,2.0534822178623418,340.74,174458.88,19.86,"GPU taking a nap","meh..."],[80,16,1,128,2,4,16,0,1.5568352251651365,30.06,3847.68,35.71,"GPU taking a nap","meh..."],[3.57,64,2,2,64,1,8,0,2.112200984397913,814.48,417013.76,13.91,"GPU taking a nap","meh..."],[3.57,64,4,4,16,1,32,1,2.0965811299796,806.57,412963.84,8.73,"GPU taking a nap","meh..."],[3.57,16,1,64,4,4,8,1,2.076201599628813,730.53,93507.84,4.63,"GPU taking a nap","meh..."],[8.86,32,2,8,16,8,2,1,2.0107540705223053,333.65,85414.4,38.88,"GPU taking a nap","meh..."],[3.57,4,1,256,1,4,8,0,2.0677038749865804,727.54,23281.28,6.27,"GPU taking a nap","meh..."],[1.34,2,1,256,1,1,16,0,1.957191101021703,2005.74,32091.84,3.14,"GPU taking a nap","meh..."],[80,64,1,32,8,4,16,1,1.512295029102528,29.2,14950.4,23.73,"GPU taking a nap","meh..."],[80,32,2,32,4,2,32,1,1.51022339207636,29.16,7464.96,21.78,"GPU taking a nap","meh..."],[1.34,4,1,128,2,1,16,0,1.9416271704717332,1989.79,63673.28,3.32,"GPU taking a nap","meh..."],[3.57,8,1,256,1,8,8,0,2.0395675960708273,717.64,45928.96,4.78,"GPU taking a nap","meh..."],[1.34,8,1,64,4,1,16,1,1.932932836854164,1980.88,126776.32,2.38,"GPU taking a nap","meh..."],[1.34,4,1,128,2,1,16,1,1.9323668757432555,1980.3,63369.6,2.71,"GPU taking a nap","meh..."],[1.34,64,16,1,16,1,32,0,1.9279074925073083,1975.73,1011573.76,15.71,"GPU taking a nap","meh..."],[8.86,32,1,8,32,8,1,1,1.9656755737517169,326.17,83499.52,45.26,"GPU taking a nap","meh..."],[8.86,64,4,4,16,4,8,1,1.9628431013610517,325.7,166758.4,17.3,"GPU taking a nap","meh..."],[3.57,32,1,4,64,4,1,1,2.024760524035365,712.43,182382.08,26.78,"GPU taking a nap","meh..."],[1.34,16,2,4,32,4,1,1,1.9207790758746857,1685.49,215742.72,25.53,"GPU taking a nap","meh..."],[1.34,32,2,32,4,8,8,1,1.916015561088388,1681.31,430415.36,4.63,"GPU taking a nap","meh..."],[1.34,16,1,32,8,1,16,1,1.9137096749836684,1961.18,251031.04,2.26,"GPU taking a nap","meh..."],[8.86,32,4,8,8,2,16,1,1.950669496618192,323.68,82862.08,10.13,"GPU taking a nap","meh..."],[80,64,2,16,8,2,32,1,1.4734518348618806,28.45,14566.4,21.7,"GPU taking a nap","meh..."],[3.57,32,2,4,32,4,2,1,1.989291760311264,699.95,179187.2,21.58,"GPU taking a nap","meh..."],[1.34,64,8,1,32,2,8,1,1.886397439247272,1655.32,847523.84,6.09,"GPU taking a nap","meh..."],[3.57,8,4,32,2,2,16,1,1.9793161705138609,696.44,44572.16,6.49,"GPU taking a nap","meh..."],[1.34,16,1,32,8,1,16,0,1.8763757575641171,1922.92,246133.76,3.27,"GPU taking a nap","meh..."],[3.57,32,4,4,16,4,4,1,1.976218337784752,695.35,178009.6,20.62,"GPU taking a nap","meh..."],[8.86,32,4,8,8,8,4,1,1.9125817827692455,317.36,81244.16,36.42,"GPU taking a nap","meh..."],[3.57,4,64,4,1,1,32,0,1.955226899288785,752.19,24070.08,46,"GPU taking a nap","meh..."],[8.86,64,4,4,16,2,16,1,1.8752172533604696,311.16,159313.92,10.29,"GPU taking a nap","meh..."],[1.34,4,128,2,1,1,32,0,1.833596903939624,1879.08,60130.56,62.9,"GPU taking a nap","meh..."],[1.34,32,1,16,16,1,16,1,1.8305914552816984,1876,480256,2.23,"GPU taking a nap","meh..."],[3.57,16,2,8,16,8,1,1,1.9279603563716723,678.37,86831.36,38.86,"GPU taking a nap","meh..."],[80,64,4,32,2,8,32,1,1.404051994485258,27.11,13880.32,22.36,"GPU taking a nap","meh..."],[3.57,4,1,64,4,2,4,0,1.90718498274802,671.06,21473.92,15.79,"GPU taking a nap","meh..."],[1.34,32,8,16,2,8,16,1,1.79598182605473,1575.98,403450.88,11.17,"GPU taking a nap","meh..."],[3.57,8,1,32,8,2,4,0,1.8900474310447888,665.03,42561.92,15.7,"GPU taking a nap","meh..."],[3.57,16,4,32,2,4,16,1,1.8804981485036845,661.67,84693.76,8.35,"GPU taking a nap","meh..."],[3.57,32,1,128,32,1,8,0,1.8775580894608697,724,185344,10.92,"GPU taking a nap","meh..."],[8.86,32,8,8,4,8,8,1,1.816217456116611,301.37,77150.72,36.6,"GPU taking a nap","meh..."],[3.57,16,4,16,4,2,16,1,1.8644689956668312,656.03,83971.84,5.85,"GPU taking a nap","meh..."],[8.86,8,2,64,2,2,16,1,1.8040438513737516,299.35,19158.4,7.83,"GPU taking a nap","meh..."],[1.34,32,1,16,16,1,16,0,1.766901314404174,1810.73,463546.88,3.29,"GPU taking a nap","meh..."],[3.57,16,1,16,16,2,4,0,1.84508622574709,649.21,83098.88,15.14,"GPU taking a nap","meh..."],[8.86,32,8,8,4,4,16,1,1.785301321299349,296.24,75837.44,17.11,"GPU taking a nap","meh..."],[8.86,16,2,32,4,2,16,1,1.769572059725654,293.63,37584.64,7.01,"GPU taking a nap","meh..."],[1.34,64,8,1,32,1,16,0,1.7174382649008535,1760.04,901140.48,8.07,"GPU taking a nap","meh..."],[1.34,16,8,32,1,8,16,0,1.7157200206961376,1505.55,192710.4,11.22,"GPU taking a nap","meh..."],[8.86,64,2,4,32,1,16,1,1.7344976143774156,287.81,147358.72,8.2,"GPU taking a nap","meh..."],[1.34,64,8,2,16,1,32,1,1.697112454659106,1739.21,890475.52,9.54,"GPU taking a nap","meh..."],[1.34,32,1,4,64,2,2,1,1.6964608341199596,1488.65,381094.4,9.07,"GPU taking a nap","meh..."],[3.57,64,2,1,128,1,4,0,1.7615851643770437,679.28,347791.36,27.66,"GPU taking a nap","meh..."],[1.34,64,2,1,128,1,4,0,1.6688924627152113,1710.29,875668.48,12.29,"GPU taking a nap","meh..."],[3.57,32,1,128,2,16,8,1,1.7560448501480446,617.88,158177.28,4.38,"GPU taking a nap","meh..."],[3.57,64,16,1,16,1,32,0,1.7544508643759755,674.95,345574.4,12.96,"GPU taking a nap","meh..."],[3.57,32,1,32,8,4,8,1,1.751327049844999,616.22,157752.32,4.58,"GPU taking a nap","meh..."],[3.57,32,1,8,32,2,4,1,1.7466092495419532,614.56,157327.36,9.32,"GPU taking a nap","meh..."],[8.86,4,1,128,2,2,8,0,1.6883946063165869,280.16,8965.12,15.76,"GPU taking a nap","meh..."],[3.57,32,1,16,16,2,8,1,1.7420051311739213,612.94,156912.64,4.61,"GPU taking a nap","meh..."],[3.57,16,1,256,1,16,8,0,1.733649508950455,610,78080,4.84,"GPU taking a nap","meh..."],[3.57,64,8,2,16,2,16,1,1.7187003344962264,604.74,309626.88,10.13,"GPU taking a nap","meh..."],[3.57,32,4,16,4,4,16,1,1.71258993048927,602.59,154263.04,8.27,"GPU taking a nap","meh..."],[3.57,32,1,64,4,8,8,1,1.7009943731179291,598.51,153218.56,4.48,"GPU taking a nap","meh..."],[3.57,32,4,8,8,2,16,1,1.6909051013855123,594.96,152309.76,5.55,"GPU taking a nap","meh..."],[8.86,16,1,32,8,2,8,0,1.6242722526215247,269.52,34498.56,15.72,"GPU taking a nap","meh..."],[8.86,8,1,64,4,2,8,0,1.622765618371171,269.27,17233.28,15.8,"GPU taking a nap","meh..."],[3.57,32,4,32,2,8,16,1,1.6655824503613343,586.05,150028.8,9.97,"GPU taking a nap","meh..."],[3.57,16,1,128,2,8,8,1,1.6653835069750618,585.98,75005.44,4.78,"GPU taking a nap","meh..."],[8.86,32,16,8,2,8,16,1,1.609205910117986,267.02,68357.12,36.71,"GPU taking a nap","meh..."],[8.86,8,1,128,2,1,32,1,1.602018247996829,265.56,16995.84,6.29,"GPU taking a nap","meh..."],[8.86,16,2,64,2,4,16,1,1.5989005318455651,265.31,33959.68,8.34,"GPU taking a nap","meh..."],[3.57,8,1,128,2,4,8,1,1.6425050175536662,577.93,36987.52,5.37,"GPU taking a nap","meh..."],[8.86,64,8,4,8,4,16,1,1.5846779045222246,262.95,134630.4,16.77,"GPU taking a nap","meh..."],[8.86,4,1,256,1,1,32,0,1.5830155462300346,262.41,8397.12,7.56,"GPU taking a nap","meh..."],[8.86,8,1,128,2,1,32,0,1.5771035945692538,261.43,16731.52,8.07,"GPU taking a nap","meh..."],[80,64,1,64,4,16,8,1,1.18290474194184,22.84,11694.08,30.04,"GPU taking a nap","meh..."],[3.57,64,1,4,64,2,4,1,1.6074625610858646,565.6,289587.2,8.97,"GPU taking a nap","meh..."],[3.57,64,32,1,8,2,32,1,1.6046773536780423,564.62,289085.44,11.16,"GPU taking a nap","meh..."],[8.86,16,1,64,4,1,32,1,1.549775899647482,256.9,32883.2,5.48,"GPU taking a nap","meh..."],[8.86,16,1,64,4,1,32,0,1.5464579675929628,256.35,32812.8,8.12,"GPU taking a nap","meh..."],[1.34,64,16,1,16,2,16,1,1.5008148488301234,1316.97,674288.64,6.19,"GPU taking a nap","meh..."],[80,64,1,64,4,32,4,1,1.158045097627826,22.36,11448.32,49.36,"GPU taking a nap","meh..."],[8.86,16,16,16,1,4,32,0,1.5099607149932457,250.3,32038.4,17.81,"GPU taking a nap","meh..."],[1.34,32,2,4,32,2,4,1,1.48007418636347,1298.77,332485.12,6.96,"GPU taking a nap","meh..."],[1.34,64,2,2,64,1,8,0,1.4688056941088614,1505.24,770682.88,6.39,"GPU taking a nap","meh..."],[1.34,16,32,4,2,2,32,1,1.4565301156733024,1278.11,163598.08,11.01,"GPU taking a nap","meh..."],[80,64,2,64,2,32,8,1,1.107289990486714,21.38,10946.56,45.38,"GPU taking a nap","meh..."],[3.57,64,1,2,128,1,4,0,1.5025132677787827,579.38,296642.56,21.91,"GPU taking a nap","meh..."],[8.86,8,8,32,1,2,32,0,1.4536161961037657,240.96,15421.44,11.18,"GPU taking a nap","meh..."],[3.57,64,4,2,32,1,16,0,1.4862989254634258,571.79,292756.48,9.34,"GPU taking a nap","meh..."],[80,32,2,64,2,4,32,1,1.0891631665077453,21.03,5383.68,17.11,"GPU taking a nap","meh..."],[8.86,64,1,8,32,2,8,1,1.4352800522571325,238.16,121937.92,9.6,"GPU taking a nap","meh..."],[8.86,16,8,16,2,2,32,1,1.4329846913283883,237.54,30405.12,9.96,"GPU taking a nap","meh..."],[8.86,64,1,16,16,1,32,0,1.431235236245096,237.25,121472,8.18,"GPU taking a nap","meh..."],[3.57,8,16,16,1,2,32,0,1.474426276636732,518.79,33202.56,11.32,"GPU taking a nap","meh..."],[80,64,2,32,4,4,32,1,1.0720721610418604,20.7,10598.4,16.21,"GPU taking a nap","meh..."],[8.86,32,16,8,2,4,32,1,1.4065618869669403,233.16,59688.96,16.94,"GPU taking a nap","meh..."],[1.34,2,1,256,1,2,8,0,1.3726557883356278,1204.51,19272.16,3.26,"GPU taking a nap","meh..."],[1.34,8,1,64,4,2,8,1,1.3696700446179009,1201.89,76920.96,2.28,"GPU taking a nap","meh..."],[1.34,64,1,2,128,2,2,1,1.3656928516503943,1198.4,613580.8,9.07,"GPU taking a nap","meh..."],[80,64,2,64,2,16,16,1,1.052909518549808,20.33,10408.96,24.45,"GPU taking a nap","meh..."],[1.34,32,32,2,4,2,32,1,1.3513225355127847,1185.79,303562.24,11,"GPU taking a nap","meh..."],[1.34,8,32,8,1,2,32,0,1.347128819145901,1182.11,75655.04,12.36,"GPU taking a nap","meh..."],[1.34,16,1,32,8,2,8,1,1.335607495105359,1172,150016,2.15,"GPU taking a nap","meh..."],[8.86,32,8,8,4,2,32,1,1.35021736807746,223.82,57297.92,9.38,"GPU taking a nap","meh..."],[1.34,4,1,128,2,2,8,1,1.3226616750162832,1160.64,37140.48,2.6,"GPU taking a nap","meh..."],[3.57,16,16,8,2,2,32,1,1.3818039200847725,486.2,62233.6,10.58,"GPU taking a nap","meh..."],[3.57,64,1,4,64,1,8,0,1.3715509977266087,528.88,270786.56,10.94,"GPU taking a nap","meh..."],[1.34,32,1,16,16,2,8,1,1.301613321403147,1142.17,292395.52,2.13,"GPU taking a nap","meh..."],[1.34,16,1,256,1,8,16,0,1.2986617655905843,1139.58,145866.24,2.3,"GPU taking a nap","meh..."],[8.86,8,1,64,4,4,4,0,1.316255946479175,218.41,13978.24,21.39,"GPU taking a nap","meh..."],[3.57,64,2,4,32,2,8,1,1.3475288166783097,474.14,242759.68,6.14,"GPU taking a nap","meh..."],[3.57,16,32,8,1,4,32,0,1.3413615717038463,471.97,60412.16,20.76,"GPU taking a nap","meh..."],[3.57,4,1,64,4,4,2,0,1.325161895964473,466.27,14920.64,22.45,"GPU taking a nap","meh..."],[8.86,8,1,64,4,8,2,0,1.271358245818629,210.96,13501.44,33.48,"GPU taking a nap","meh..."],[80,64,1,64,4,8,16,1,.9576142153460868,18.49,9466.88,17.9,"GPU taking a nap","meh..."],[8.86,16,1,32,8,4,4,0,1.254785269064736,208.21,26650.88,21.06,"GPU taking a nap","meh..."],[3.57,32,16,4,4,2,32,1,1.2858847874174315,452.45,115827.2,10,"GPU taking a nap","meh..."],[1.34,8,4,32,2,2,16,1,1.2150495455258403,1066.21,68237.44,3.21,"GPU taking a nap","meh..."],[1.34,16,4,32,2,4,16,1,1.2084056959554783,1060.38,135728.64,4.28,"GPU taking a nap","meh..."],[1.34,16,64,4,1,4,32,0,1.2004171221153004,1053.37,134831.36,24.32,"GPU taking a nap","meh..."],[3.57,64,2,8,16,1,32,1,1.2541743796618483,482.49,247034.88,4.75,"GPU taking a nap","meh..."],[8.86,8,1,64,4,16,1,0,1.2118763256146572,201.09,12869.76,62.56,"GPU taking a nap","meh..."],[8.86,64,16,4,4,4,32,1,1.2075462897331122,200.17,102487.04,16.03,"GPU taking a nap","meh..."],[1.34,32,1,128,32,1,8,0,1.182497677640736,1211.83,310228.48,4.71,"GPU taking a nap","meh..."],[8.86,16,1,32,8,8,2,0,1.202535193262463,199.54,25541.12,33.99,"GPU taking a nap","meh..."],[1.34,4,1,256,1,4,8,0,1.175927186048906,1031.88,33020.16,3.26,"GPU taking a nap","meh..."],[1.34,64,4,2,32,2,8,1,1.173363093161545,1029.63,527170.56,5.83,"GPU taking a nap","meh..."],[1.34,16,4,16,4,2,16,1,1.1667762234331245,1023.85,131052.8,3.01,"GPU taking a nap","meh..."],[3.57,8,1,32,8,4,2,0,1.225832305246738,431.32,27604.48,22.46,"GPU taking a nap","meh..."],[8.86,64,8,4,8,2,32,1,1.1721349058057835,194.3,99481.6,9.27,"GPU taking a nap","meh..."],[3.57,16,1,16,16,4,2,0,1.2059095461356846,424.31,54311.68,22.19,"GPU taking a nap","meh..."],[1.34,8,1,128,2,4,8,1,1.1371353096552317,997.84,63861.76,2.85,"GPU taking a nap","meh..."],[80,64,1,16,16,1,32,1,.8788920083517088,16.97,8688.64,30.53,"GPU taking a nap","meh..."],[3.57,4,1,64,4,8,1,0,1.1829742157467826,416.24,13319.68,33.53,"GPU taking a nap","meh..."],[3.57,8,1,32,8,8,1,0,1.1812974072053388,415.65,26601.6,33.49,"GPU taking a nap","meh..."],[3.57,64,2,8,16,1,32,0,1.173983537131292,451.64,231239.68,6.13,"GPU taking a nap","meh..."],[8.86,64,1,8,32,4,4,1,1.12461206983416,186.61,95544.32,13.08,"GPU taking a nap","meh..."],[1.34,32,4,16,4,4,16,1,1.0877907665340183,954.54,244362.24,4.12,"GPU taking a nap","meh..."],[3.57,32,32,4,2,4,32,1,1.145373915741185,403.01,103170.56,18.75,"GPU taking a nap","meh..."],[1.34,16,1,64,4,4,8,1,1.086651169695191,953.54,122053.12,2.51,"GPU taking a nap","meh..."],[8.86,16,1,32,8,16,1,0,1.1060503358697995,183.53,23491.84,62.56,"GPU taking a nap","meh..."],[1.34,8,1,256,1,8,8,0,1.077386247395527,945.41,60506.24,2.72,"GPU taking a nap","meh..."],[8.86,32,2,16,8,4,8,1,1.0873680711654117,180.43,46190.08,11.14,"GPU taking a nap","meh..."],[3.57,64,16,2,8,2,32,1,1.1129461437786856,391.6,200499.2,9.99,"GPU taking a nap","meh..."],[8.86,32,2,16,8,8,4,1,1.0777858773331608,178.84,45783.04,20.92,"GPU taking a nap","meh..."],[3.57,32,2,8,16,4,4,1,1.1067504783204691,389.42,99691.52,11.51,"GPU taking a nap","meh..."],[80,16,1,128,2,2,32,1,.80793844020546,15.6,1996.8,21.82,"GPU taking a nap","meh..."],[1.34,32,1,32,8,4,8,1,1.040087242860716,912.68,233646.08,2.04,"GPU taking a nap","meh..."],[3.57,32,1,8,32,4,2,1,1.0938759991802305,384.89,98531.84,14.8,"GPU taking a nap","meh..."],[1.34,64,4,4,16,1,32,0,1.0363723735744064,1062.08,543784.96,5.74,"GPU taking a nap","meh..."],[3.57,16,2,16,8,8,2,1,1.085975104696817,382.11,48910.08,21.02,"GPU taking a nap","meh..."],[8.86,64,2,8,16,4,8,1,1.0456041697456016,173.5,88832,10.87,"GPU taking a nap","meh..."],[8.86,32,4,16,4,8,8,1,1.0420485129147663,172.91,44264.96,19.75,"GPU taking a nap","meh..."],[3.57,16,1,16,16,8,1,0,1.0756868895781273,378.49,48446.72,33.51,"GPU taking a nap","meh..."],[1.34,8,1,16,16,4,1,1,1.014275374461282,890.03,56961.92,15.94,"GPU taking a nap","meh..."],[80,16,1,128,2,2,32,0,.7665056996821031,14.8,1894.4,30.31,"GPU taking a nap","meh..."],[8.86,32,2,16,8,2,16,1,1.0075164558966552,167.18,42798.08,6.55,"GPU taking a nap","meh..."],[3.57,64,1,4,64,4,2,1,1.0352161207134478,364.25,186496,13.64,"GPU taking a nap","meh..."],[8.86,64,2,8,16,2,16,1,.9931732978332862,164.8,84377.6,6.38,"GPU taking a nap","meh..."],[8.86,64,1,8,32,8,2,1,.9903408254426208,164.33,84136.96,22.87,"GPU taking a nap","meh..."],[80,64,2,64,2,8,32,1,.7457893294204246,14.4,7372.8,13.47,"GPU taking a nap","meh..."],[3.57,64,2,4,32,4,4,1,1.0076482514727605,354.55,181529.6,11.4,"GPU taking a nap","meh..."],[80,64,1,32,8,2,32,1,.7354311442895854,14.2,7270.4,18.49,"GPU taking a nap","meh..."],[8.86,64,2,8,16,8,4,1,.9658730852168737,160.27,82058.24,19.86,"GPU taking a nap","meh..."],[1.34,64,4,2,32,1,16,0,.9359825820395552,959.2,491110.4,4.57,"GPU taking a nap","meh..."],[1.34,32,1,4,64,4,1,1,.934697327205986,820.2,209971.2,15.82,"GPU taking a nap","meh..."],[8.86,32,8,16,2,8,16,1,.9477332088426128,157.26,40258.56,18.92,"GPU taking a nap","meh..."],[3.57,16,2,32,4,2,16,1,.9745952288676886,342.92,43893.76,3.68,"GPU taking a nap","meh..."],[1.34,32,1,64,4,8,8,1,.9224694531253712,809.47,207224.32,2.55,"GPU taking a nap","meh..."],[1.34,64,8,2,16,2,16,1,.9200193199218928,807.32,413347.84,5.68,"GPU taking a nap","meh..."],[3.57,16,2,64,2,4,16,1,.9675469488968736,340.44,43576.32,4.78,"GPU taking a nap","meh..."],[8.86,64,4,8,8,8,8,1,.9367046461300222,155.43,79580.16,18.41,"GPU taking a nap","meh..."],[8.86,32,4,16,4,4,16,1,.9315820896788192,154.58,39572.48,9.51,"GPU taking a nap","meh..."],[8.86,32,4,16,4,16,4,1,.9306781091286068,154.43,39534.08,41.41,"GPU taking a nap","meh..."],[80,32,1,64,4,2,32,1,.7053924074101516,13.62,3486.72,19.03,"GPU taking a nap","meh..."],[1.34,16,1,128,2,8,8,1,.9100478475821556,798.57,102216.96,2.62,"GPU taking a nap","meh..."],[1.34,32,4,4,16,4,4,1,.908418224102633,797.14,204067.84,12.44,"GPU taking a nap","meh..."],[8.86,8,1,128,2,2,16,1,.9267005947076724,153.77,9841.28,5.89,"GPU taking a nap","meh..."],[3.57,32,1,8,32,8,1,1,.9558092891067664,336.31,86095.36,23.39,"GPU taking a nap","meh..."],[3.57,8,2,64,2,2,16,1,.9510630683199676,334.64,21416.96,4.1,"GPU taking a nap","meh..."],[1.34,32,4,32,2,8,16,1,.9012387640180224,790.84,202455.04,5.81,"GPU taking a nap","meh..."],[8.86,32,2,16,8,16,2,1,.9192879541959312,152.54,39050.24,42.3,"GPU taking a nap","meh..."],[3.57,64,4,4,16,2,16,1,.94753892833456,333.4,170700.8,5.52,"GPU taking a nap","meh..."],[8.86,16,1,64,4,2,16,1,.9146475207048412,151.77,19426.56,5.06,"GPU taking a nap","meh..."],[8.86,16,1,128,2,4,16,1,.909524964253638,150.92,19317.76,5.25,"GPU taking a nap","meh..."],[8.86,8,1,256,1,4,16,0,.908741514443454,150.79,9650.56,5.76,"GPU taking a nap","meh..."],[1.34,16,1,16,16,2,4,1,.8823784363354338,774.29,99109.12,4.34,"GPU taking a nap","meh..."],[1.34,32,2,4,32,4,2,1,.8818314298527967,773.81,198095.36,13.38,"GPU taking a nap","meh..."],[8.86,32,8,16,2,16,8,1,.8993401167212457,149.23,38202.88,41.2,"GPU taking a nap","meh..."],[3.57,64,4,4,16,4,8,1,.9275309077722468,326.36,167096.32,9.84,"GPU taking a nap","meh..."],[3.57,64,8,2,16,1,32,1,.9222336797865804,354.79,181652.48,6,"GPU taking a nap","meh..."],[8.86,64,4,8,8,4,16,1,.887045981238358,147.19,75361.28,9.22,"GPU taking a nap","meh..."],[3.57,32,2,32,4,4,16,1,.8956715454848129,315.15,80678.4,4.59,"GPU taking a nap","meh..."],[8.86,32,1,32,8,2,16,1,.8591431149218038,142.56,36495.36,4.64,"GPU taking a nap","meh..."],[3.57,32,2,16,8,2,16,1,.8849854435935773,311.39,79715.84,3.49,"GPU taking a nap","meh..."],[8.86,64,1,8,32,1,16,0,.8529960471803599,141.54,72468.48,12.35,"GPU taking a nap","meh..."],[8.86,32,1,64,4,4,16,1,.8387131544870049,139.17,35627.52,4.12,"GPU taking a nap","meh..."],[8.86,16,1,256,1,8,16,0,.8358204167263256,138.69,17752.32,5.08,"GPU taking a nap","meh..."],[8.86,4,1,256,1,2,16,0,.8348561708060991,138.53,4432.96,7.31,"GPU taking a nap","meh..."],[1.34,16,2,8,16,8,1,1,.8166009067983336,716.57,91720.96,24.08,"GPU taking a nap","meh..."],[8.86,64,8,8,4,8,16,1,.8297336143548959,137.68,70492.16,17.05,"GPU taking a nap","meh..."],[1.34,16,4,8,8,8,2,1,.8122248549372376,712.73,91229.44,21.99,"GPU taking a nap","meh..."],[1.34,16,8,8,4,8,4,1,.8092277152511224,710.1,90892.8,21.3,"GPU taking a nap","meh..."],[3.57,4,1,128,2,2,8,0,.8430652300574802,296.64,9492.48,7.83,"GPU taking a nap","meh..."],[3.57,8,1,64,4,2,8,0,.842752604736194,296.53,18977.92,7.82,"GPU taking a nap","meh..."],[3.57,16,1,32,8,2,8,1,.833515947516376,293.28,37539.84,4.67,"GPU taking a nap","meh..."],[80,64,1,128,2,32,8,1,.6106150134629726,11.79,6036.48,26.23,"GPU taking a nap","meh..."],[1.34,32,1,8,32,2,4,1,.7888403278045055,692.21,177205.76,4.12,"GPU taking a nap","meh..."],[1.34,8,16,16,1,2,32,0,.7833360750729707,687.38,43992.32,6.62,"GPU taking a nap","meh..."],[3.57,64,2,4,32,1,16,1,.8203640162931446,315.6,161587.2,4.11,"GPU taking a nap","meh..."],[8.86,16,8,32,1,4,32,0,.7886422863406592,130.73,16733.44,9.8,"GPU taking a nap","meh..."],[1.34,16,16,8,2,2,32,1,.7447379301418968,653.51,83649.28,6.47,"GPU taking a nap","meh..."],[3.57,4,1,256,1,1,32,0,.7816592627994662,300.71,9622.72,4,"GPU taking a nap","meh..."],[3.57,8,1,128,2,1,32,1,.777058361947632,298.94,19132.16,3.4,"GPU taking a nap","meh..."],[3.57,8,1,128,2,1,32,0,.7764345109846714,298.7,19116.8,4.18,"GPU taking a nap","meh..."],[3.57,64,8,4,8,4,16,1,.7749981714683599,272.69,139617.28,10.15,"GPU taking a nap","meh..."],[1.34,64,1,4,64,2,4,1,.7338661762994864,643.97,329712.64,4.08,"GPU taking a nap","meh..."],[3.57,16,1,64,4,1,32,1,.7725354424661679,297.2,38041.6,3.05,"GPU taking a nap","meh..."],[3.57,16,1,64,4,1,32,0,.7649972433303943,294.3,37670.4,4.18,"GPU taking a nap","meh..."],[3.57,16,1,32,8,2,8,0,.7631752502269928,268.53,34371.84,7.68,"GPU taking a nap","meh..."],[80,64,1,64,4,4,32,1,.5603778155784024,10.82,5539.84,12.46,"GPU taking a nap","meh..."],[80,64,1,128,2,16,16,1,.5541629044998988,10.7,5478.4,15.19,"GPU taking a nap","meh..."],[80,32,1,128,2,4,32,1,.553644995243357,10.69,2736.64,13.83,"GPU taking a nap","meh..."],[3.57,8,8,32,1,2,32,0,.7505849759242873,264.1,16902.4,6.51,"GPU taking a nap","meh..."],[8.86,32,8,16,2,4,32,1,.7264461418277532,120.42,30827.52,9.35,"GPU taking a nap","meh..."],[1.34,64,2,8,16,1,32,1,.7032554700541153,720.7,368998.4,2.79,"GPU taking a nap","meh..."],[3.57,16,16,16,1,4,32,0,.7287580444017634,256.42,32821.76,10.96,"GPU taking a nap","meh..."],[3.57,64,1,16,16,1,32,1,.7257726140342488,279.21,142955.52,2.8,"GPU taking a nap","meh..."],[8.86,32,1,128,2,8,16,1,.7002835995644894,116.2,29747.2,5.08,"GPU taking a nap","meh..."],[8.86,64,16,8,2,8,32,1,.6984548604950779,115.78,59279.36,18.31,"GPU taking a nap","meh..."],[1.34,64,1,1,256,1,2,0,.6800510645068878,696.92,356823.04,18.51,"GPU taking a nap","meh..."],[8.86,64,8,8,4,4,32,1,.6933271473199111,114.93,58844.16,8.81,"GPU taking a nap","meh..."],[3.57,64,1,8,32,2,8,1,.7114499697960122,250.33,128168.96,4.03,"GPU taking a nap","meh..."],[1.34,32,16,4,4,2,32,1,.6754846302463741,592.74,151741.44,5.69,"GPU taking a nap","meh..."],[3.57,64,1,16,16,1,32,0,.7045096937133426,271.03,138767.36,4.15,"GPU taking a nap","meh..."],[1.34,64,16,2,8,2,32,1,.6686128613082467,586.71,300395.52,5.66,"GPU taking a nap","meh..."],[8.86,16,4,32,2,2,32,1,.673238576880728,111.6,14284.8,5.97,"GPU taking a nap","meh..."],[3.57,16,8,16,2,2,32,1,.6933745421289221,243.97,31228.16,5.58,"GPU taking a nap","meh..."],[1.34,16,32,8,1,4,32,0,.654538840348732,574.36,73518.08,12.32,"GPU taking a nap","meh..."],[1.34,64,2,8,16,1,32,0,.6502210107819616,666.35,341171.2,3.26,"GPU taking a nap","meh..."],[8.86,32,4,16,4,2,32,1,.6628624984556846,109.88,28129.28,5.5,"GPU taking a nap","meh..."],[3.57,32,8,8,4,2,32,1,.6820916100788675,240,61440,5.92,"GPU taking a nap","meh..."],[3.57,32,16,8,2,4,32,1,.6698708020649545,235.7,60339.2,10.31,"GPU taking a nap","meh..."],[8.86,64,4,8,8,2,32,1,.6467554464819252,107.21,54891.52,5.44,"GPU taking a nap","meh..."],[8.86,32,1,256,1,16,16,0,.6437546824912114,106.82,27345.92,5.42,"GPU taking a nap","meh..."],[1.34,64,2,4,32,2,8,1,.6276557509208008,550.77,281994.24,3.45,"GPU taking a nap","meh..."],[8.86,16,1,64,4,8,4,1,.6357996536493428,105.5,13504,13.43,"GPU taking a nap","meh..."],[1.34,8,2,64,2,2,16,1,.618538976210184,542.77,34737.28,2.05,"GPU taking a nap","meh..."],[8.86,8,1,128,2,8,4,0,.6306770971981396,104.65,6697.6,17.48,"GPU taking a nap","meh..."],[8.86,4,1,256,1,8,4,0,.6276638286974318,104.15,3332.8,15.73,"GPU taking a nap","meh..."],[8.86,8,1,128,2,16,2,0,.6206730457757896,102.99,6591.36,31.66,"GPU taking a nap","meh..."],[1.34,32,32,4,2,4,32,1,.6003965945360566,526.85,134873.6,11.95,"GPU taking a nap","meh..."],[3.57,8,1,64,4,4,4,0,.6280926909476239,221,14144,11.66,"GPU taking a nap","meh..."],[3.57,8,1,64,4,4,4,1,.6250801196697755,219.94,14076.16,8.34,"GPU taking a nap","meh..."],[3.57,64,8,4,8,2,32,1,.6241706641896704,219.62,112445.44,5.4,"GPU taking a nap","meh..."],[3.57,8,1,64,4,8,2,0,.6209307290417958,218.48,13982.72,17.35,"GPU taking a nap","meh..."],[3.57,4,1,128,2,8,2,0,.6178044758289343,217.38,6956.16,16.79,"GPU taking a nap","meh..."],[3.57,64,16,4,4,4,32,1,.6063794413601132,213.36,109240.32,9.55,"GPU taking a nap","meh..."],[3.57,16,1,32,8,4,4,0,.5985353878442062,210.6,26956.8,11.7,"GPU taking a nap","meh..."],[1.34,16,2,32,4,2,16,1,.5632913214638463,494.29,63269.12,1.89,"GPU taking a nap","meh..."],[3.57,16,1,32,8,8,2,0,.5925386657722629,208.49,26686.72,17.45,"GPU taking a nap","meh..."],[8.86,32,2,32,4,8,8,1,.5665547435030779,94.01,24066.56,11.03,"GPU taking a nap","meh..."],[8.86,8,1,128,2,4,8,0,.5658315590629079,93.89,6008.96,10.79,"GPU taking a nap","meh..."],[1.34,32,2,16,8,2,16,1,.5540833790061234,486.21,124469.76,1.86,"GPU taking a nap","meh..."],[8.86,16,1,64,4,4,8,0,.5649878438827098,93.75,12e3,10.74,"GPU taking a nap","meh..."],[1.34,1,1,128,1,8,1,1,.5526019031156482,484.91,3879.28,18.38,"GPU taking a nap","meh..."],[8.86,8,1,128,2,2,16,0,.563119617412271,93.44,5980.16,7.95,"GPU taking a nap","meh..."],[8.86,64,1,16,16,4,8,1,.5542606080201901,91.97,47088.64,6.76,"GPU taking a nap","meh..."],[3.57,8,1,64,4,16,1,0,.5709390981197654,200.89,12856.96,29.15,"GPU taking a nap","meh..."],[8.86,64,1,16,16,8,4,1,.5520307893296663,91.6,46899.2,11.87,"GPU taking a nap","meh..."],[8.86,32,2,32,4,16,4,1,.5464261099183498,90.67,23211.52,23.26,"GPU taking a nap","meh..."],[1.34,32,2,32,4,4,16,1,.531485173692182,466.38,119393.28,1.96,"GPU taking a nap","meh..."],[1.34,16,2,64,2,4,16,1,.5280777791440889,463.39,59313.92,2.94,"GPU taking a nap","meh..."],[8.86,64,2,16,8,8,8,1,.5360604662759151,88.95,45542.4,10.49,"GPU taking a nap","meh..."],[8.86,32,4,32,2,16,8,1,.5310584405647402,88.12,22558.72,22.55,"GPU taking a nap","meh..."],[3.57,32,1,16,16,8,2,1,.5481458701496299,192.87,49374.72,12.62,"GPU taking a nap","meh..."],[3.57,64,1,8,32,4,4,1,.5450480374205218,191.78,98191.36,6.97,"GPU taking a nap","meh..."],[8.86,16,1,64,4,2,16,0,.5275630491039192,87.54,11205.12,7.93,"GPU taking a nap","meh..."],[8.86,32,4,32,2,8,16,1,.5215365121025036,86.54,22154.24,10.3,"GPU taking a nap","meh..."],[1.34,32,1,8,32,4,2,1,.5055935335140304,443.66,113576.96,8.59,"GPU taking a nap","meh..."],[8.86,64,1,16,16,2,16,1,.5137020140006633,85.24,43642.88,4.59,"GPU taking a nap","meh..."],[80,64,1,128,2,8,32,1,.3822170313279676,7.38,3778.56,9.54,"GPU taking a nap","meh..."],[8.86,32,8,32,1,16,16,0,.5002628364875066,83.01,21250.56,22.53,"GPU taking a nap","meh..."],[1.34,64,4,4,16,2,16,1,.4879411784805986,428.17,219223.04,3.19,"GPU taking a nap","meh..."],[8.86,64,2,8,16,1,32,1,.4963626353561496,82.28,42127.36,4.01,"GPU taking a nap","meh..."],[8.86,32,2,32,4,4,16,1,.4900177235851001,81.31,20815.36,5.62,"GPU taking a nap","meh..."],[1.34,32,2,64,2,8,16,1,.4795423497784428,420.8,107724.8,2.93,"GPU taking a nap","meh..."],[3.57,64,2,8,16,2,16,1,.5038099154945035,177.27,90762.24,3.39,"GPU taking a nap","meh..."],[3.57,64,2,8,16,4,8,1,.5021899479205661,176.7,90470.4,6.37,"GPU taking a nap","meh..."],[8.86,64,1,16,16,16,2,1,.4837501251036279,80.27,41098.24,24.32,"GPU taking a nap","meh..."],[3.57,16,1,64,4,2,16,1,.4942606329533994,173.91,22260.48,2.59,"GPU taking a nap","meh..."],[8.86,64,4,16,4,8,16,1,.4765785460719434,79.08,40488.96,9.92,"GPU taking a nap","meh..."],[3.57,64,1,8,32,8,2,1,.4880081265276764,171.71,87915.52,11.96,"GPU taking a nap","meh..."],[3.57,64,4,4,16,1,32,0,.4879294344055329,187.71,96107.52,5.11,"GPU taking a nap","meh..."],[3.57,8,1,128,2,2,16,1,.4850523962173346,170.67,10922.88,2.9,"GPU taking a nap","meh..."],[8.86,64,2,16,8,16,4,1,.4690453748201739,77.83,39848.96,21.77,"GPU taking a nap","meh..."],[3.57,4,1,256,1,2,16,0,.4841997817047361,170.37,5451.84,3.67,"GPU taking a nap","meh..."],[3.57,16,1,256,1,8,16,0,.4770093993151547,167.84,21483.52,3.14,"GPU taking a nap","meh..."],[8.86,32,4,32,2,32,4,1,.4609095498682629,76.48,19578.88,53.95,"GPU taking a nap","meh..."],[8.86,32,2,32,4,32,2,1,.4601261000580788,76.35,19545.6,54.13,"GPU taking a nap","meh..."],[8.86,64,4,16,4,16,8,1,.4577757506275267,75.96,38891.52,21.62,"GPU taking a nap","meh..."],[1.34,64,2,4,32,4,4,1,.4454342163923478,390.87,200125.44,7.05,"GPU taking a nap","meh..."],[8.86,64,2,16,8,4,16,1,.4534969093565217,75.25,38528,5.45,"GPU taking a nap","meh..."],[3.57,32,8,16,2,8,16,1,.4657548877488534,163.88,41953.28,10.04,"GPU taking a nap","meh..."],[1.34,64,1,4,64,4,2,1,.4396906483246592,385.83,197544.96,8.19,"GPU taking a nap","meh..."],[3.57,32,2,16,8,16,2,1,.4618328609908999,162.5,41600,19.29,"GPU taking a nap","meh..."],[3.57,16,1,128,2,4,16,1,.4610086669620546,162.21,20762.88,3.12,"GPU taking a nap","meh..."],[3.57,64,2,8,16,8,4,1,.4574561064928938,160.96,82411.52,10.41,"GPU taking a nap","meh..."],[3.57,32,1,128,2,8,16,1,.4564898100452821,160.62,41118.72,3.05,"GPU taking a nap","meh..."],[3.57,64,4,8,8,8,8,1,.4547845810200849,160.02,81930.24,10.24,"GPU taking a nap","meh..."],[1.34,4,1,256,1,1,32,0,.4309013223549282,441.59,14130.88,2.03,"GPU taking a nap","meh..."],[1.34,8,1,128,2,1,32,1,.4294473877769059,440.1,28166.4,1.73,"GPU taking a nap","meh..."],[1.34,8,1,128,2,1,32,0,.4294278718765298,440.08,28165.12,2.1,"GPU taking a nap","meh..."],[3.57,32,1,64,4,4,16,1,.4510330771646511,158.7,40627.2,2.86,"GPU taking a nap","meh..."],[3.57,64,1,8,32,1,16,1,.4454035937637209,171.35,87731.2,2.89,"GPU taking a nap","meh..."],[8.86,64,8,16,2,16,16,1,.4312589878212983,71.56,36638.72,22.06,"GPU taking a nap","meh..."],[1.34,16,1,64,4,1,32,1,.4207530541593367,431.19,55192.32,1.64,"GPU taking a nap","meh..."],[3.57,8,1,256,1,4,16,0,.4369080967392679,153.73,9838.72,3.65,"GPU taking a nap","meh..."],[1.34,16,1,64,4,1,32,0,.4142542593340829,424.53,54339.84,2.1,"GPU taking a nap","meh..."],[1.34,64,4,4,16,4,8,1,.4134571290948594,362.81,185758.72,6.03,"GPU taking a nap","meh..."],[3.57,32,1,32,8,2,16,1,.4336113206238867,152.57,39057.92,2.31,"GPU taking a nap","meh..."],[3.57,64,4,8,8,4,16,1,.4274724961331769,150.41,77009.92,5.44,"GPU taking a nap","meh..."],[8.86,64,8,16,2,8,32,1,.4067181438467623,67.42,34519.04,9.64,"GPU taking a nap","meh..."],[3.57,64,1,8,32,1,16,0,.4174862631712356,160.61,82232.32,5.53,"GPU taking a nap","meh..."],[1.34,16,8,16,2,2,32,1,.3910640512119069,343.16,43924.48,3.29,"GPU taking a nap","meh..."],[1.34,32,4,8,8,8,4,1,.3900384140569626,342.26,87618.56,11.5,"GPU taking a nap","meh..."],[3.57,64,8,8,4,8,16,1,.4094254889498402,144.06,73758.72,10.09,"GPU taking a nap","meh..."],[1.34,64,1,16,16,1,32,1,.3822384247670293,391.72,200560.64,1.6,"GPU taking a nap","meh..."],[8.86,32,4,32,2,4,32,1,.3813812081577027,63.22,16184.32,5.19,"GPU taking a nap","meh..."],[1.34,32,8,8,4,2,32,1,.3736851994197937,327.91,83944.96,3.25,"GPU taking a nap","meh..."],[3.57,16,8,32,1,4,32,0,.3850407138895207,135.48,17341.44,5.93,"GPU taking a nap","meh..."],[1.34,64,8,4,8,4,16,1,.3624829624941234,318.08,162856.96,5.97,"GPU taking a nap","meh..."],[1.34,64,1,16,16,1,32,0,.3616393919200161,370.61,189752.32,2.13,"GPU taking a nap","meh..."],[1.34,16,16,16,1,4,32,0,.3581410985381921,314.27,40226.56,6.81,"GPU taking a nap","meh..."],[8.86,64,4,16,4,4,32,1,.3637056792127158,60.29,30868.48,5.04,"GPU taking a nap","meh..."],[8.86,32,2,32,4,2,32,1,.3615339418679393,59.93,15342.08,3.56,"GPU taking a nap","meh..."],[1.34,8,8,32,1,2,32,0,.3482835858823377,305.62,19559.68,3.83,"GPU taking a nap","meh..."],[1.34,64,1,8,32,2,8,1,.3440100977367361,301.87,154557.44,2.4,"GPU taking a nap","meh..."],[3.57,32,8,16,2,4,32,1,.357558106100093,125.81,32207.36,5.8,"GPU taking a nap","meh..."],[1.34,64,8,4,8,2,32,1,.3354745174139211,294.38,150722.56,3.22,"GPU taking a nap","meh..."],[3.57,16,4,32,2,2,32,1,.3515329635443963,123.69,15832.32,3.33,"GPU taking a nap","meh..."],[3.57,32,4,16,4,2,32,1,.3443710016385682,121.17,31019.52,3.14,"GPU taking a nap","meh..."],[3.57,64,8,8,4,4,32,1,.3433478642234499,120.81,61854.72,5.31,"GPU taking a nap","meh..."],[1.34,32,16,8,2,4,32,1,.3259360918729383,286.01,73218.56,6.63,"GPU taking a nap","meh..."],[3.57,64,16,8,2,8,32,1,.3413868508444732,120.12,61501.44,10.22,"GPU taking a nap","meh..."],[8.86,16,2,64,2,2,32,1,.3297421201819049,54.66,6996.48,3.96,"GPU taking a nap","meh..."],[8.86,64,2,16,8,2,32,1,.3267258183141597,54.16,27729.92,3.55,"GPU taking a nap","meh..."],[1.34,4,1,256,1,2,16,0,.3163520824584024,277.6,8883.2,1.71,"GPU taking a nap","meh..."],[1.34,16,1,128,2,4,16,1,.3126597887006026,274.36,35118.08,1.77,"GPU taking a nap","meh..."],[1.34,8,1,256,1,4,16,0,.3102096554971243,272.21,17421.44,1.98,"GPU taking a nap","meh..."],[8.86,16,1,128,2,16,4,0,.3159713349842184,52.43,6711.04,16.34,"GPU taking a nap","meh..."],[3.57,64,4,8,8,2,32,1,.3238229918849423,113.94,58337.28,3.19,"GPU taking a nap","meh..."],[1.34,32,1,32,8,2,16,1,.2967738087673529,260.42,66667.52,1.3,"GPU taking a nap","meh..."],[1.34,64,16,4,4,4,32,1,.2963521579369869,260.05,133145.6,6.21,"GPU taking a nap","meh..."],[1.34,16,1,64,4,2,16,1,.2959874869485622,259.73,33245.44,1.42,"GPU taking a nap","meh..."],[3.57,8,1,128,2,8,4,0,.3110906151634702,109.46,7005.44,9.07,"GPU taking a nap","meh..."],[3.57,16,1,64,4,8,4,1,.3104653645208979,109.24,13982.72,6.95,"GPU taking a nap","meh..."],[3.57,16,1,64,4,8,4,0,.3071117474380101,108.06,13831.68,9.22,"GPU taking a nap","meh..."],[3.57,8,1,128,2,16,2,0,.3049517906727603,107.3,6867.2,14.61,"GPU taking a nap","meh..."],[8.86,64,1,32,8,8,8,1,.2896353682880324,48.06,24606.72,6.34,"GPU taking a nap","meh..."],[8.86,32,2,64,2,16,8,1,.2860797114571972,47.47,12152.32,12.36,"GPU taking a nap","meh..."],[3.57,8,1,128,2,4,8,0,.2934699152364328,103.26,6608.64,5.81,"GPU taking a nap","meh..."],[1.34,8,1,128,2,2,16,1,.2774804342860101,243.49,15583.36,1.41,"GPU taking a nap","meh..."],[3.57,16,1,64,4,2,16,0,.2892352631571931,101.77,13026.56,3.93,"GPU taking a nap","meh..."],[3.57,16,2,64,2,16,4,1,.28713214735945,101.03,12931.84,10.83,"GPU taking a nap","meh..."],[3.57,32,1,32,8,8,4,1,.2871037268756967,101.02,25861.12,6.47,"GPU taking a nap","meh..."],[3.57,8,1,128,2,2,16,0,.2867626810706572,100.9,6457.6,3.94,"GPU taking a nap","meh..."],[3.57,16,1,64,4,16,2,0,.2861658509118382,100.69,12888.32,14.98,"GPU taking a nap","meh..."],[1.34,32,1,64,4,4,16,1,.2697539677187623,236.71,60597.76,1.28,"GPU taking a nap","meh..."],[8.86,32,2,64,2,32,4,1,.2746895565245217,45.58,11668.48,29.11,"GPU taking a nap","meh..."],[8.86,32,2,64,2,8,16,1,.2720378802438989,45.14,11555.84,5.85,"GPU taking a nap","meh..."],[8.86,32,4,64,1,16,16,0,.2719776148738847,45.13,11553.28,12.26,"GPU taking a nap","meh..."],[8.86,64,2,32,4,16,8,1,.2656497510223984,44.08,22568.96,11.69,"GPU taking a nap","meh..."],[3.57,64,1,16,16,8,4,1,.2719271885514419,95.68,48988.16,6.15,"GPU taking a nap","meh..."],[1.34,64,2,8,16,2,16,1,.2556001749805298,224.29,114836.48,1.95,"GPU taking a nap","meh..."],[3.57,16,1,64,4,4,8,0,.2676925364722022,94.19,12056.32,5.67,"GPU taking a nap","meh..."],[8.86,64,2,32,4,8,16,1,.2583576412506855,42.87,21949.44,5.41,"GPU taking a nap","meh..."],[1.34,32,1,128,2,8,16,1,.247155762404821,216.88,55521.28,1.82,"GPU taking a nap","meh..."],[3.57,64,1,16,16,4,8,1,.2602747902125945,91.58,46888.96,3.93,"GPU taking a nap","meh..."],[8.86,64,4,32,2,16,16,1,.2511257968489868,41.67,21335.04,11.6,"GPU taking a nap","meh..."],[3.57,64,2,16,8,8,8,1,.2575748442560323,90.63,46402.56,5.7,"GPU taking a nap","meh..."],[8.86,16,1,128,2,4,16,0,.2464853633578968,40.9,5235.2,5.38,"GPU taking a nap","meh..."],[1.34,16,4,32,2,8,8,1,.2365916997088938,207.61,26574.08,6.99,"GPU taking a nap","meh..."],[8.86,64,1,32,8,4,16,1,.2412422761666653,40.03,20495.36,3.53,"GPU taking a nap","meh..."],[1.34,16,2,32,4,8,4,1,.2355546665855611,206.7,26457.6,6.85,"GPU taking a nap","meh..."],[1.34,64,1,8,32,4,4,1,.2354748948068432,206.63,105794.56,4.53,"GPU taking a nap","meh..."],[8.86,64,1,16,16,1,32,1,.2396753464110333,39.73,20341.76,3.53,"GPU taking a nap","meh..."],[3.57,64,1,16,16,2,16,1,.2442740578594944,85.95,44006.4,2.28,"GPU taking a nap","meh..."],[8.86,64,1,32,8,32,2,1,.2363607811955187,39.22,20080.64,29.78,"GPU taking a nap","meh..."],[1.34,64,2,8,16,4,8,1,.2285347500583862,200.54,102676.48,3.47,"GPU taking a nap","meh..."],[1.34,32,2,16,8,4,8,1,.2263467241278382,198.62,50846.72,3.67,"GPU taking a nap","meh..."],[8.86,64,2,32,4,32,4,1,.2305753056741597,38.26,19589.12,27.69,"GPU taking a nap","meh..."],[3.57,64,1,16,16,16,2,1,.2309448509792032,81.26,41605.12,11.06,"GPU taking a nap","meh..."],[8.86,64,4,32,2,8,32,1,.2232666642504994,37.01,18949.12,5.23,"GPU taking a nap","meh..."],[3.57,64,4,16,4,8,16,1,.2288701556652133,80.53,41231.36,5.46,"GPU taking a nap","meh..."],[3.57,64,2,16,8,16,4,1,.2247776060047401,79.09,40494.08,9.78,"GPU taking a nap","meh..."],[3.57,64,2,16,8,4,16,1,.2223050239182042,78.22,40048.64,3,"GPU taking a nap","meh..."],[3.57,64,4,16,4,16,8,1,.2189798273190697,77.05,39449.6,9.63,"GPU taking a nap","meh..."],[1.34,64,1,8,32,8,2,1,.2015490969149605,176.86,90552.32,7.02,"GPU taking a nap","meh..."],[1.34,64,2,8,16,8,4,1,.2007057952542285,176.12,90173.44,6.21,"GPU taking a nap","meh..."],[1.34,64,4,8,8,4,16,1,.1970476894015935,172.91,88529.92,3.29,"GPU taking a nap","meh..."],[3.57,64,8,16,2,16,16,1,.207185326561456,72.9,37324.8,10.22,"GPU taking a nap","meh..."],[1.34,64,4,8,8,8,8,1,.1941189255258079,170.34,87214.08,6.13,"GPU taking a nap","meh..."],[1.34,16,4,32,2,2,32,1,.1899935849692537,166.72,21340.16,1.96,"GPU taking a nap","meh..."],[8.86,32,2,64,2,4,32,1,.193405275759822,32.06,8207.36,3.18,"GPU taking a nap","meh..."],[3.57,64,8,16,2,8,32,1,.198147612727911,69.72,35696.64,5.49,"GPU taking a nap","meh..."],[8.86,64,2,32,4,4,32,1,.1911128863403356,31.68,16220.16,2.95,"GPU taking a nap","meh..."],[8.86,8,1,256,1,2,32,0,.1887601708834944,31.29,2002.56,3.82,"GPU taking a nap","meh..."],[3.57,16,2,64,2,2,32,1,.1927761412985399,67.83,8682.24,2.19,"GPU taking a nap","meh..."],[1.34,32,4,16,4,2,32,1,.1826773732639838,160.3,41036.8,1.93,"GPU taking a nap","meh..."],[8.86,16,1,128,2,2,32,1,.1862868033519433,30.88,3952.64,2.93,"GPU taking a nap","meh..."],[8.86,32,1,64,4,2,32,1,.1840547399698119,30.51,7810.56,2.6,"GPU taking a nap","meh..."],[1.34,64,8,8,4,8,16,1,.1779936302564044,156.19,79969.28,6.42,"GPU taking a nap","meh..."],[3.57,32,2,32,4,2,32,1,.1856994408439717,65.34,16727.04,2,"GPU taking a nap","meh..."],[1.34,64,4,8,8,2,32,1,.1756574567368089,154.14,78919.68,1.89,"GPU taking a nap","meh..."],[3.57,32,4,32,2,4,32,1,.1845910419775935,64.95,16627.2,3.2,"GPU taking a nap","meh..."],[1.34,32,8,16,2,4,32,1,.1741987727831102,152.86,39132.16,3.76,"GPU taking a nap","meh..."],[1.34,16,8,32,1,4,32,0,.1739366655101799,152.63,19536.64,3.82,"GPU taking a nap","meh..."],[3.57,64,4,16,4,4,32,1,.1758375329815814,61.87,31677.44,2.94,"GPU taking a nap","meh..."],[8.86,64,1,32,8,2,32,1,.1685509483696016,27.94,14305.28,2.59,"GPU taking a nap","meh..."],[1.34,64,8,8,4,4,32,1,.1626774487425682,142.75,73088,3.35,"GPU taking a nap","meh..."],[1.34,64,16,8,2,8,32,1,.1587914235221678,139.34,71342.08,6.64,"GPU taking a nap","meh..."],[3.57,64,2,16,8,2,32,1,.159410493372182,56.09,28718.08,1.97,"GPU taking a nap","meh..."],[8.86,64,1,64,4,16,8,1,.1456613993242143,24.17,12375.04,6.73,"GPU taking a nap","meh..."],[3.57,16,1,128,2,16,4,1,.149378062607272,52.56,6727.68,6.56,"GPU taking a nap","meh..."],[8.86,32,2,128,1,16,16,0,.1444560919239312,23.97,6136.32,6.92,"GPU taking a nap","meh..."],[3.57,16,1,128,2,8,8,0,.1434666019865884,50.48,6461.44,4.69,"GPU taking a nap","meh..."],[8.86,64,2,64,2,16,16,1,.1378269012223741,22.87,11709.44,6.47,"GPU taking a nap","meh..."],[8.86,64,1,64,4,32,4,1,.1364407977120485,22.64,11591.68,15.58,"GPU taking a nap","meh..."],[8.86,64,1,64,4,8,16,1,.134693101981638,22.35,11443.2,3.28,"GPU taking a nap","meh..."],[3.57,64,1,32,8,8,8,1,.1384077558785035,48.7,24934.4,3.48,"GPU taking a nap","meh..."],[8.86,64,2,64,2,32,8,1,.1334877945813549,22.15,11340.8,14.91,"GPU taking a nap","meh..."],[1.34,8,1,128,2,8,4,1,.1299368315630657,114.02,7297.28,4.36,"GPU taking a nap","meh..."],[1.34,64,1,16,16,2,16,1,.1265522289517492,111.05,56857.6,1.53,"GPU taking a nap","meh..."],[3.57,64,1,32,8,16,4,1,.1303363384925703,45.86,23480.32,5.92,"GPU taking a nap","meh..."],[3.57,32,2,64,2,8,16,1,.1292279396261921,45.47,11640.32,3.11,"GPU taking a nap","meh..."],[3.57,64,2,32,4,16,8,1,.1270964033446956,44.72,22896.64,5.47,"GPU taking a nap","meh..."],[3.57,64,2,32,4,8,16,1,.1243964573881334,43.77,22410.24,3.07,"GPU taking a nap","meh..."],[1.34,32,1,32,8,8,4,1,.1180622325024873,103.6,26521.6,3.84,"GPU taking a nap","meh..."],[1.34,64,1,16,16,4,8,1,.1154525557415733,101.31,51870.72,2.11,"GPU taking a nap","meh..."],[8.86,64,2,64,2,8,32,1,.1165499041696744,19.32,9891.84,3.05,"GPU taking a nap","meh..."],[1.34,64,1,16,16,8,4,1,.1141192274401456,100.14,51271.68,3.74,"GPU taking a nap","meh..."],[3.57,64,4,32,2,16,16,1,.1199344414388675,42.2,21606.4,5.47,"GPU taking a nap","meh..."],[3.57,16,1,128,2,4,16,0,.1197354980525945,42.13,5392.64,2.86,"GPU taking a nap","meh..."],[3.57,64,1,32,8,4,16,1,.1151882206520687,40.53,20751.36,1.92,"GPU taking a nap","meh..."],[1.34,64,2,16,8,8,8,1,.1088314981079878,95.5,48896,3.25,"GPU taking a nap","meh..."],[1.34,64,2,16,8,4,16,1,.1041477551004085,91.39,46791.68,1.92,"GPU taking a nap","meh..."],[3.57,64,4,32,2,8,32,1,.1076567924574479,37.88,19394.56,3.06,"GPU taking a nap","meh..."],[1.34,64,4,16,4,8,16,1,.1003984815006673,88.1,45107.2,3.47,"GPU taking a nap","meh..."],[8.86,16,1,256,1,4,32,0,.1006238303079797,16.68,2135.04,2.67,"GPU taking a nap","meh..."],[8.86,32,1,128,2,4,32,1,.0979091586270091,16.23,4154.88,2.18,"GPU taking a nap","meh..."],[8.86,64,1,64,4,4,32,1,.0970042680666855,16.08,8232.96,1.91,"GPU taking a nap","meh..."],[3.57,8,1,256,1,2,32,0,.0964875423424064,33.95,2172.8,1.93,"GPU taking a nap","meh..."],[1.34,16,2,64,2,2,32,1,.090882847896461,79.75,10208,1.29,"GPU taking a nap","meh..."],[3.57,32,2,64,2,4,32,1,.0953223025085217,33.54,8586.24,1.88,"GPU taking a nap","meh..."],[3.57,16,1,128,2,2,32,1,.0950096771872355,33.43,4279.04,1.6,"GPU taking a nap","meh..."],[3.57,32,1,64,4,2,32,1,.0946117904146895,33.29,8522.24,1.53,"GPU taking a nap","meh..."],[1.34,64,8,16,2,8,32,1,.0898002308995753,78.8,40345.6,3.65,"GPU taking a nap","meh..."],[1.34,32,4,32,2,4,32,1,.0895153316898685,78.55,20108.8,2.01,"GPU taking a nap","meh..."],[1.34,64,2,16,8,2,32,1,.0885922582504186,77.74,39802.88,1.22,"GPU taking a nap","meh..."],[1.34,32,2,32,4,2,32,1,.0882047953252174,77.4,19814.4,1.25,"GPU taking a nap","meh..."],[3.57,64,2,32,4,4,32,1,.0921676288119069,32.43,16604.16,1.77,"GPU taking a nap","meh..."],[1.34,64,4,16,4,4,32,1,.0860167693946693,75.48,38645.76,1.92,"GPU taking a nap","meh..."],[3.57,16,1,128,2,2,32,0,.0894108418878382,31.46,4026.88,2.09,"GPU taking a nap","meh..."],[3.57,64,1,32,8,2,32,1,.0837835861046875,29.48,15093.76,1.51,"GPU taking a nap","meh..."],[8.86,64,1,128,2,32,8,1,.0731018938271708,12.13,6210.56,8.29,"GPU taking a nap","meh..."],[8.86,64,1,128,2,16,16,1,.0715952595768169,11.88,6082.56,3.75,"GPU taking a nap","meh..."],[3.57,64,1,64,4,16,8,1,.0690049345529787,24.28,12431.36,3.22,"GPU taking a nap","meh..."],[3.57,64,2,64,2,16,16,1,.0650829077950252,22.9,11724.8,3.13,"GPU taking a nap","meh..."],[3.57,64,1,64,4,8,16,1,.0644292366686997,22.67,11607.04,1.86,"GPU taking a nap","meh..."],[8.86,64,1,128,2,8,32,1,.0598434290560647,9.92,5079.04,1.9,"GPU taking a nap","meh..."],[1.34,64,2,32,4,8,16,1,.0533559239938847,46.82,23971.84,1.98,"GPU taking a nap","meh..."],[3.57,64,2,64,2,8,32,1,.0560736144452335,19.73,10101.76,1.71,"GPU taking a nap","meh..."],[1.34,64,1,32,8,4,16,1,.053116608657731,46.61,23864.32,1.3,"GPU taking a nap","meh..."],[1.34,8,1,256,1,2,32,0,.0501194689716157,43.98,2814.72,.99,"GPU taking a nap","meh..."],[1.34,32,1,64,4,2,32,1,.0493559390896016,43.31,11087.36,.9,"GPU taking a nap","meh..."],[1.34,16,1,128,2,2,32,1,.048615201144364,42.66,5460.48,.94,"GPU taking a nap","meh..."],[1.34,64,4,32,2,8,32,1,.0473274567164893,41.53,21263.36,1.87,"GPU taking a nap","meh..."],[3.57,32,1,128,2,4,32,1,.0487411296368857,17.15,4390.4,1.45,"GPU taking a nap","meh..."],[3.57,64,1,64,4,4,32,1,.0487127091531324,17.14,8775.68,1.35,"GPU taking a nap","meh..."],[3.57,16,1,256,1,4,32,0,.0482295609293265,16.97,2172.16,1.51,"GPU taking a nap","meh..."],[1.34,32,2,64,2,4,32,1,.0445924243033043,39.13,10017.28,1.23,"GPU taking a nap","meh..."],[1.34,64,1,32,8,2,32,1,.0443417129987623,38.91,19921.92,.9,"GPU taking a nap","meh..."],[1.34,64,2,32,4,4,32,1,.0439428541051729,38.56,19742.72,1.21,"GPU taking a nap","meh..."],[3.57,64,1,128,2,16,16,1,.0341045805039433,12,6144,1.84,"GPU taking a nap","meh..."],[1.34,64,1,64,4,8,16,1,.0271793846060262,23.85,12211.2,1.19,"GPU taking a nap","meh..."],[3.57,64,1,128,2,8,32,1,.0286194271395591,10.07,5155.84,1.14,"GPU taking a nap","meh..."],[1.34,64,2,64,2,8,32,1,.025025546580643,21.96,11243.52,1.18,"GPU taking a nap","meh..."],[1.34,64,1,64,4,4,32,1,.0230654400178604,20.24,10362.88,.83,"GPU taking a nap","meh..."]],hoverlabel:{namelength:0},hovertemplate:"<span style='font-family: monospace; background: rgba(255,255,255,0.95); padding: 8px; border-radius: 4px; box-shadow: 2px 2px 6px rgba(0,0,0,0.1);'><b style='font-size: 14px; color: #2a4b7c;'>Model: %{customdata[0]}B</b><br><b style='font-size: 14px; color: #2a4b7c;'>Utils: %{customdata[12]} (%{customdata[8]:.1f}% MFU)</b><br><b style='font-size: 14px; color: #2a4b7c;'>Speed: %{customdata[13]} (%{customdata[10]:,.0f} tok/s)</b><br><span style='font-size: 11px; color: #666; line-height: 1.4;'>Nodes: %{customdata[1]} | MBS: %{customdata[2]} | GradAcc: %{customdata[3]}<br>DP: %{customdata[4]} | PP: %{customdata[5]} | TP: %{customdata[6]} | ZeRO: %{customdata[7]}<br>Throughput: %{customdata[9]:,.0f} tok/s/GPU<br>GPU memory: %{customdata[11]:.1f} GB</span></span>",marker:{color:{dtype:"f8",bdata:"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"},colorscale:[[0,"rgb(78, 165, 183)"],[.5,"rgb(206, 192, 250)"],[1,"rgb(232, 137, 171)"]],opacity:.9,size:{dtype:"f8",bdata:"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"}},mode:"markers",showlegend:!1,x:{dtype:"f8",bdata:"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"},y:{dtype:"f8",bdata:"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"},type:"scatter"},{customdata:{dtype:"f8",bdata:"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",shape:"1578, 12"},hoverlabel:{namelength:0},hovertemplate:"<span style='font-family: monospace; background: rgba(255,255,255,0.95); padding: 8px; border-radius: 4px; box-shadow: 2px 2px 6px rgba(0,0,0,0.1);'><b style='font-size: 14px; color: #2a4b7c;'>Model:  %{customdata[0]}B params</b><br><b style='font-size: 14px; color: #2a4b7c;'>Status: Run failed</b><br><span style='font-size: 11px; color: #666; line-height: 1.4;'>Nodes: %{customdata[1]} | MBS: %{customdata[2]} | GradAcc: %{customdata[3]}<br>DP: %{customdata[4]} | PP: %{customdata[5]} | TP: %{customdata[6]} | ZeRO: %{customdata[7]}<br>GPU memory: %{customdata[11]:.1f} GB</span></span>",marker:{color:"rgb(180,180,180)",line:{width:.01},size:7,symbol:"x"},mode:"markers",showlegend:!1,x:{dtype:"f8",bdata:"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"},y:{dtype:"f8",bdata:"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"},type:"scatter"}],{template:{data:{histogram2dcontour:[{type:"histogram2dcontour",colorbar:{outlinewidth:0,ticks:""},colorscale:[[0,"#0d0887"],[.1111111111111111,"#46039f"],[.2222222222222222,"#7201a8"],[.3333333333333333,"#9c179e"],[.4444444444444444,"#bd3786"],[.5555555555555556,"#d8576b"],[.6666666666666666,"#ed7953"],[.7777777777777778,"#fb9f3a"],[.8888888888888888,"#fdca26"],[1,"#f0f921"]]}],choropleth:[{type:"choropleth",colorbar:{outlinewidth:0,ticks:""}}],histogram2d:[{type:"histogram2d",colorbar:{outlinewidth:0,ticks:""},colorscale:[[0,"#0d0887"],[.1111111111111111,"#46039f"],[.2222222222222222,"#7201a8"],[.3333333333333333,"#9c179e"],[.4444444444444444,"#bd3786"],[.5555555555555556,"#d8576b"],[.6666666666666666,"#ed7953"],[.7777777777777778,"#fb9f3a"],[.8888888888888888,"#fdca26"],[1,"#f0f921"]]}],heatmap:[{type:"heatmap",colorbar:{outlinewidth:0,ticks:""},colorscale:[[0,"#0d0887"],[.1111111111111111,"#46039f"],[.2222222222222222,"#7201a8"],[.3333333333333333,"#9c179e"],[.4444444444444444,"#bd3786"],[.5555555555555556,"#d8576b"],[.6666666666666666,"#ed7953"],[.7777777777777778,"#fb9f3a"],[.8888888888888888,"#fdca26"],[1,"#f0f921"]]}],contourcarpet:[{type:"contourcarpet",colorbar:{outlinewidth:0,ticks:""}}],contour:[{type:"contour",colorbar:{outlinewidth:0,ticks:""},colorscale:[[0,"#0d0887"],[.1111111111111111,"#46039f"],[.2222222222222222,"#7201a8"],[.3333333333333333,"#9c179e"],[.4444444444444444,"#bd3786"],[.5555555555555556,"#d8576b"],[.6666666666666666,"#ed7953"],[.7777777777777778,"#fb9f3a"],[.8888888888888888,"#fdca26"],[1,"#f0f921"]]}],surface:[{type:"surface",colorbar:{outlinewidth:0,ticks:""},colorscale:[[0,"#0d0887"],[.1111111111111111,"#46039f"],[.2222222222222222,"#7201a8"],[.3333333333333333,"#9c179e"],[.4444444444444444,"#bd3786"],[.5555555555555556,"#d8576b"],[.6666666666666666,"#ed7953"],[.7777777777777778,"#fb9f3a"],[.8888888888888888,"#fdca26"],[1,"#f0f921"]]}],mesh3d:[{type:"mesh3d",colorbar:{outlinewidth:0,ticks:""}}],scatter:[{fillpattern:{fillmode:"overlay",size:10,solidity:.2},type:"scatter"}],parcoords:[{type:"parcoords",line:{colorbar:{outlinewidth:0,ticks:""}}}],scatterpolargl:[{type:"scatterpolargl",marker:{colorbar:{outlinewidth:0,ticks:""}}}],bar:[{error_x:{color:"#2a3f5f"},error_y:{color:"#2a3f5f"},marker:{line:{color:"#E5ECF6",width:.5},pattern:{fillmode:"overlay",size:10,solidity:.2}},type:"bar"}],scattergeo:[{type:"scattergeo",marker:{colorbar:{outlinewidth:0,ticks:""}}}],scatterpolar:[{type:"scatterpolar",marker:{colorbar:{outlinewidth:0,ticks:""}}}],histogram:[{marker:{pattern:{fillmode:"overlay",size:10,solidity:.2}},type:"histogram"}],scattergl:[{type:"scattergl",marker:{colorbar:{outlinewidth:0,ticks:""}}}],scatter3d:[{type:"scatter3d",line:{colorbar:{outlinewidth:0,ticks:""}},marker:{colorbar:{outlinewidth:0,ticks:""}}}],scattermap:[{type:"scattermap",marker:{colorbar:{outlinewidth:0,ticks:""}}}],scattermapbox:[{type:"scattermapbox",marker:{colorbar:{outlinewidth:0,ticks:""}}}],scatterternary:[{type:"scatterternary",marker:{colorbar:{outlinewidth:0,ticks:""}}}],scattercarpet:[{type:"scattercarpet",marker:{colorbar:{outlinewidth:0,ticks:""}}}],carpet:[{aaxis:{endlinecolor:"#2a3f5f",gridcolor:"white",linecolor:"white",minorgridcolor:"white",startlinecolor:"#2a3f5f"},baxis:{endlinecolor:"#2a3f5f",gridcolor:"white",linecolor:"white",minorgridcolor:"white",startlinecolor:"#2a3f5f"},type:"carpet"}],table:[{cells:{fill:{color:"#EBF0F8"},line:{color:"white"}},header:{fill:{color:"#C8D4E3"},line:{color:"white"}},type:"table"}],barpolar:[{marker:{line:{color:"#E5ECF6",width:.5},pattern:{fillmode:"overlay",size:10,solidity:.2}},type:"barpolar"}],pie:[{automargin:!0,type:"pie"}]},layout:{autotypenumbers:"strict",colorway:["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],font:{color:"#2a3f5f"},hovermode:"closest",hoverlabel:{align:"left"},paper_bgcolor:"white",plot_bgcolor:"#E5ECF6",polar:{bgcolor:"#E5ECF6",angularaxis:{gridcolor:"white",linecolor:"white",ticks:""},radialaxis:{gridcolor:"white",linecolor:"white",ticks:""}},ternary:{bgcolor:"#E5ECF6",aaxis:{gridcolor:"white",linecolor:"white",ticks:""},baxis:{gridcolor:"white",linecolor:"white",ticks:""},caxis:{gridcolor:"white",linecolor:"white",ticks:""}},coloraxis:{colorbar:{outlinewidth:0,ticks:""}},colorscale:{sequential:[[0,"#0d0887"],[.1111111111111111,"#46039f"],[.2222222222222222,"#7201a8"],[.3333333333333333,"#9c179e"],[.4444444444444444,"#bd3786"],[.5555555555555556,"#d8576b"],[.6666666666666666,"#ed7953"],[.7777777777777778,"#fb9f3a"],[.8888888888888888,"#fdca26"],[1,"#f0f921"]],sequentialminus:[[0,"#0d0887"],[.1111111111111111,"#46039f"],[.2222222222222222,"#7201a8"],[.3333333333333333,"#9c179e"],[.4444444444444444,"#bd3786"],[.5555555555555556,"#d8576b"],[.6666666666666666,"#ed7953"],[.7777777777777778,"#fb9f3a"],[.8888888888888888,"#fdca26"],[1,"#f0f921"]],diverging:[[0,"#8e0152"],[.1,"#c51b7d"],[.2,"#de77ae"],[.3,"#f1b6da"],[.4,"#fde0ef"],[.5,"#f7f7f7"],[.6,"#e6f5d0"],[.7,"#b8e186"],[.8,"#7fbc41"],[.9,"#4d9221"],[1,"#276419"]]},xaxis:{gridcolor:"white",linecolor:"white",ticks:"",title:{standoff:15},zerolinecolor:"white",automargin:!0,zerolinewidth:2},yaxis:{gridcolor:"white",linecolor:"white",ticks:"",title:{standoff:15},zerolinecolor:"white",automargin:!0,zerolinewidth:2},scene:{xaxis:{backgroundcolor:"#E5ECF6",gridcolor:"white",linecolor:"white",showbackground:!0,ticks:"",zerolinecolor:"white",gridwidth:2},yaxis:{backgroundcolor:"#E5ECF6",gridcolor:"white",linecolor:"white",showbackground:!0,ticks:"",zerolinecolor:"white",gridwidth:2},zaxis:{backgroundcolor:"#E5ECF6",gridcolor:"white",linecolor:"white",showbackground:!0,ticks:"",zerolinecolor:"white",gridwidth:2}},shapedefaults:{line:{color:"#2a3f5f"}},annotationdefaults:{arrowcolor:"#2a3f5f",arrowhead:0,arrowwidth:1},geo:{bgcolor:"white",landcolor:"#E5ECF6",subunitcolor:"white",showland:!0,showlakes:!0,lakecolor:"white"},title:{x:.05},mapbox:{style:"light"}}},margin:{l:0,r:0,t:0,b:0},xaxis:{showgrid:!1,zeroline:!1,showticklabels:!1,range:[-1.2,1.2]},yaxis:{showgrid:!1,zeroline:!1,showticklabels:!1,scaleanchor:"x",scaleratio:1,range:[-.7,.7]},width:1200,height:675,paper_bgcolor:"white",plot_bgcolor:"white",showlegend:!1},{displayModeBar:!1,responsive:!0,scrollZoom:!1})</script>        </div>
            </div>
            <p style="text-align: cekter; font-style: italic; margin-top: 10px; max-width: 900px; margin-left: auto; margin-right: auto;">We ran over 4000 scaling experiments on up to 512 GPUs and measured throughput (size of markers) and GPU utilization (color of markers). Note that both are normalized per model size in this visualization.</p>
  
        </div>
    </d-title>
    <d-byline></d-byline>
      <d-article>
        <d-contents>
        </d-contents>
        
        <p>
          Thousands of GPUs humming in perfect harmony. That's what it takes to train today's most powerful AI models – a symphony of computing power that until recently was the exclusive domain of elite research labs. Open source has transformed this landscape, but not completely. Yes, you can download the latest <a href="https://huggingface.co/meta-llama">Llama</a> or <a href="https://huggingface.co/deepseek-ai">DeepSeek</a> models. Yes, you can read their <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">technical</a> and <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">experiment</a> reports. But the most challenging part – the training code, the knowledge and technics necessary to coordinate GPUs to train these massive systems – remains shrouded in complexity and spread around a series of disconnected papers and often private codebases.
          </p>
          <aside>Reading time: 2-4 days. For the best reading experience, we recommend not using a mobile phone.</aside>
          <p>
            This open-source book is here to changes that. Starting from the basics, we'll walk you through the knowledge necessary to scale the training of large language models from one GPU to tens, hundreds and even thousands of GPUs, illustrating theory with practical code examples and reproducible benchmarks.
        </p>

        <p>As the size of the clusters used to train these models grew, various techniques such as data parallelism, tensor parallelism, pipeline parallelism or context parallelism as well as ZeRO or kernel fusion have been invented to makes sure that GPUs are highly utilized at all times. This significantly reduces training time and makes the best use of this expensive hardware. Even more, as the challenge of scaling up AI training goes beyond just building the initial models and teams have found that fine-tuning large models on specialized data often produces the best results, generally involving the same distributed training techniques. In this book we'll progressively go over all of these techniques –from the simplest to the most raffined one– while keeping a single story-line to understand where each method comes from.</p>

        <p>We'll assumes you have some simple basic knowledge about current LLM architecture and are roughtly familiar with how deep learning model are trained, but you can be generally new to distributed training. If needed, the basics of model training can be found in great courses found at <a href="https://www.deeplearning.ai">DeepLearning.ai</a> or on the <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">PyTorch tutorial sections</a>. This book can be seen as the second part of a trilogy following our first blog on processing data for pre-training, the so-called “<a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb blog post</a>”. Having read both blog posts, you should have almost all the core knowledge needed to fully understand how how performing LLMs are being built nowadays, just missing some final spices regarding data mixing and architecture choices to complete the recipe (stay tuned for part three…).</p>

        <aside>We are extremely thankful to the whole <a href="https://distill.pub/">distill.pub</a> team for creating
            the template on which we based this blog post.</aside>
        
        <p>The book is built on the following <strong>three general foundations</strong>:</p>

        <p><strong>Quick intros on theory and concepts:</strong> before diving into code and experiments, we want to understand how each method works at a high level and what it’s advantages and limits are. You’ll learn about which parts of a language model eat away your memory and when during training it happens. You’ll learn how we can solve memory constraints by parallelizing the models and increase the throughput by scaling up GPUs. As a result you'll understand how the following widget to compute the memory breakdown of a transformer model works: </p>

        <div id="graph"></div>
        <div id="controls">
            <div class="cell column-1">
            <label for="a">Attention Heads (a):</label>
            <input type="range" id="a" name="a" min="1" max="128" value="8">
            <input type="number" id="a_input" value="8" min="1" max="128">
            </div>
            <div class="cell column-2">
            <label for="mixed">Mixed Precision:</label>
            <input type="checkbox" id="mixed" name="mixed" checked>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="b">Micro Batch Size (b):</label>
            <input type="range" id="b" name="b" min="1" max="53248" value="32">
            <input type="number" id="b_input" value="32" min="1" max="53248">
            </div>
            <div class="cell column-2">
            <label for="seq_parallel">Sequence Parallelism:</label>
            <input type="checkbox" id="seq_parallel" name="seq_parallel">
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="h">Hidden Dimension (h):</label>
            <input type="range" id="h" name="h" min="1" max="16384" value="512">
            <input type="number" id="h_input" value="512" min="128" max="16384">
            </div>
            <div class="cell column-2">
            <label for="recomputation">Recomputation:</label>
            <select id="recomputation" name="recomputation">
                <option value="none">None</option>
                <option value="selective">Selective</option>
                <option value="full">Full</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="h_ff">Feedforward Dimension (h_ff):</label>
            <input type="range" id="h_ff" name="h_ff" min="1" max="65536" value="2048">
            <input type="number" id="h_ff_input" value="2048" min="512" max="65536">
            </div>
            <div class="cell column-2">
            <label for="zero">Zero:</label>
            <select id="zero" name="zero">
                <option value="0">0</option>
                <option value="1">1</option>
                <option value="2">2</option>
                <option value="3">3</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="L">Number of Layers (L):</label>
            <input type="range" id="L" name="L" min="1" max="126" value="12">
            <input type="number" id="L_input" value="12" min="1" max="126">
            </div>
            <div class="cell column-2">
            <label for="ff_activation">FF Activation:</label>
            <select id="ff_activation" name="ff_activation">
                <option value="relu">ReLU</option>
                <option value="gelu">GELU</option>
                <option value="swiglu">SwiGLU</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="s">Sequence Length (s):</label>
            <input type="range" id="s" name="s" min="1" max="128000" value="128">
            <input type="number" id="s_input" value="128" min="64" max="128000">
            </div>
            <div class="cell column-2">
            <label for="presets">Presets:</label>
            <select id="presets" name="presets">
                <option value="Llama 3 Tiny">Llama 3 Tiny</option>
                <option value="Llama 3 8B">Llama 3 8B</option>
                <option value="Llama 3 70B">Llama 3 70B</option>
                <option value="Llama 3 405B">Llama 3 405B</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="v">Vocabulary Size (v):</label>
            <input type="range" id="v" name="v" min="1000" max="100000" value="30522">
            <input type="number" id="v_input" value="30522" min="1000" max="100000">
            </div>
            <div class="cell column-2">
            <label for="tp">Tensor Parallelism (t):</label>
            <input type="range" id="tp" name="tp" min="1" max="16" value="8">
            <input type="number" id="tp_input" value="8" min="1" max="16">
            </div>
            <div class="cell column-1">
            <label for="k">Optimizer Parameters (k):</label>
            <input type="range" id="k" name="k" min="1" max="16" value="8">
            <input type="number" id="k_input" value="8" min="1" max="16">
            </div>
            <div class="cell column-2">
            <label for="dp">Data Parallelism (d):</label>
            <input type="range" id="dp" name="dp" min="1" max="256" value="1">
            <input type="number" id="dp_input" value="1" min="1" max="256">
            </div>
        </div>

        <p>While this widget gives a theoretical breakdown the following tool can be used to predict the memory usage:</p>
        <ul>
          <li>
            <p>
              <a href="https://huggingface.co/spaces/nanotron/predict_memory">predict_memory</a>
            </p>
          </li>
          <li>
            <p>
              <a href="https://pytorch.org/docs/stable/torch_cuda_memory.html">torch_cuda_memory</a>
            </p>
          </li>
        </ul>

        <p><strong>Clear code implementations:</strong> theory is one thing, but we discover all kinds of edge cases and important details when we implement something. That’s why we link to implementation references where possible. Depending on the case, we’ll use two code references:</p>
        
        <ul>
          <li>
            <p>
              the <a href="https://github.com/huggingface/picotron">picotron</a> repository is built for education, thus it implements concepts usually in single, self-contained short files.
            </p>
          </li>
          <li>
            <p>
              On the other hand, to look at production ready code, we’ll refer to the <a href="https://github.com/huggingface/nanotron">nanotron</a> implementations which is a production training codebase used at Hugging Face.
            </p>
          </li>
        </ul>
        
        <aside>If you want to watch a video on distributed training rather than reading the blog or picotron code checkout <a href="https://www.youtube.com/watch?v=u2VSwDDpaBM&list=PL-_armZiJvAnhcRr6yTJ0__f3Oi-LLi9S">Ferdinand's YouTube channel</a>.</aside>
        
        <!-- <p><img alt="Picotron implements each key concept in a self-contained way, such that the method can be studied separately and in isolation." src="assets/images/placeholder.png" /></p> -->

        <p><strong>Real training efficiency benchmarks:</strong> Finally, how to <em>actually</em> scale your LLM training depends on your infrastructure, such as the kind of chips, interconnect etc., and we can’t give a single unified recipe. What we will give though is a way to benchmark several setups and it is what we have done on our cluster! We ran over 4100 distributed experiments (over 16k including test runs) with up to 512 GPUs to scan many possible distributed training layouts and model sizes. </p>
        
        <!-- <iframe id="plotFrame" src="assets/data/benchmarks/benchmarks_interactive.html" scrolling="no" frameborder="0" height="840" width="720"></iframe> -->
        <div id="fragment-benchmarks_interactive"></div>

        <p>As you can see, there’s a lot of ground to be covered. Before getting into the trenches of distributed training let’s take a quick high level look on the challenges we'll cover in the book.</p>


        <h2>High level overview</h2>

        <p> All the techniques we'll cover in this book tackle one or several of the following three key challenges, which we'll keep bumping into throughout the book:</p>
        <ol>
          <li><strong>Memory Usage</strong>: it's a hard limitation - if a training step doesn't fit in memory, training cannot proceed</li>
          <li><strong>Compute Efficiency</strong>: we want our hardware to spend most time computing, so we need to reduce time spent on data transfers or waiting for other GPUs to perform work.</li>
          <li><strong>Communication overhead</strong>: we want to minimize communication overhead as it keeps GPUs idle. To archieve this we will try to make best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible.</li>
          </ol>
        <p>In many places we'll see that we can trade one of these (computation, communication, memory) for another (e.g. recomputation or Tensor Parallelism). Finding the right balance is key to scaling training.</p>
        <p>
          As this book is very extensive, we've made a <a href="assets/images/ultra-cheatsheet.svg">cheatsheet</a> to help you navigate the book and get the general take-away. Keep it close to your heart as you navigate these stormy waters!
        </p>
        <div class="center"><a href="assets/images/ultra-cheatsheet.svg"><img src="assets/images/ultra-cheatsheet.svg" alt="Cheatsheet" /></a></div>
        

        <!-- <p>This book is very extensive so we decide to start with a very general overview of how you can think about distributed training. At a high level, the key challenge in scaling LLM training is to make a training step (forward/backward/optimizer step) with a large batch size the fastest possible.</p>
        <p>When scaling up models and input batches, we quickly end up in situations where either our target batch size won't fit in memory, or/and the model itself is too large to fit in a single GPU's memory.</p>
        <p>To solve this scaling issue we’ll need to carefully evaluate different parallelization strategies and find the optimal balance between three main factors:</p>
        <ol>
        <li><strong>Memory Usage</strong><ul>
        <li>Hard limitation - if a training step doesn't fit in memory, training cannot proceed</li>
        <li>Sometimes we can increase compute (e.g. recomputation) or increase communication (e.g. ZeRO) to reduce memory usage</li>
        </ul>
        </li>
        <li><strong>Compute Efficiency</strong><ul>
        <li>Memory transfer can also decrease compute efficiency.</li>
        <li>We want our hardware to spend most time computing, so we need to reduce time spent on data transfers or unoptimized kernels.</li>
        <li>GPUs need sufficient workload (large enough matrices/batch sizes) to maintain high utilization (compute-bound) otherwise they become memory-bound (limited by memory bandwidth).</li>
        </ul>
        </li>
        <li><strong>Communication overhead</strong><ul>
        <li>Two main types. For GPUs: intra-node (NVLink TODO: bandwidth) and inter-node (network TODO: bandwidth)</li>
        <li>Two main attributes: base latency and peak bandwidth. Base latency is a constant overhead that makes us want to do the least number of comms possible, and peak bandwidth controls the how fast we can move data between gpus</li>
        <li>We prioritize using the fastest communication channels (like NVLink) for operations that occur frequently and/or block computation (e.g. tensor parallelism)</li>
        <li>We want to minimize communication overhead as it keeps GPUs idle, so we try to overlap communication with compute as much as possible</li>
        </ul>
        </li>
        </ol>
        <p>But let’s not get too much ahead of our self and scale progressively. To guide you along the journey and as a practical reference we summarized the key concepts in a cheatsheet:</p>
        <p>[TODO: ADD CHEATSHEET]</p> -->
        <!-- <p>Time to get started by quickly revisiting the basic training steps of an LLM!</p> -->
                
        <h2>First Steps: Training on one GPU</h2>
        
        <p>Let’s start by quickly reviewing the very basics of model training before we start to scale to many GPUs. When a model is trained on a single GPU, the training typically consists of three steps: </p>

        <ol>
            <li>a forward pass which passes inputs through the model to yield its outputs,</li>
            <li>a backward pass to compute the gradients, and</li>
            <li>an optimization step using the gradients to update the parameters</li>
        </ol>

        <aside>As we’ll see later, these steps may be repeated or intertwined but for now we’ll start simple.</aside>

        <p>It looks generally like this: </p>

        <div class="svg-container" id="svg-first_steps_simple_training"> </div>
        <div class="info" id="svg-first_steps_simple_training-info">Hover over the network elements to see their details</div>
        <script src="../assets/images/first_steps_simple_training.js"></script>

        <p>In this figure, the boxes on the top line can be seen as successive layers inside a model (same for the last line). The red boxes are the associated gradients for each of these layers, computed during the backward pass.</p>

        <p>The <strong>batch size (<d-math>bs</d-math>)</strong> is one of the important hyper-parameters for model training and affects both model convergence and throughput.</p>

        <p>A small batch size can be useful early in training to quickly move along the training landscape reaching an optimal learning point. However, further along the model training, small batch sizes will keep gradients noisy and the model may not be able to converge to the most optimal final performances. At the other extreme, a large batch size while giving very accurate gradient estimations will tend to make less use of each training token rendering convergence slower and potentially wasting compute. You can find a nice early discussion of this topic in OpenAI’s paper on large batch training<d-cite bibtex-key="mccandlish2018largebatchtraining"></d-cite> or Section 4.2 of MiniMax-01 <a href="https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf">technical report</a>.
        </p>

        <aside>For instance, during DeepSeek-V3/R1 training “the batch size is gradually increased from 3072 input sequences to 15360 in the training of the first 469B tokens, and then keeps at 15360 input samples in the remaining training”.</aside>

        <p>Batch size also affects the time it takes to train on a given text dataset: a small batch size will require more optimizer steps to train on the same amount of samples. Optimizer steps are costly (in compute time) and the total time to train will thus increase compared to using a larger batch size. This being said, note that the batch size can often be adjusted quite largely around the optimal batch size without major impact to the performance of the model, i.e. the sensitivity of final model performances to the exact batch size value is usually rather low around the optimal batch size.</p>

        <p>In the LLM pretraining community, batch sizes are commonly reported in terms of tokens rather than in number of samples (<d-math>bst</d-math> = Batch Size Tokens), this makes training numbers generally independent of the exact input sequence length used during the training.</p>

        <p>In the simplest case, training on a single machine, the <d-math>bs</d-math> (in samples) and <d-math>bst</d-math> can be computed from the model input sequence length (seq) as follows :</p>

        <d-math block>
        bst=bs *seq
        </d-math>

        <p>From here onward we’ll show the formulas for the batch size in terms of samples but you can always get its token-unit counterpart by multiplying it with the sequence length.</p>

        <p>A sweet spot for recent LLM training is typically on the order of 4-60 million tokens per batch. The batch size as well as the training corpus have been steadily increasing over the years: Llama 1 was trained with a batch size of ~4M tokens for 1.4 trillions tokens while DeepSeek was trained with a batch size of ~60M tokens for 14 trillion tokens.</p>

        <p><strong>And our first challenge is already coming ahead when scaling the training of our model to these large batch sizes: out-of-memory issues. What should we do when our GPU doesn’t have enough memory to hold a full batch of our target batch size?</strong></p>

        <p>Let’s start by quickly understanding what led to our out-of-memory issue in the first place. This will help us gain some useful intuitions on the memory requirements for training a model.</p>

        <h3>Memory usage in Transformers</h3>

        <p>When training a neural network model, one store several items in memory:</p>

        <ul>
            <li>Model weights</li>
            <li>Model gradients</li>
            <li>Optimizer states</li>
            <li>Activations needed to compute the gradients</li>
        </ul>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
            <p>
                You would think for a model you could compute the memory requirements exactly but there are a few additional memory occupants that makes it hard to be exact:
        <ul>
            <li>CUDA Kernels typically require 1-2 GB of GPU memory, which you can quickly verify by running <code>import torch; torch.ones((1, 1)).to("cuda")</code> and then checking the GPU memory with <code>nvidia-smi</code>.</li>
            <li>Some rest memory usage from buffers, intermediate results and some memory that can’t be used due to fragmentation</li>
        </ul>
        We’ll neglect these last two contributors as they are typically small and constant factors.
            </p></div>
        </div>

        <p>These items are stored as tensors which come in different <em>shapes</em> and <em>precisions</em>. The <em>shapes</em> are determined by hyper-parameters such as batch size, sequence length, model hidden dimensions, attention heads, vocabulary size, and potential model sharding as we’ll see later. <em>Precision</em> refers to formats like FP32, BF16, or FP8, which respectively require 4, 2, or 1 byte to store each single value in the tensor.</p>

        <p>So how can I quickly determine memory usage from these variable? One simple way is to do this empirically and just measure it.</p>

        <h4>Profiling the memory usage</h4>

        <p>Using this snippet, we can understand how memory is allocated throughout training. We can see that memory utilization is not a static thing but varies a lot during training and during a training step:</p>

        <aside>Check out <a target="_self" href="#a1%3A_distributed_training_profiling" class="">A1: Distributed Training Profiling</a> for a walkthrough how to profile your model.</aside>


        <!-- <div class="svg-container l-body-outset" id="svg-first_steps_memory_profile"> </div>
        <div class="info" id="svg-first_steps_memory_profile-info">Hover over the elements to see their details</div>
        <script src="../assets/images/first_steps_memory_profile.js"></script>
 -->
        <!--  -->
        <div id="fragment-memory-profile"></div>

        <p>Clearly the first step looks very different from the subsequent ones, but let’s first have a look at the general anatomy of a step: first the activations increase quickly as we do the forward pass, then during the backward pass the gradients build up and as the backward pass propagates, the stored activations used to compute the gradients are progressively cleared. Finally, we perform the optimization step during which we need all the gradients and then update the optimizer states before we start the next forward pass. </p>

        <p>Why does the first step looks different: the activations increase quickly and then plateau for a while. In this first step the torch cache allocator does a lot of preparation preparing memory allocations to speed up the subsequent steps so that they don’t require searching for free memory blocks afterwards (see <a href="https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html">Zach’s blog</a>). After the first step we also see the optimizer states appearing which generally offset the memory usage for further training steps.</p>

        <aside>Ever noticed how sometimes the training succeeds in the first step but then OOMs during the following training steps? This can be explained by the build-up of the optimizer state after the first step.
        </aside>

        <p>Now that we’ve a first view of memory, let’s see how scaling up training is often a question of maximizing compute efficiency while keeping the memory requirements of these various items (activations, parameters, gradients, optimizer states) within the memory constraints of the GPUs.</p>

        <h4>Weights/grads/optimizer states memory</h4>

        <p>Let's start with the first 3 items in our list: the model’s weights, gradients and optimizer states. We can actually pretty easily estimate the memory needed for them.</p>

        <p>For a simple transformer LLM the number of parameters is given by the <a href="https://michaelwornow.net/2024/01/18/counting-params-in-transformer">following formula</a>:</p>

        <d-math block>
            N = h * v + L * (12 * h^2 + 13 * h) + 2*h
        </d-math>

        <aside>We excluded the positional embedding count as we're not using fixed positional embeddings.</aside>

        <p>In that equation, <d-math>h</d-math> is the hidden dimension, <d-math>v</d-math> the vocabulary size, and <d-math>L</d-math> the number of layers in the model. Note that looking at the equation we can see that the term that will dominate at large hidden dimensions is the <d-math>h^2</d-math> term since it’s the only one growing quadratically as we scale the parameters.</p>

        <p>Memory requirements for the parameters and gradients are simply the number of parameters multiplied by the number of bytes per parameter. In good old-fashioned full precision (FP32) training both parameters and gradients require 4 bytes while the optimizer, if we use Adam, requires the momentum and variance to be stored, which adds another two 4 bytes per parameter. In summary:</p>

        <d-math block>
            \begin{aligned}
            & m_{params} = 4 * N \\
            & m_{grad} = 4 * N \\
            & m_{opt} = (4+4) * N
            \end{aligned}
        </d-math>

        <p>Now let’s have look how things change if we use a lower precision. For stability reason (see <a target="_self" href="#mixed_precision_training">the mixed-precision training section below</a>) we often don't use full low precision training but a mix of higher and lower precision called "mixed precision"<d-cite bibtex-key="micikevicius2018mixedprecisiontraining"></d-cite>. The default nowadays for mixed precision training is to generally use BF16 for most of the computations –requiring 2 bytes per parameter and gradient– as well as an additional copy of the model weights and gradients in FP32, thus 12 bytes per parameter in total. In addition to the parameters and gradient, we need to store the optimizer states: for the Adam optimizer, this requires the momentum and the variance usually stored in FP32 for numerical stability, each using 4 bytes. </p>

        <aside>See some more details below when we cover the ZeRO methods.</aside>

        <p>Here’s the summary:</p>
        
        <d-math block>
            \begin{aligned}
                & m_{params} = 2 * N \\ 
                & m_{grad} = 2 * N \\ 
                & m_{params\_fp32} = 4 * N  \\ 
                & m_{opt} = (4+4) * N
            \end{aligned}
        </d-math>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>Some libraries store grads in fp32 which would require an additional <d-math>m_{params\_fp32} = 4 * N</d-math> memory. This is done for example in nanotron, because <code>bf16</code> is lossy for smaller values and we always prioritize stability. See  <a href="https://github.com/microsoft/DeepSpeed/issues/1773">this DeepSpeed issue</a> for more information.</p>
            </div>
        </div>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>The FP32 copy of parameters (<d-math>m_{params\_fp32}</d-math>) is sometimes called "master weights" in the literature and codebases.</p>
            </div>
        </div>

        <p>Interestingly, mixed precision itself doesn’t save overall memory as it just distributes the memory differently across the three components, and in fact adds another 4 bytes over full precision training if we accumulate gradients in FP32. It’s still advantageous as computing the forward/backward passes in half precision allows us to (1) use optimized lower precision operations on the GPU which are faster and (2) reduces the activation memory requirements during the forward pass which is a large part of the memory usage as we saw on the graph above and below.</p>

        <p>Let’s get a sense of how much general memory we need for a model (full and mixed precision giving the same overall value):</p>

        <table>
        <thead>
            <tr>
            <th><strong>Model parameters</strong></th>
            <th><strong>FP32 or BF16 w/o FP32 grad acc</strong></th>
            <th><strong>BF16 w/ FP32 grad acc</strong></th>
            </tr>
        </thead>
        <tbody>
            <tr>
            <td>1B</td>
            <td>16 GB</td>
            <td>20 GB</td>
            </tr>
            <tr>
            <td>7B</td>
            <td>112 GB</td>
            <td>140 GB</td>
            </tr>
            <tr>
            <td>70B</td>
            <td>1120 GB</td>
            <td>1400 GB</td>
            </tr>
            <tr>
            <td>405B</td>
            <td>6480 GB</td>
            <td>8100 GB</td>
            </tr>
        </tbody>
        </table>

        <aside><p>Using FP8 training instead of BF16 would further decrease the memory usage but it is less stable and a very active research topic (see <a href="https://x.com/xariusrke/status/1826669126955278401">this tweet</a>) and we’ll cover it in more detail later.
        </aside>

        <p>As we can see, as soon as we reach <strong>7B</strong> (!), weights and optimizer requirements already starts to add up significantly and exceed the size of a typical GPU memory, e.g. 80GB for a H100 GPU.</p>
        
        <p>But for now, let’s start with models which still fits in a single GPU, take a look at the last big contributor to our memory budget: the activation memory.</p>

        <h4>Activations memory</h4>
        
        <p>Activation memory is a bit more complex to compute than the weights, gradients and optimizer states, in part because it depends on the inputs of the model. If you’re unsure why we even need to store activations for the backward pass, <a href="https://www.determined.ai/blog/act-mem-2">this reference</a> is a good quick refresh. After a careful inspection of how backward pass is computed we can estimate the total memory required for the activations in mixed precision and we arrive at the following equation:</p>

        <d-math block>
            m_{act} =  L \cdot seq \cdot bs \cdot h \cdot (34 + \frac{5 \cdot n_{heads} \cdot seq}{h})</p>
        </d-math>

        <p>Here <d-math>L</d-math> is the number of layers, <d-math>seq</d-math> the sequence length, <d-math>bs</d-math> the batch size in samples, <d-math>h</d-math> the hidden dimension of the model and <d-math>n_{heads}</d-math> the number of heads.</p>

        <p>For the exact derivation of the numbers, you can follow this original NVIDIA paper on recomputation <d-cite bibtex-key="korthikanti2022recomputation"></d-cite>, it essentially requires you to do some accounting of all the sizes of intermediate activations between each operation in a transformer layer.</p>

        <p>An interesting observation here is how the memory is not static for a given model but it scales linearly with both the sequence length and batch size. This means the activation memory is the part which will blow up when we increase our batch size or train with longer sequences. We can use this equation to look at how memory usage changes for various sequence lengths for example for Llama models (<code>bs=1</code>):</p>

        <div class="l-body-outset" id="fragment-memusage_activations"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame3');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->

        <p>This graph tells a striking story: for short sequences (or similar for small batch-sizes), activations are almost negligible, but starting at around 2-4k tokens they come to take a significant amount of memory while parameter, gradient and optimizer states usage (that we’ll discuss later) stays roughly independent of the sequence length and batch size.</p>

        <p><strong>For large input tokens (a.k.a large batch-sizes/sequences), activations become by far the largest memory burden.</strong> </p>

        <p>Is there a way to tame this “activation explosion”? Good question, reader!</p>

        <p>It’s time to explain our first technique – called <strong><em>activation recomputation</em><em>–</em> </strong>which will help us cap activation memory footprint. An essential tool in today’s large model training toolbox.</p>

        <h3>Activation recomputation</h3>
        
        <p>The general idea behind <strong><em>activation recomputation</em></strong> – also called <em>gradient checkpointing</em> or <em>rematerialization</em> – is to discard some activations during the forward pass to save memory and spend some extra compute to recompute these on the fly during the backward pass. Without recomputation, we store every hidden state between two learnable operations (e.g. feed-forward, layernorm etc.), such that we can use them during the backward pass to compute gradients. When we use recomputation we typically will only store activations at a few key points along the model architecture, discard the rest of activations and recompute them on the fly during the backward pass from the nearest saved activations, basically performing again a sub-part of the forward pass to trade of memory for compute. It generally looks like this:</p>

        <div class="svg-container" id="svg-activation_recomputation"> </div>
        <div class="info" id="svg-activation_recomputation-info">Hover over the network elements to see their details</div>
        <script src="../assets/images/activation_recomputation.js"></script>
        <p>There are several strategies to select key activations to store:</p>

        <ul>
            <li><strong>Full</strong>: We checkpoint activations at the transition point between each layer of the Transformer model. This is usually called the <code>full</code> strategy since it requires a forward pass through each layer essentially adding a full forward pass during the backward pass. This strategy saves the most memory but is the most expensive one in terms of compute. It generally increases the compute cost and time by up to 30-40% which is very noticeable.</li>
            <li><strong>Selective</strong>: In general we can do better than full. The authors of the recomputation paper<d-cite bibtex-key="korthikanti2022recomputation"></d-cite> did a detailed analysis studying which activations grow the largest and have the cheapest recomputation cost in terms of FLOPs. Turns out that the attention computations fall in that category, and thus we can usually discard them and focus on checkpointing expensive the feedforward computations. For a GPT-3 (175B) model this means <strong>70% activation memory reduction at a 2.7% compute cost</strong>.</li>
        </ul>

        <aside>In recent models like DeepSeek V3, selective checkpointing is performed, storing even a smaller size of attention activation —using so-called “Multi-Head Latent Attention” (MLA)– to optimize activation memory usage.</aside>

        <p>Let’s see how drastically recomputation strategies can in practice reduce the memory footprint and how selective recomputation strikes a nice balance between memory saving and recomputation cost:</p>

        <div id="fragment-memory-recomputation"></div>

        <p>Another trend that's clearly visibile here is how the activations for long sequences play a bigger role for smaller models, so the effect of recomputation becomes even more noticeable.</p>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
              <p>
                When you’re measuring how efficient your training setup is at using your GPU/TPU/accelerator, you usually want to take recomputation into account to compute total FLOPS (Floating point operations per second) and compare it to theoretical maximum FLOPS of the GPU/TPU/accelerator. Taking recomputation into account when calculating FLOPS for a training step gives a value called “hardware FLOPS” which is the real number of operations performed on the accelerator. Dividing this number by the duration of the training step and the maximum accelerator FLOPS yields the <strong><em>Hardware FLOPS Utilization (HFU).</em></strong>
              </p><p>
                However, what really matters at the end of the day is the start-to-end time needed to train a model on a given dataset. So when comparing various GPU/TPU/accelerator together, if one of these accelerator provide for instance enough memory to skip recomputation and thus perform less operation per second (lower HFU) but for a faster training, it should be rewarded not punished. Thus, an alternative is to compute what is called <strong><em>Model FLOPS Utilization (MFU)</em></strong> which, in contrast to HFU, only takes into account the required operations for the forward+backward passes through the model, and do not include recomputation in the measured FLOPs. This value is thus more specific to the model than the training implementation.
              </p>
            </div>
        </div>


        <aside> </aside>
        
        <aside></aside>

        <p>Most training frameworks these days use FlashAttention (that we cover <a target="_self" href="#flash_attention_1-3">further below</a>) which integrate natively activation recomputation in its optimization strategy by recomputing attention scores and matrices in the backward pass instead of storing them. Thus most people using Flash Attention are already making use of selective recomputation.</p>

        <p><strong>As you’ve now understood, activation recomputation increases the number of FLOPs slightly due to recomputation, while it significantly reduces memory access overhead.</strong> </p>

        <p>This trade-off is particularly advantageous on hardware with small high-speed memory, like GPUs, as accessing memory is typically slower than performing computations. Despite the additional operations involves, the overall effect is thus often faster computation as well, in addition to the much lower memory footprint.</p>

        <p>Now that we’ve learned about recomputation, we can tame the activations memory usage as we saw in the above graphs!</p>

        <p>However, activations still bears a linear dependance on the batch size and all our profiles in the barplots above were using <code>bs=1</code> so as we move to larger batch sizes it might become an issue again. Do not despair as we have a second tool in our box - <strong><em>gradient accumulation</em></strong> to the rescue!</p>

        <h3>Gradient accumulation</h3>

        <p>Gradient accumulation is a very straightforward method to avoid memory explosion which consists in splitting our batch into micro-batches. We'll perform forward and backward passes successively on each micro-batch, compute the gradients, and, as the name suggests, sum the gradients of all micro-batch before we perform an optimizer step. In practice, the optimization step is conducted not on the sum but on the average of the gradients, so that the result is independent of the number of gradient accumulation steps.</p>

        <p>Let’s call the batch size for each forward pass the <code>micro batch size</code> (mbs). We’ll refer to the overall batch size between each optimizer step as the <code>global batch size</code> (gbs). If we do one optimizer step for each 8 forward/backward passes, the <code>global batch size</code> will be 8 times the <code>micro batch size</code>.</p>

        <p>What we now call <code>global batch size</code> thus corresponds to what we’ve called up to now just <code>batch size</code> for simplicity (we now make our terms more precise to avoid ambiguity).</p>

        <p>With gradient accumulation the global batch size can be simply computed as follows:</p>

        <d-math block>
            bs = gbs = mbs \times grad\_acc 
        </d-math>

        <p>Gradient accumulation allows us to effectively increase our batch size up to infinity (and beyond!) while the memory footprint stays constant. Gradient accumulation is also compatible with activation recomputation for further memory reduction.</p>

        <p><img alt="image.png" src="/assets/images/gradaccumulation_diag.png" /></p>

        <aside>Using gradient accumulation means we need to keep buffers where we accumulate gradients which persist throughout a training step. Whereas without gradient accumulation, in the backward gradients are computed while freeing the activations memory, which means a lower peak memory.</aside>

        <p>Gradient accumulation allows us to reduce memory of activations which grow linearly with batch size by computing only only partial, micro-batches.        </p>
        
        <p><strong>One drawback however, is that gradient accumulation requires multiple consecutive forward/backward passes per optimization step thereby increasing the compute overhead and slowing down training. No free lunch! </strong></p>

        <p>But if you’ve carefully followed, you probably noticed that the forward/backward passes for each micro-batch can actually be run in parallel. Forward/backward passes are independent from each other, with independent input samples being the only difference. Seems like it’s time to start extending our training to more than one GPU! </p>

        <p>Before that, let's quickly see how we can vizualise computation and communication with a short tour of one of the most usefull tool in the distributed training toolbox: the <strong>profiler</strong>. This tool will be extremely usefull to understand and validate how communications between GPUs and compute are happening and where bottlenecks are.</p>

        <h4>Profiling GPU compute and communication</h4>

        <p>PyTorch's <a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">profiler</a> allows us to trace and visualize exactly what's happening on both CPU and GPU during training. It's natively integrated in PyTorch. Let's see how to use it:</p>

        <d-code block language="python">
            with torch.profiler.profile(
                activities=[
                    torch.profiler.ProfilerActivity.CPU,
                    torch.profiler.ProfilerActivity.CUDA,
                ],
                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),
                on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/profile'),
                with_stack=True
            ) as prof:
                for step in range(steps):
                    train_step() 
                    prof.step()</d-code>

        <p>This generates a trace that we can visualize in TensorBoard or Chrome's trace viewer. The trace shows:</p>

        <ul>
            <li>CPU thread launching kernels asynchronously to GPU</li>
            <li>Multiple CUDA streams handling compute and communication in parallel</li>
            <li>Kernel execution times and memory allocation</li>
        </ul>

        <p><img alt="profile_trace_annotated.png" src="/assets/images/profile_trace_annotated.png" /></p>
        <div class="figure-legend"><p>Example trace showing CPU thread launching kernels asynchronously to GPU, with compute kernels and communication happening in parallel across different CUDA streams</p></div>

        <p>The trace helps identify bottlenecks like:</p>
        <ul>
            <li>Sequential compute and communication that could be overlapped</li>
            <li>Idle GPU time waiting for data transfers</li>
            <li>Memory movement between CPU and GPU</li>
            <li>Kernel launch overhead from CPU</li>
        </ul>

        <p>Understanding these patterns is crucial for optimizing distributed training performance. For example, the trace would clearly show if gradient synchronization is properly overlapped with backward computation as we'll discuss later.</p>

        <p>Now let’s get a larger workstation 🖥️  with a couple of GPUs and start investigating our first scaling technique called <em><strong>data parallelism</strong> which –as we'll see– is just a parallel version of gradient accumulation</em>.</p>

        <h2>Data Parallelism</h2>
        
        <p>The idea behind data parallelism (DP) is to replicate the model on several GPUs (we call the replica's “model instances”) and run forward and backward passes on different micro batches of data in parallel for each GPU, hence the name Data Parallelism. You've probably already seen Data Parallelism in simple training examples but as you'll soon see we'll dive quite deeper in this section so stay tuned even if you know the general approach.</p>

        <p><img alt="image.png" src="/assets/images/dp_diagram.png" /></p>

        <aside>If you are not familiar with distributed communications patterns like broadcast, gather or all-reduce we put together a small crash course in <a target="_self" href="#a0%3A_parallel_programming_crash_course" class="">A0: Parallel Programming Crash Course</a>.</aside>
  
        <p>Using a different micro batch for each GPU means we’ll have different gradients in each GPU, so to keep the model instances in sync across different GPUs, the gradients from the model instances will be averaged using an operation called “all-reduce”, which happens during the backward pass, before the optimizer step.</p>


        <p>This involves our first “distributed communication” primitive: <em><strong>all-reduce</em></strong> which handles the synchronization and communication between GPU instances and nodes.</p>

        <p><img alt="image.png" src="/assets/images/dp_overlap1.svg" /></p>

        <p>A naive DP implementation would just wait for the backward pass the finish so that we have all gradients, then it triggers an all-reduce over all DP ranks, to sync these gradients. But such an sequential steps of computation followed by communication is <strong>A BIG NO!</strong> Because we don’t want our GPUs to stay idle while communication is happening, like on the above graph.</p>

        <p>Instead we should try to overlap communication and computation whenever possible so that they happen at the same time as much as possible.</p>

        <p>Let’s see three optimizations that allow us to do much better than our naive first implementation! </p>

        <h4><strong>First optimization:</strong> Overlap gradient synchronization with backward pass</h4>

        <p>The main drawback of the naive DDP approach we’ve just described is that after the backward pass (<em>computation</em>), we have to wait for gradient synchronization (<em>communication</em>) before updating the parameters. Could we overlap this communication with our computation? The answer is yes!</p>

        <p>As shown in the figure above, the gradients (red boxes) for a layer can be gathered and summed even before the gradients from earlier layers (red boxes to the left) have been computed. For example, as soon as the backward pass of the last layer is complete (last box on the right), those gradients can already be gathered and summed while the backward computations continue for earlier layers, moving toward the left.</p>

        <p><img alt="image.png" src="/assets/images/dp_overlap2.svg"/></p>

        <p>This can be achieved in pytorch by attaching an <em>all-reduce hook function</em> to each parameter. An all-reduce operation is triggered as soon as the gradient for that parameter is ready, while gradients for other parameters are still being computed. This approach overlaps most of the all-reduce operations with gradient calculations, thereby improving efficiency. Here's a simple function to attach a hook:</p>

        <d-code block language="python">
            def register_backward_hook(self, hook):
                """
                Registers a backward hook for all parameters of the model that 
                require gradients.
                """
                for p in self.module.parameters():
                    if p.requires_grad is True:
                        p.register_post_accumulate_grad_hook(hook)</d-code>

        <p>Overlapping computation and communication reduces the time spent waiting for gradient synchronization across the entire model. Gradient synchronization can occur (at least partially) in parallel with backward pass, significantly speeding up data parallelism. Here's a full implementation of naive DP with synchronization overlap:</p>

        <details style="background: #f6f8fa; border: 1px solid #d0d7de; border-radius: 6px; margin: 1em 0;">
            <summary style="padding: 12px; cursor: pointer; user-select: none; background: #f3f4f6; border-bottom: 1px solid #d0d7de;">
                👉 Naive DP implementation with overlap in Picotron (Click to expand)
            </summary>
                <div class="code-embed-container" style="margin: 0; border-radius: 0; overflow-x: scroll; width: max-content; min-width: 100%; font-size: 8px;"></div>
                    <script 
                        src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fpicotron%2Fblob%2F0035cce0e04afd6192763b11efe50010d8ad0f71%2Fpicotron%2Fdata_parallel%2Fdata_parallel.py%23L10-L60&style=github&type=code&showBorder=off&showLineNumbers=on&showFileMeta=on&showCopy=on&showFullPath=on">
                    </script>
                </div>
        </details>

        <p>This is our first example of “<em>overlapping computation and communication</em>” which we will discuss several times in this blog post and is an essential technique to maximal scaling efficiency. But we can improve the efficiency even further!</p>


        <h4><strong>Second optimization:</strong> Bucketing gradients</h4>
        
        <p>GPU operations are usually more efficient when performed on large tensors rather than having many operations running on smaller tensors. This is also true for communication operations. Thus, we can advantageously group gradients into buckets and launch a single all-reduce for all the gradients within the same bucket instead of performing independent all-reduce for each gradient. It will generally look like the following:
        </p>
        <p><img alt="dp_overlap3.svg" src="/assets/images/dp_overlap3.svg" /></p>
        
        <p>Think of it like packing items into boxes before shipping. It's more efficient to send a few big boxes than many small ones. By performing a single all-reduce operation for each bucket, we can significantly reduce communication overhead and speed up the communication operation.</p>

        <p>Here's a code implementation with bucketing:</p>

        <details style="background: #f6f8fa; border: 1px solid #d0d7de; border-radius: 6px; margin: 1em 0;">
            <summary style="padding: 12px; cursor: pointer; user-select: none; background: #f3f4f6; border-bottom: 1px solid #d0d7de;">
                👉 Bucket DP implementation in Picotron (Click to expand)
            </summary>
                <div class="code-embed-container" style="margin: 0; border-radius: 0; overflow-x: scroll; width: max-content; min-width: 100%; font-size: 8px;"></div>
                    <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fpicotron%2Fblob%2F0035cce0e04afd6192763b11efe50010d8ad0f71%2Fpicotron%2Fdata_parallel%2Fdata_parallel.py%23L62-L171&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on">
                    </script>
                </div>
        </details>

        <h4><strong>Third optimization: </strong>Interplay with gradient accumulation</h4>

        <p>Finally, as we’ve seen before, gradient accumulation works by performing multiple forward and backward passes before updating the parameters with <code>optimizer.step()</code>. When combining gradient accumulation with data parallelism, we should be careful when we want to synchronize gradients.</p>

        <p>In a naive version, an all-reduce operation is automatically triggered after each backward pass during the accumulation, which is sub-optimal as a single reduce after the final step would have the same effect while reducing overhead.</p>

        <p>In PyTorch, this is typically solved by adding a <a href="https://github.com/pytorch/pytorch/blob/5ea67778619c31b13644914deef709199052ee55/torch/nn/parallel/distributed.py#L1408-L1435"><code>model.no_sync()</code></a> decorator, which disables gradient synchronization, on the backward passes which don’t need reduction.</p>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>When performing communication operations, tensors must be contiguous in memory to avoid redundant memory copies. To perform this optimally, we often pre-allocate continuous buffers of the size of activations or model parameters specifically for communication. While this speed up communication, it also contributes in part to the peak memory usage during training.</p>
                </div>
        </div>

        <p>Now let's have a look what that means for the global batch size.</p>

        <h3>Revisit global batch size</h3>
        <p>We can update our batch size equation with our newly added Data Parallelism and Gradient Accumulation parameters:</p>

        <d-math block>
            bs = gbs = mbs \times grad\_acc  \times dp
        </d-math>
        <p>Here  <d-math>grad\_acc</d-math> is the number of gradient accumulation steps and <d-math>dp</d-math> is the number of parallel instances used for data parallelism.</p>

        <p>Given a targeted global batch size, we can thus trade gradient accumulation steps for data-parallel processes to speed up training.</p>
        
        <p>In practice, people tend to maximize the number of data-parallel nodes (DP) over gradient accumulation as much as possible since it's inherently parallel, unlike the sequential nature of gradient accumulation. Gradient accumulation is then added on top of data parallelism to achieve the target global batch size when scaling data parallelism alone is not sufficient before you run out of GPUs.</p>

        <aside>A good resource for further reading on Data Parallelism is <a href="https://siboehm.com/articles/22/data-parallel-training">https://siboehm.com/articles/22/data-parallel-training</a>.
        </aside>

        <p>Being able to distribute the training over different samples gives us a first dimension of parallelization, thus making this 1D parallelism (we’ll progressively cover 4 more dimensions).</p>

        <h3>Our journey up to now</h3>
        <p>Let’s quickly summarize how to setup our first 1D parallel training with a draft recipe for an optimal data-parallel setup:</p>

        <ol>
            <li>We should first determine the best (global) batch size in tokens (<code>GBST</code>) either by consulting literature or running experiments measuring model convergence.</li>
            <li>We then select a sequence length for training, again by either consulting literature or running experiments. Generally, 2-8k tokens work reliably well for the evaluations we have today (we won’t dive in training recipes here but teams usually increase the sequence at the end of the training, adding some longer-context data samples in the mix to reach the longer context size of today).</li>
            <li>We now know the batch size (gbs). We can find the maximum local batch size (mbs) on a single GPU by increasing the local batch size until we run out of memory.</li>
            <li>Finally, we determine the number of available GPUs for our target DP. The ratio of GBS to DP gives us the remaining number of gradient accumulation steps needed for the desired GBS. </li>
        </ol>

        <aside>For instance DeepSeek and Llama models are trained with a 4k tokens sequence length during the main pretraining phase.<br><br>The reason 2-8k work well for pretraining is that documents that are longer are very rare on the web. See <a href="https://www.harmdevries.com/post/context-length/">Harm’s blogpost</a> for a detailed analysis.</aside>


        <p>If the gradient accumulation ratio is lower than one, i.e. we have too many GPUs a.k.a GPU-rich 🤑 (!), we can either choose to not use all our GPUs, explore a larger global batch size or test if a lower MBS will speed up training. In the latter case we’ll end up prioritizing throughput over individual GPU compute efficiency, using a smaller MBS than possible in order to speed up training.</p>

        <p>Time to take a concrete example: Let’s say we want to train a recent model with a GBS of 4M tokens and a sequence length of 4k. Our batch size will thus be 1024 samples (we pick the closest powers of two). Let's assume we observe that a single GPU can only fit MBS=2 in memory and we have 128 GPUs available for training. This means with 4 gradient accumulation steps we’ll achieve our goal of 1024 samples or 4M tokens per training step. Now what if we suddenly have 512 GPUs available? We can achieve the same GBS and thus identical training by keeping MBS=2 and setting gradient accumulation steps to 1 and achieve faster training!</p>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>Bear in mind that at the 512+ GPUs scale, depending on the network used, the communication operations will start to be bound by <em>ring latency</em>  (time required for a signal to propagate once around the ring) which means we can no longer fully overlap the DP communications. This will decrease our compute efficiency and hit our throughput. In this case we should start exploring other dimensions to parallelize on.</p>
                </div>
        </div>

        <p>While data parallelism nicely overlaps the all-reduce gradient synchronization with backward computation to save time, this benefit starts to break down at large scales. Why? Because as we add more and more GPUs (hundreds or thousands), the overhead of coordinating between them grows significantly and the network requirements are becoming too large for the benefits. As a result, our setup will become less and less efficient which each additional GPU we add to the system.</p>

        <p>Lets see this happening in practice with some benchmark:</p>

        <!-- <p><img alt="image.png" src="/assets/images/dp_scaling.svg"/></p> -->
        <div class="l-body-outset" id="fragment-dp_scaling"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame4');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->

        <p>We see that above some limit, our throughput starts to drop quite significantly while the memory usage per GPU stays constant and is not affected by adding more DP ranks.</p>

        <p><strong>Data parallelism was our first (simple) strategy to scale training across more GPUs. This technique works like gradient accumulation but parallelizes the forward and backward passes on micro batches, thus increasing throughput!</strong></p>
        
        <p>The keen reader has already probably noted however that this assumes that we can fit at least one input sample forward pass (mbs<em>=1)</em> into our GPU memory. This is not always the case! As we can see, larger models don’t fit into a single GPU, even with activation recomputation activated: </p>
        <aside>Tip: you can quickly eyeball the minimal memory required for your model’s parameters by multiplying by 2 e.g. 70B → 140GB (=133GiB)</aside>

        <!-- <iframe class="l-body-outset" id="plotFrame5" src="assets/data/benchmarks/dp_ourjourney_memoryusage.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-dp_ourjourney_memoryusage"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame5');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->
        <!-- <p><img alt="dp_ourjourney_memoryusage.svg" src="/assets/images/dp_ourjourney_memoryusage.svg" /></p> -->


        <p>We've also seen that Data Parallelism starts to have some limiting communication overhead above a certain level of scaling. Do we have other options for these larger models or large batch-size? We do have some solutions thankfully. They will involve either move some tensors to the CPU or split the weights/gradients/optimizer-states tensors across GPUs devices! Let's start diving in them.</p>

        <p>There are two main approaches to splitting: parallelism (tensor, context, or pipeline parallelism) and sharing (DeepSpeed Zero or PyTorch FSDP). Both approaches are somewhat orthogonal and can actually be combined!</p>
        
        <p>The sharing paradigm is closely related to DP so we’ll have a look at it first by investigating the ZeRO method!</p>


        <h3>ZeRO (<strong>Ze</strong>ro <strong>R</strong>edundancy <strong>O</strong>ptimizer)</h3>

        <p>In this section we will introduce DeepSpeed ZeRO (<strong>Ze</strong>ro <strong>R</strong>edundancy <strong>O</strong>ptimizer), a memory optimization technology designed to reduce memory redundancies in LLM training.</p>
        
        <p>While Data Parallelism is an efficient way to scale training, the naive replication of optimizer states, gradients, and parameters across each DP rank introduces a significant memory redundancy. ZeRO eliminates memory redundancy by partitioning the optimizer states, gradients, and parameters across the data parallel dimension, while still allowing computation with the full set of parameters. This sometimes requires more communications between DP ranks which may or may not be fully overlapped as we’ll see next!</p>

        <aside>We’ll focus on ZeRO-1 to ZeRO-3 in this blog as it should give a broad view on how it helps reduce memory while showing the tradeoffs to take into account. You can find more ZeRO flavors in the <a href="https://www.deepspeed.ai/tutorials/zero/">DeepSpeed docs</a>.</aside>

        <p>This approach is organized into three possible optimization stage of ZeRO:</p>
        
        <ul>
            <li>ZeRO-1: optimizer state partitioning</li>
            <li>ZeRO-2: optimizer state + gradient partitioning</li>
            <li>ZeRO-3 (also called FSDP for “Fully-Sharded Data Parallelism”): optimizer state + gradient + parameter partitioning</li>
        </ul>

        <aside>When we say partitioning, it means along the DP axis, as ZeRO is part of Data Parallelism. We’ll see later that we can partition along other axes.</aside>

        <p>You might be missing the activations among the things we can shard. Since each DP replica of the model receives a different micro-batch the activations on each DP rank also differ so they are not duplicated and thus can’t be sharded!</p>
        
        <p>Let’s have a closer look how much we can save with the partitioning of each ZeRO stage!</p>

        <h4>Memory usage revisited</h4>
        
        <p>You likely remember from <a target="_self" href="#memory_usage_in_transformers"> our previous section</a> the memory usage of optimizer states, gradients, and parameters during a standard training. Lets call our model's parameters count <d-math>\Psi</d-math> (previously N but here we use the original ZeRO paper notation). In mixed-precision training with the Adam optimizer, the memory usage for each item we need to store is:</p>

        <ul>
            <li>Model’s parameters (half precision i.e. bf16/fp16): <d-math>2\Psi</d-math></li>
            <li>Model’s gradients (half precision i.e. bf16/fp16): <d-math>2\Psi</d-math></li>
            <li>Model’s parameters in fp32 and optimizer states: <d-math>4\Psi + (4\Psi + 4\Psi)</d-math></li>
            <li>Model’s gradients in fp32: <d-math>4\Psi</d-math> (optional, only accounted if we want to accumulate grads in fp32)</li>
        </ul>

        <p>If we don’t accumulate gradients in fp32 this gives us a total memory consumption of  <d-math>2\Psi + 2\Psi + 12\Psi</d-math>, and if we accumulate it would be   <d-math>2\Psi + 6\Psi + 12\Psi</d-math>. Let’s focus for now on the case without fp32 gradient accumulation for simplicity but you can just add the additional bytes to the gradient term which are affected by ZeRO-2 and 3.</p>

        <p>The idea of ZeRO is to shard these objects across the DP ranks, each node only storing a slice of the items which are reconstructed when and if needed, thereby dividing memory usage by the data parallel degree <d-math>N_d</d-math>:</p>

        <p><img alt="zero_memory.svg" src="/assets/images/zero_memory.svg" /></p>
        <p>Here <d-math>\Psi</d-math> denotes number of parameters, <d-math>k</d-math> denotes the memory multiplier of optimizer states (<d-math>k=12</d-math> for Adam as we've just seen), and <d-math>N_d</d-math> denotes DP degree.</p>


        <p>Let’s explain this graph and it’s values by exploring how each ZeRO stage works. We’ll start with ZeRO-1.</p>

        <h4>ZeRO-1: Partitioning Optimizer States</h4>
        
        <p>In vanilla DP, all ranks gather the same gradients after the backward pass and simultaneously perform identical optimizer steps. This seems like a lot of duplicated work. Can we avoid it and reduce memory usage at the same time?</p>

        <p>In ZeRO-1, the optimizer states are partitioned into <d-math>N_d</d-math>  equal parts where  <d-math>N_d</d-math> is the DP degree. This means that each model replica distributed on each DP rank only keeps track of <d-math>\frac{1}{N_d}</d-math> of the optimizer states. During the optimization step only <d-math>\frac{1}{N_d}</d-math> of the float32 weights are updated.</p>

        <p>However during the forward pass, each replica@ need all the parameters, we thus need to add an additional <strong><em>all-gather</em></strong> (the second type of collective communication primitive we encounter!) after the optimizer step so that each model replica has the full set of updated weights.</p>

        <p>This explains the memory formula of <d-math>2\Psi + 2\Psi + \frac{k\Psi}{N_d}</d-math>  that we saw on the above graph! Here’s a summary of the sequence of operations for a single training step</p>

        <ul>
            <li>Forward pass with the same, full set of bf16 parameters on each replica, but different microbatches across replicas</li>
            <li>Backward pass with the same, full set of gradients on each replica, but different microbatches across replicas</li>
            <li>Perform an reduce-scatter on the gradients (we'll explain the reduce-scatter primitive in the graph below)</li>
            <li>Each replica perform an optimizer step on its local optimizer steps (only <d-math>\frac{1}{N_d}</d-math> optimizer states) to get updated <d-math>\frac{1}{N_d}</d-math> fp32 parameters which can then be converted to <d-math>\frac{1}{N_d}</d-math> of the full set of bf16 parameters.</li>
            <li>Perform an all-gather among the bf16 parameters to send missing slices back to each replica. This is a new operation in ZeRO, and not used in vanilla DP.</li>
        </ul>
        <aside>Note: reduce-scatter is 2 times faster than all reduce! <em>Yay, a third communication primitive!</em></aside>

        <p>You may be wondering what is this "reduce-scatter" operation and how this all look so lets try to make this more graphical with the figure below. We'll go over all the steps of a forward/backward pass cycle:</p>

        <p><img alt="dp_zero1.gif" src="/assets/images/dp_zero1.gif" /></p>

        <p>In terms of practical communications, compared to vanilla DP, Zero-1 change our "all-reduce" gradient communication to a "reduce-scatter" operation and adds an all-gather operation over all parameters after the optimizer step. Here is how it looks:</p>

        <p><img alt="dp_zero1_overlap.svg" src="/assets/images/dp_zero1_overlap.svg" /></p>

        <p>If you've been following along, you'll recall from vanilla DP that we can overlap the all-reduce gradient communication with the backward pass computation. In ZeRO-1, we can also investigate how to efficiently overlap the newly added all-gather of bf16 parameters. There are two main strategies for this:</p>

        <ul>
            <li>During optimizer step: We can initiate the all-gather immediately after the optimizer updates part of the parameters. This allows the communication to potentially overlap with other parameters update.</li>
            <li>During forward: We can overlap the all-gather of each layer’s parameters with the forward pass.</li>
        </ul>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>Unfortunately these techniques are not straightforward to implement and require sophisticated use of hooks/bucketing. In practice we can just use PyTorch native ZeRO-3/FSDP implementation and set the FSDPUnit to be the entire model, more details about this later.</p>
            </div>
        </div>

        <p>In ZeRO-1 the optimizer states have been partitioned, which means that each replica only updates <d-math>\frac{1}{N_d}</d-math> of the optimizer states. The keen reader must have noticed that there is no real need to have all gradients on all DP ranks in the first place as only a subset is needed for the optimization step. Meet ZeRO-2!</p>

        <h4>ZeRO-2: Adding <strong>Gradient Partitioning</strong></h4>
        
        <p>Since we only need, on each replica, to have the gradient shard corresponding to the optimizer state shard, it makes sense to shard gradient as well similarly to the optimizer states. During the backward pass, instead of performing an all-reduce over the gradients, we only perform a <strong><em>reduce-scatter</em></strong> operation! Where we only spread the <d-math>\frac{1}{N_d}</d-math> gradients needed in memory, thus saving more memory compared to ZeRO-1.</p>

        <aside>In case of FP32 gradient accumulation, we only need to keep <d-math>\frac{1}{N_d}</d-math> fp32_grads where we accumulate the bf16 grads coming from the reduce-scatter. And in the optimizer step we use the <d-math>\frac{1}{N_d}</d-math> fp32_grads.</aside>

        <p><img alt="dp_zero2.gif" src="/assets/images/dp_zero2.gif" /></p>

        <p>It’s easy to see now that sharding the gradients leads to to <d-math>2\Psi + \frac{2\Psi+k\Psi}{N_d}</d-math> and as <d-math>N_d</d-math> is increased we can save up to 8x memory over the baseline. In terms of communication the same process applies as for ZeRO-1, with the only difference that we communicate and release on the fly. In total, ZeRO-2 is thus also equivalent to vanilla DP training w.r.t. communication.</p>

        <p>In terms of communication ZeRO-2 is similar to ZeRO-1, they both require a reduce-scatter for the gradients, and an all-gather over all parameters.</p>

        <p><img alt="dp_zero2_overlap.svg" src="/assets/images/dp_zero2_overlap.svg" /></p>

        <aside>Note: You might notice that there is no real overhead of using ZeRO-2 over ZeRO-1 and indeed ZeRO-2 is usually the best option.</aside>

        <p>Now that we’ve sharded gradients as well, are we done or can we keep getting away with this? Well, sort of. Here comes ZeRO-3!</p>

        <h4>ZeRO-3: Adding <strong>Parameter Partitioning</strong></h4>
        
        <p>For Stage 3 we extend the above approach of sharding optimizer states and gradients over DP replicas up to sharding the model’s parameters.</p>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>This stage is also called FSDP (Fully Shared Data Parallelism) in PyTorch native implementation. We’ll just refer to ZeRO-3 in this blogpost but you can think of FSDP wherever you see it.</p>
            </div>
        </div>

        <p>So how do we do a forward or backward pass in practice if all parts of the model are distributed? Quite simply we gather them on-demand when we need them. In the forward pass this looks as follows:</p>

        <p><img alt="dp_zero3_fwd.svg" src="/assets/images/dp_zero3_fwd.svg" /></p>

        <p>So as we perform the forward pass and sequentially go through the layers we retrieve the necessary parameters on demand and immediately flush them from memory when we don't need them anymore. The backward pass works the same way just inverted in flow and we produce the gradient shards: </p>

        <p><img alt="dp_zero3_bwd.svg" src="/assets/images/dp_zero3_bwd.svg" /></p>

        <p>The other issue is that we need to do these all-gathers continuously throughout the forward and backward step, which amounts to <d-math>2\cdot \text{num\_layers} -1</d-math> additional all-gathers in <strong>a training step</strong> compared to Zero-2, each comes with a small <strong>base latency</strong> overhead as we can see in the following figure:</p>

        <p><img alt="dp_zero3_overlap.svg" src="/assets/images/dp_zero3_overlap.svg" /></p>

        <p>During the forward pass we do all-gather operations for the parameters when we need them, so a <d-math>\Psi</d-math> communication tax. Since we discard the parameters immediately after we needed them in the forward pass we need one more all-gather during the backward pass as well incurring another <d-math>\Psi</d-math> in communication tax. Finally we need the same <strong><em>reduce-scatter</em></strong> as in ZeRO-2 for the gradients which costs also <d-math>\Psi</d-math> in communication and we arrive at a total communication cost of <d-math>3\Psi</d-math>, compared to <d-math>2\Psi</d-math> for Zero-2.</p>

        <p>This may sounds like a lot of communication overhead but it's actually pretty fine as we can overlap the communication of the parameters for the next layer with the forward pass of the current layer in what is called <strong>prefetching</strong>. With prefetching, we will "all-gather" weights for *Layer n+1* while we do the current forward for <em>Layer n</em> in the forward, and similarly, we will "all-gather" weights for <em>Layer n-1</em>  while doing the backward for <em>Layer n</em>. Of course this overlap only holds true as long as we don’t scale DP too much. (as a rule of thumb DP shouldn’t exceed 512)</p>

        <p>In terms of memory we can see that our equation now reached it’s final form of <d-math>\frac{2\Psi +2\Psi+k\Psi}{N_d}</d-math> which means we can drive memory usage down indefinitely if we can increase the DP rank, at least for the model related parameters. Notice how it doesn’t help with the intermediate activations, for that we can use activation checkpointing and gradient accumulation as we’ve seen in the previous chapters.</p>                  

        <p><strong>Let’s summarize our journey into DP and ZeRO so far: we have seen that we can increase throughput of training significantly with DP, simply scaling training by adding more model replicas. With ZeRO we can train even models that would ordinarily not fit into a single GPU by sharding the parameters, gradients and optimizers states across DP, while incurring a small communications cost.
        </strong></p>
        <aside>If you want to read more about FSDP1, FSDP2 and some of the implementation complexities around them, you should take some time to go over <a href="https://christianjmills.com/posts/mastering-llms-course-notes/conference-talk-012/">this nice blog</a>.</aside>

        <p>However, there is a limit here, DP only works if a layer of the model fits in a single GPU and ZeRO can only partition the parameters, gradients, and optimizer states, but not the activation memory! We recall from <a target="_self" href="#memory_usage_in_transformers">the activation memory discussion</a> that this part of the memory scales with sequence length and batch size. Naturally we could just limit those, but in practice we don’t want to be limited by hardware to train with only with a short sequence length. </p>        
        
        <!-- <iframe class="l-body-outset" id="plotFrame6" src="assets/data/benchmarks/zero3_memoryusage.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-zero3_memoryusage"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame6');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->
        <!-- <p><img alt="zero3_memoryusage.svg" src="/assets/images/zero3_memoryusage.svg" /></p> -->

        <p>To overcome this issues, it's time to explore a new, orthogonal axis of parallelism - Tensor Parallelism (TP). Unlike ZeRO3 which relies on heavy parameter communication, TP proposes to shard parameters, gradients, optimizer states AND activations across devices without requiring any communication of model parameters between GPUs.</p>
        
        <p>What? How is this even possible?! Let's explore this seemingly magical approach together! 🙂</p>
        
        <h2>Tensor Parallelism</h2>

        <p>So we have sharded the model’s parameters, gradients and optimizers states with ZeRO but we hit a limit once activation memory overtakes our memory budget. Welcome Tensor Parallelism (TP), a method which shards weights, gradients, and optimizers states as well as activations and without the need to gather them all prior to the computation. Seems like a dream! Let’s first have a look at how Tensor Parallel works with simple matrix multiplications.</p>

        <p>Tensor Parallelism leverages the mathematical properties of matrix multiplication <d-math>A \times B</d-math>. To understand how it works, let's examine two fundamental equations that make this parallelization possible:</p>

        <d-math block>
            \begin{aligned}
            &\text{1.} \quad A\cdot B = A \cdot \begin{bmatrix} B_1 & B_2 & \cdots \end{bmatrix} = \begin{bmatrix} AB_1 & AB_2 & \cdots \end{bmatrix} \\
            &\text{2.} \quad A\cdot B =\begin{bmatrix} A_1 & A_2 & \cdots \end{bmatrix} \begin{bmatrix} B_1 \\ B_2 \\ \vdots \end{bmatrix} = \sum_{i=1}^n A_i B_i
            \end{aligned}
        </d-math>

        <p>This means that we can compute matrix product by either 1) multiplying each column of <d-math>B</d-math> individually or 2) multiplying each row individually and combining the results. In a neural network, the matrix multiplication is more often represented in the following format: <d-math>X \times W</d-math>, where:</p>

        <ul>
            <li>X represents the input or activation values</li>
            <li>W represents the weight of the <code>nn.Linear</code></li>
        </ul>

        <p>In practice a small example of the operation looks like this:</p>

        <p><img class="l-body" width="500px" alt="TP diagram" src="/assets/images/tp_diagram.svg" /></p>

        <p>Let’s see how we can parallelise this operation! In tensor parallelism, tensors will be split into N shards along a particular dimension and distributed across N GPUs. Matrices can be split either on the column part or row part leading to row and column parallelism. One thing we’ll see in the following is that choosing row or column sharding will require different communications primitives.</p>

        <p>Our first option is to use column-wise sharding (also called <strong><em>column-linear</em></strong>): We'll copy the complete input matrices to each worker, requiring an operation called <strong><em>broadcast</em></strong>, and split the weight matrix into columns. The inputs are then multiplied with the partial weight matrices, and the results are finally combined using an <strong><em>all-gather</em></strong> operation.</p>

        <p><img alt="image.png" src="/assets/images/tp_diagram2.png" /></p>

        <p>Here's the code implementation of column wise tensor parallelism:</p>

        <details style="background: #f6f8fa; border: 1px solid #d0d7de; border-radius: 6px; margin: 1em 0;">
            <summary style="padding: 12px; cursor: pointer; user-select: none; background: #f3f4f6; border-bottom: 1px solid #d0d7de;">
                👉 Column parallel TP implementation in Picotron (Click to expand)
            </summary>
                <div class="code-embed-container" style="margin: 0; border-radius: 0; overflow-x: scroll; width: max-content; min-width: 100%; font-size: 8px;"></div>
                <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fpicotron%2Fblob%2F1004ae37b87887cde597c9060fb067faa060bafe%2Fpicotron%2Ftensor_parallel%2Ftensor_parallel.py%23L54-L123&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>
                </div>
        </details>

        <p>The second option is called row-wise sharding (also called <strong><em>row-linear</em></strong>): As the attentive reader might guess, row-linear means that we split the weight matrix into chunks of rows. However, this also requires us to split the inputs, which needs a <strong><em>scatter</em></strong> operation rather than a broadcast as used in column-linear sharding. The results on each worker are already in the right shape but need to be summed for the final result, thus requiring an all-reduce operation in this scenario.</p>

        <p>We see here our fourth distributed primitive: <strong><em>scatter</em></strong>!</p>

        <p><img alt="image.png" src="/assets/images/tp_diagram3.png" /></p>

        <p>Here's the implementation for row-wise tensor parallelism:</p>

        <details style="background: #f6f8fa; border: 1px solid #d0d7de; border-radius: 6px; margin: 1em 0;">
            <summary style="padding: 12px; cursor: pointer; user-select: none; background: #f3f4f6; border-bottom: 1px solid #d0d7de;">
                👉 Row parallel TP implementation in Picotron (Click to expand)
            </summary>
                <div class="code-embed-container" style="margin: 0; border-radius: 0; overflow-x: scroll; width: max-content; min-width: 100%; font-size: 8px;"></div>
                <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fpicotron%2Fblob%2F1004ae37b87887cde597c9060fb067faa060bafe%2Fpicotron%2Ftensor_parallel%2Ftensor_parallel.py%23L125-L189&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>
                </div>
        </details>

        <p>Now that we have the basic building blocks of TP, let's have a look at how we can effectively combine them inside a transformer layer!</p>

        <h3>Tensor Parallelism in a Transformer Block</h3>
        
        <p>To come up with a strategy to follow, let’s move from a toy example to a real model building block. A Transformer model is made of two main building blocks : Feedforward layers (MLP) and Multi-Head Attention (MHA). We can apply tensor parallelism to both.</p>

        <p>The Feedforward part can be parallelized by having a “Column linear” followed by a “Row Linear” which amounts to a broadcast to copy the input and an all-reduce in forward. Note that the broadcast isn’t needed in actual training where we can make sure inputs are already synced across TP ranks. This setup is more efficient than starting with "Row Linear" followed by "Column Linear" as we can skip the intermediate all-reduce between both splitted operations.</p>

        <p><img alt="image.png" src="/assets/images/tp_diagram4.png" /></p>

        <p>Now that we’ve found an efficient schema for the Feedforward part of the transformer, let’s take a look at the multi-head attention block (MHA).</p>

        <p>We can generally follow a similar approach where Q, K, and V matrices are split in a column-parallel fashion, and the output projection is split along the row dimension. With multi-head attention, the column-parallel approach has a very natural interpretation: each worker computes the attention for an individual or a subset of heads. The same approach works as well for <a href="https://arxiv.org/abs/1911.02150"><strong><em>multi-query</em></strong> (MQA)</a> or <a href="https://arxiv.org/abs/2305.13245"><strong><em>grouped query attention</em></strong> (GQA)</a> where key and values are shared between queries. </p>

        <p>It's worth noting however that the tensor parallelism degree should not exceed the number of Q/K/V heads because we need intact heads per TP rank (otherwise we cannot compute the attentions independently on each GPU and we'll need additional communication operations). In case we’re using GQA, the TP degree should actually be smaller than the number of K/V heads. For instance, LLaMA-3 8B has 8 Key/Value heads, so the tensor parallelism degree should advantageously not exceed 8. If we use TP=16 for this model, we will need to duplicate the K/V heads on each GPU and make sure they stay in sync.</p>

        <p><img alt="image.png" src="/assets/images/tp_full_diagram.png" /></p>
        
        <p>Finally note that Tensor Parallelsim is still not a silver bullet for training. We’ve added several distributed communication primitive directly in the computation path of our model which are therefore hard to fully hide/overlap with computation (like we did in ZeRO), our final performances will be the results of a tradeoff between the computation and memory gains and the added communication overhead. Let's illustrate this:</p>

        <p><img alt="Forward pass in Tensor Parallelism" src="/assets/images/tp_overlap.svg" /></p>

        <aside>It's possible to partially hide this communication by performing block matrix multiplication coupled with async communication/computation.</aside>

        <p>Looking at the timeline of operations in tensor-parallel MLP (same applies for Attention), we can better understand the tradeoffs involved. In the forward of each decoder layer, we hit a synchronization point with the AllReduce operation that cannot be overlapped with computation. This <em>exposed communication</em> overhead is necessary to combine partial results across tensor-parallel ranks before the final LayerNorm can be applied. </p>

        <aside>For example, Megatron-LM/Nanotron implement a partial overlapping of all-gather with FC1 computation where a portion of the matrix multiplication result will start to be sent to the other GPU while the other part is still being computed.</aside>

        <p>Tensor parallelism does help reduce activation memory for the matrix multiplications since the intermediate activations are sharded across GPUs. However, we still need to gather the full activations for operations like LayerNorm, which means we're not getting the full memory benefits we could. Additionally, TP introduces significant communication requirements that heavily depend on the network infrastructure. The inability to fully hide this particular AllReduce behind computation means it directly adds to the critical path of forward propagation.</p>
        
        <aside>This area of research is still an active area of research, with recent work like Domino <d-cite bibtex-key="wang2024domino"></d-cite> exploring novel techniques to maximize this overlap. </aside>

        <p> Let's take a better look at the trade-off as we scale the TP degree:</p>

        <!-- <iframe class="l-body-outset" id="plotFrame13" src="assets/data/benchmarks/tp_scaling.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-tp_scaling"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame13');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->
<!-- 
        <p><img alt="Impact of Tensor Parallelism on model performance and batch size capacity: while increasing TP leads to reduced per-GPU throughput (left), it enables processing of larger batch sizes (right), illustrating the trade-off between computational efficiency and memory availability in distributed training." src="/assets/images/tp_scaling.svg" /></p> -->

        <p>While increasing TP leads to reduced per-GPU throughput (left), it enables processing of larger batch sizes (right), illustrating the trade-off between computational efficiency and memory availability in distributed training.</p>

        <p>In practice and as we see above on the left plot, the communication overhead of tensor parallelism becomes particularly noticeable as we scale beyond 8 GPUs. While tensor parallelism within a single node can leverage fast NVLink interconnects, going across nodes requires slower network connections. We observe significant drops when moving from TP=8 to TP=16, and an even steeper decline from TP=16 to TP=32. At higher degrees of parallelism, the communication overhead becomes so high that it quickly dominates the computation time.</p>

        <p>This being said, tensor parallelism provides important benefits for memory usage by distributing model parameters, gradients, optimizer states and activations (to some extent) across GPUs. Let's examine this effect on a 70B parameter model:</p>

        <!-- <iframe class="l-body-outset" id="plotFrame7" src="assets/data/benchmarks/tp_memoryusage.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-tp_memoryusage"></div>
<!--    <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame7');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            }); -->
        <!-- </script> -->
        <!-- <p><img alt="tp_memoryusage.svg" src="/assets/images/tp_memoryusage.svg" /></p> -->

        <p>Increasing tensor parallelism reduces the memory needed for model parameters, gradients and optimizer states on each GPU to the point where we can start fitting a large model on a single node of 8 GPUs. </p>
        
        <p>Is there a way to get even more benefits from this technique? We've seen that layer normalization and dropout still require gathering the full activations on each GPU, partially negating the memory savings. We can do better by finding ways to parallelize these remaining operations as well.</p>

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>One interesting note about layer normalization in tensor parallel training - since each TP rank sees the same activations after the all-gather, the layer norm weights don't actually need an all-reduce to sync their gradients after the backward pass. They naturally stay in sync across ranks. However, for dropout operations, we must make sure to sync the random seed across TP ranks to maintain deterministic behavior.</p>
                </div>
        </div>

        <p>Let's explore next a small and natural extension to tensor parallelism, called <strong>Sequence Parallelism</strong> which does exactly that.</p>

        <h3>Sequence Parallelism</h3>
        
        <p><strong>Sequence parallelism (SP)</strong> involves splitting the activations and computations for the parts of the model not handled by tensor parallelism (TP) such as Dropout and LayerNorm, but along the input sequence dimension rather than across hidden dimension.</p>
        
        <div class="note-box">
          <p class="note-box-title">📝 Note</p>
          <div class="note-box-content">
              <p>The term Sequence Parallelism is a bit overloaded: the Sequence Parallelism in this section is tightly coupled to Tensor Parallelism and applies to dropout and layer norm operation. However, when we will move to longer sequences the attention computation will become a bottleneck, which calls for techniques such as Ring-Attention, which are sometimes also called <em>Sequence Parallelism</em> but we’ll refer to them as <em>Context Parallelism</em> to differentiate the two approaches. So each time you see sequence parallelism, remember that it is used together with tensor parallelism (in contrast to context parallelism, which can be used independently).</p>
              </div>
      </div>

      <p>This is needed because these operations require access to the full hidden dimension to compute correctly. For example, LayerNorm needs the full hidden dimension to compute mean and variance:</p>

      <d-math block>
            \text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
        </d-math>

        <p>where <d-math>\mu = \text{mean}(x)</d-math> and <d-math>\sigma^2 = \text{var}(x)</d-math> are computed across hidden dimension <d-math>h</d-math>.</p>

        <p>So even though these operations are computationally cheap, they still require significant activation memory since they need the complete hidden dimension. SP allows us to shard this <strong>memory</strong> burden across GPUs by splitting along the sequence dimension instead.</p>

        <p>In practice we’ll go from the left diagram to the right:</p>

        <p style="text-align: center"><img alt=" in forward: f = no-op ; f* = all-reduce ; g = all-gather ; g* = reduce-scatter
            in backward: f = all-reduce ; f* = no-op ; g = reduce-scatter ; g* = all-gather
           SP region needs full hidden_dim" src="/assets/images/tp_sp_diagram.png"  style="width: 500px" /></p>

        <p>The diagram shows how we transition between tensor-parallel and sequence-parallel regions using different collective operations (labeled "f" and "g"). The key challenge is managing these transitions efficiently while keeping memory usage low and maintaining correctness.</p>

        <p>In the forward pass:</p>
        <ul>
            <li>"f" is a no-op (no operation) because activations are already duplicated across ranks</li>
            <li>"f*" is an all-reduce to synchronize activations and ensure correctness</li>
        </ul>
        <p>In the backward pass:</p>
        <ul>
            <li>"f*" is a no-op because gradients are already duplicated across ranks</li>
            <li>"f" is an all-reduce to synchronize gradients</li>
        </ul>

        <p>These operations "f" and "f*" are called <strong>conjugate</strong> pairs because they complement each other - when one is a no-op in forward, the other is an all-reduce in backward, and vice versa.</p>
            
        <p>For sequence parallelism (SP), we use different operations labeled "g" and "g*". Specifically, we avoid using all-reduce in the SP region since that would require gathering the full activations and increase our peak memory usage, defeating the purpose of SP.</p>
        
        <p>So what is actually happening here? As a famous LLM would say, let’s take it step-by-step:</p>

        <div class="l-body" style="display: grid; grid-template-columns: 1fr 1fr; align-items: center;">
        <div>
          <p style="margin-bottom: 0;"><strong>Initial LayerNorm (SP Region)</strong></p>
        <ul style="margin-top: 0;">
            <li>Input tensors X1<em> and X2</em> (b,s/2,h) enter LayerNorm, already split across sequence dimension</li>
            <li>Each GPU computes LayerNorm independently on its sequence chunk and give Y1<em> and Y2</em></li>
        </ul>
        <p style="margin-bottom: 0;"><strong>First Transition (SP → TP)</strong></p>
        <ul style="margin-top: 0;">
            <li>"g" operation (all-gather) combines Y1<em> and Y2</em> back to full sequence length</li>
            <li> Restores Y (b,s,h) since column linear layer needs full hidden dimension h</li>
        </ul>
        <p style="margin-bottom: 0;"><strong>First Linear Layer (TP Region)</strong></p>
        <ul style="margin-top: 0;">
            <li>A1 is a column-linear layer, so it splits Y along the hidden dimension</li>
            <li>GeLU is applied independently on each GPU</li>
            <li>Z1* is (b,s,h/2)</li>
        </ul>
        <p style="margin-bottom: 0;"><strong>Second Linear Layer (TP Region)</strong></p>
        <ul style="margin-top: 0;">
            <li>B1 is a row-linear layer, so it restores the hidden dimension</li>
            <li>W1 is (b,s,h)</li>
        </ul>
        <p style="margin-bottom: 0;"><strong>Final Transition (TP → SP)</strong></p>
        <ul style="margin-top: 0;">
            <li>"g*" operation (reduce-scatter) which reduces for previous row-linear correctness while scattering along sequence dimension</li>
            <li>W1* is (b,s/2,h)</li>
        </ul>

      </div>
      <div>
        <img alt="image.png" src="/assets/images/tp_sp_diagram_zoomed.png" />
      </div>
    </div>


        <p>A key advantage of sequence parallelism is that it reduces the maximum activation size we need to store. In tensor parallelism alone, we had to store activations of shape (b,s,h) at various points. However, with sequence parallelism, the maximum activation size is reduced to <d-math>\frac{b \cdot s \cdot h}{tp}</d-math> since we always either split along the sequence or hidden dimensions.</p>

        <p>It’s a bit difficult to keep track of all the parts that are sharded differently in TP and TP/SP - believe us, we find it hard to map as well so we made this small table to summarize how the activations (aka <code>hidden_states</code> ) shape change across hidden dimension h and sequence dimension s during a forward pass:</p>

        <table>
            <thead>
              <tr>
                <th>Region</th>
                <th>TP only</th>
                <th>TP with SP</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Enter TP (Column Linear)</td>
                <td>h: sharded (weight_out is sharded)<br>s: full</td>
                <td>h: sharded (weight_out is sharded)<br>s: <strong>all-gather</strong> to full</td>
              </tr>
              <tr>
                <td>TP Region</td>
                <td>h: sharded<br>s: full</td>
                <td>h: sharded<br>s: full</td>
              </tr>
              <tr>
                <td>Exit TP (Row Linear)</td>
                <td>h: full (weight_out is full + <strong>all-reduce</strong> for correctness)<br>s: full</td>
                <td>h: full (weight_out is full + <strong>reduce-scatter</strong> for correctness)<br>s: <strong>reduce-scatter</strong> to sharded</td>
              </tr>
              <tr>
                <td>SP Region</td>
                <td>h: full<br>s: full</td>
                <td>h: full<br>s: sharded</td>
              </tr>
            </tbody>
           </table>
        
        <p>And for the embedding layer:</p>
        
        <table>
            <thead>
              <tr>
                <th>Region</th>
                <th>Vanilla TP</th>
                <th>TP with SP</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Embedding Layer (Row Linear sharded on vocab)</td>
                <td>h: full (weight_out is full + <strong>all-reduce</strong> for correctness)<br>s: full</td>
                <td>h: full (weight_out is full + <strong>reduce-scatter</strong> for correctness)<br>s: <strong>reduce-scatter</strong> to sharded</td>
              </tr>
            </tbody>
           </table>

        <p>By using sequence parallelism, we can achieve even greater activation memory savings, allowing us to push our batch size and sequence length further than what would be possible with tensor parallelism alone. Let's see what that means for our previous 70B model example:</p>

        <!-- <iframe class="l-body-outset" id="plotFrame8" src="assets/data/benchmarks/tp_sp_memoryusage.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-tp_sp_memoryusage"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame8');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            }); -->
        <!-- </script> -->
        <!-- <p><img alt="tp_sp_memoryusage.svg" src="/assets/images/tp_sp_memoryusage.svg" /></p> -->

        <p>As we can see, we've again strongly reduced the maximum memory usage per GPU, allowing us to fit sequence lengths of 16k tokens with TP/SP=16, an improvement over the vanilla TP case! (TP=16 is still a bit large as we've seen in the previous section, but we'll see how we can improve this in the next section).</p>

        <p>One question you may be asking yourself is whether using TP+SP incurs more communication than vanilla TP? Well, yes and no. In the forward pass of a vanilla TP we had two all-reduce per transformer block, and in SP we have two all-gather and two reduce-scatter per transformer block. So SP does twice the number of communication operations as TP. But since an all-reduce operation can be broken down into to an all-gather + reduce-scatter (see the <a target="_self" href="#a_quick_focus_on_ring_allreduce" class="">A quick focus on Ring AllReduce</a> section in the appendix) they’re actually equivalent in terms of communication. Same reasoning for backward as we just use the conjugate of each operation (no-op ↔ allreduce and allgather ↔ reducescatter).</p>

        <p>If you’ve been paying close attention, you’ll notice that we’re talking about 4 comms ops in each layer (2 for Attention and 2 for MLP). This is how the MLP profiling looks like when using Tensor + Sequence Parallelism:</p>

        <p><img alt="tp_sp_overlap.svg" src="/assets/images/tp_sp_overlap.svg" /></p>

        <p>Just like vanilla TP, TP+SP can’t easily be overlapped with compute, which makes throughput heavily dependent on the communication bandwidth. Here again, like vanilla TO, TP+SP is usually done only within a node (keeping the TP degree under the number of GPU per nodes, e.g. TP≤8).</p>

        <p>We can benchmark how this communication overhead becomes increasingly problematic as we scale up tensor parallelism. Let’s measure the throughput and memory utilization as we scale TP with SP for a 3B model with 4096 seqlen:</p>

        <!-- <iframe class="l-body-outset" id="plotFrame2" src="assets/data/benchmarks/tp_sp_scaling.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-tp_sp_scaling"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame2');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            }); -->
        <!-- </script> -->

        <!-- <p><img alt="tp_sp_scaling.svg" src="/assets/images/tp_sp_scaling.svg" /></p> -->
        <p>Here again, there's a trade-off between computational efficiency (left) and memory capacity (right). While higher parallelism degrees enable processing of significantly larger batch sizes by reducing the activation memory, they also reduce per-GPU throughput, in particular above a threshold corresponding to the number of GPUs per node.</p>

        <p>Let’s summarize our observations:</p>

        <ul>
            <li>for both methods we notice the biggest performance drop when we move from TP=8 to TP=16, because that’s when we move from only communicating within a single node (NVLink), to communicating inter-nodes (EFA)</li>
            <li>the memory savings in activations when using TP with SP helps us fit far bigger batches than TP alone</li>
            <li>the memory savings in activations when using TP with SP helps us fit far bigger batches than TP alone</li>
        </ul>

        <p><strong>We have seen how TP helps us shard activations across several GPUs by splitting the attention and feedforward operations along the hidden dimension and how SP is a natural complement for the remaining operations by splitting along the sequence dimension.</strong></p>
        
        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>Since LayerNorms in the SP region operate on different portions of the sequence, their gradients will differ across TP ranks. To ensure the weights stay synchronized, we need to all-reduce their gradients during the backward pass, similar to how DP ensures weights stay in sync. This is however a small communication overhead since LayerNorm has relatively few parameters.</p>
                </div>
        </div>

        <p>However, there are two limits to TP and SP: 1) if we scale the sequence length the activation memory will still blow up in the TP region and 2) if the model is too big to fit with TP=8 then we will see a massive slow-down due to the inter-node connectivity.</p>
   
        <p>We can tackle problem 1) with Context parallelism and problem 2) with Pipeline parallelism. Let’s first have a look at Context parallelism!</p>

        <h2>Context Parallelism</h2>
        
        <p>With Tensor Parallelism and Sequence Parallelism, we can reduce the memory requirements per GPU significantly as both model weights and activations are distributed across GPUs. However, when training models on longer and longer sequences (e.g. when scaling to 128k or more tokens per sequence) we might still exceed the memory available on a single node as we still have to process a full sequence length when we're inside the TP region.</p>

        <p>Moreover, even if we use full recomputation of the activations (which comes at a heavy compute overhead of ~30%), we still need to hold in memory some activations at the layer boundaries which scale linearly with sequence length. Let's take a look and see how Context Parallelism can help us:</p>

        <!-- <iframe class="l-body-outset" id="plotFrame9" src="assets/data/benchmarks/cp_8Bmemoryusage.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-cp_8Bmemoryusage"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame9');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->

        <!-- <p><img alt="image.png" src="/assets/images/cp_memoryusage.svg" /></p> -->

        <p>The core idea of Context Parrallelism is to apply a similar idea to the Sequence Parallelism approach (aka to split along the sequence length) but to the modules where we already apply Tensor Parallelism. We will thus split these modules along two dimensions, thereby also reducing the effect of sequence length. You will find this approach quite intuitive after all we’ve already convered but... there is a trick to it so stay awake!</p>
                
        <p>For Context Parallelism; just like Sequence Parallelism, we’ll split the input along the sequence dimension but we now apply this splitting along the full model, instead of only the sequence parallel regions of the model as we’ve done previous with Tensor + Sequence Parallelism.</p>
        
        <!-- <p><img alt="cp_8Bmemoryusage.svg" src="/assets/images/cp_8Bmemoryusage.svg" /></p>
 -->
        <p>Splitting the sequence doesn't affect most modules like MLP and LayerNorm, where each token is processed independently. It also doesn’t require expensive communication like TP, as only the inputs are split and not the weight matrices. Just like data parallelism, after computing the gradients, an all-reduce operation is initiated to synchronize the gradients across the context parallelism group.</p>
        
        <p>There is one important exception though as we we need to pay particular attention to the <strong>Attention blocks</strong> (haha.. pun intended :D). In the attention module each token needs to access key/value pairs from <strong>all</strong> other sequence tokens or in the case of causal attention at least attends to each previous token.</p>
        
        <p>Because Context Parallelism splits the inputs along the sequence dimension across GPUs, the attention module will requires full communication between GPUs to exchange the necessary key/value data.</p>
        
        <p>That sounds very expensive if we do it naively. Is there a way to do this rather efficiently and fast! Thankfully there is: a core technique to handle this communication of key/value pairs efficiently is called <em>Ring Attention</em>.</p>
        
        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>Context Parallelism shares some conceptual similarities with Flash Attention (see later for more details) - both techniques rely on online softmax computation to reduce memory usage. While Flash Attention focuses on optimizing the attention computation itself on a single GPU, Context Parallelism achieves memory reduction by distributing the sequence across multiple GPUs.</p>
                </div>
        </div>

        <h3>Discovering Ring Attention</h3>
        
        <p>In this implementation of the attention mechanism, each GPU first initiates an asynchronous communication operation to send its key/value pairs to other GPUs. While waiting for the other GPUs data, it computes the attention score for the portion of the data it already has in memory. Ideally, a next key/value pair is received from another GPU before this computation finishes, allowing the GPU to start the next round of computation immediately after it finishes its first computation.</p>
        
        <p>Let's illustrate this. We'll suppose we have 4 GPUs and an input of 4 tokens. Initially, the input sequence is split evenly along the sequence dimension, so each GPU will have just one token along with its corresponding Q/K/V values. Leyt's say Q1, K1, and V1 represent the query, key, and value of the first token, which are located on the 1st GPU. The attention calculation will take 4 time steps to complete. At each time step, each GPU performs these three successive operations:</p>
        
        <ol>
            <li>Send “current keys and values” to the next machine except during the last time step in a non-blocking manner so we can starts the following step before this step is finished</li>
            <li>Locally compute the attention score on the “current keys and values” it already has, which typically involves performing  <d-math>Softmax(\frac{QK^T}{\sqrt{d}}) * V</d-math>d-math>.</li>
            <li>Wait to receive keys and values from the previous GPU and then circle back to step 1. where “current keys and values” are now the key/values just received from the previous GPU.</li>
        </ol>

        <p>We perform these 3 steps four times to complete the attention calculation.</p>

        <p>The whole process with 4 GPUs is shown in the following animation:</p>

        <p><img alt="ring-attention.gif" src="/assets/images/ring-attention.gif" /></p>

        <p>It's probably obvious to you on this animation why the authors chose to call this approach Ring Attention.</p>
        
        <p>There is one big problem though which is that a naive implementation of Ring Attention lead to some strong imbalance between GPU coming from the shape of the causal attention matrix. Let’s take a look at the SoftMax computation by considering the attention score matrix with the causal attention mask:</p>
        
        <p><img alt="cp_attnmask.svg" src="/assets/images/cp_attnmask.svg" /></p>

        <p>The SoftMax is computed row-wise, which means whenever a GPU has received all the tokens of a row it can be computed. We see that GPU1 can immediately compute it as it starts with tokens 1-4 and GPU1 actually doesn’t need to receive any information from any other GPUs. However, GPU2 will need to wait for the second round to also receive 1-4 and thus have all values for tokens 1-8. Also, GPU1 seems to perform much less work than all the other GPUs.</p>
        
        <p>Let’s see if we can balance our computations better:</p>
        
        <h3>Zig-Zag Ring Attention – A Balanced Compute Implementation</h3>

        <p>We need a better way to distribute the input sequences. This can be achieved by assigning the tokens not purely sequential to the GPUs but by mixing the ordering a bit such that we have a good mix of early and late tokens on each GPU. This approach is called Zig-Zag attention<d-cite bibtex-key="attention brandon2023fasterring"></d-cite> and in this new arrangement, the attention mask will show an even distribution of computation but if you count the number of colored squares, you’ll see that the computation is now balanced across all GPUs.</p>

        <p><img alt="cp_zigzagmask.svg" src="/assets/images/cp_zigzagmask.svg" /></p>

        <p>At the same time we’ll also see that in order to complete all rows, each GPU will need information from all the other GPUs.</p>
        
        <p>We have two general ways to overlap computation and communication, either by performing a general all-gather, regrouping all the KV on each GPUs at the same time (in a Zero-3 type of way) or we gather them one-by-one from each GPU to each GPU as needed:</p>
        
        <p><img alt="cp_overlap_allgather.svg" src="/assets/images/cp_overlap_allgather.svg" /></p>
        <p><img alt="cp_overlap_all2all.svg" src="/assets/images/cp_overlap_all2all.svg" /></p>

        <p>The key difference between these two implementations lies in their communication patterns and memory usage:</p>
        
        <p><strong>1. AllGather Implementation:</strong></p>

        <ul>
            <li>All GPUs simultaneously gather the complete key/value pairs from all other GPUs</li>
            <li>Requires more temporary memory as each GPU needs to store the full KV pairs at once</li>
            <li>Communication happens in one step but with larger memory overhead</li>
        </ul>
        
        <p><strong>2. All-to-All (Ring) Implementation:</strong></p>

        <ul>
            <li>GPUs exchange KV pairs in a ring-like pattern, one chunk at a time</li>
            <li>More memory efficient as each GPU only needs to store one additional chunk temporarily</li>
            <li>Communication is spread out and overlapped with computation, though with some additional base latency overhead from multiple communication steps</li>
        </ul>

        <p>The All-to-All approach generally offers better memory efficiency at the cost of slightly more complex communication patterns, while the AllGather approach is simpler but requires more temporary memory during the attention computation.</p>

        <p>We've now seen how we can split a model across one node with TP to tame large models and that we can use CP to tame the activation explosion with long sequences.</p>
        
        <p>However, we still know that TP doesn't scale well across nodes, so what can we do if the model weights don't easily fit on 1 node? Here come another degree of parallelism, our forth one, called <strong>Pipeline Parallelism</strong>, to the rescue!</p>

        <h2>Pipeline Parallelism</h2>
        
        <p>In the <a target="_self" href="#tensor-parallelism">Tensor Parallelism</a> section we saw that trying to scale Tensor parallelism past the number of GPUs per single node (typically 4 or 8) hit a lower bandwidth network called “inter-node connection” which can quite strongly impair our performances. We can see this clearly on e.g. the all-reduce operation when we benchmark it on our cluster across several nodes (each node has 8 GPUs):</p>

        <!-- <iframe class="l-body-outset" id="plotFrame11" src="assets/data/benchmarks/pp_comm_bandwidth.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body-outset" id="fragment-pp_comm_bandwidth"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame11');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->

        <!-- <p><img alt="pp_comm_bandwidth.svg" src="/assets/images/pp_comm_bandwidth.svg" /></p> -->
        <p>Inter-node communication bandwidth measurements across different node counts, showing median (lines) and 5th-95th percentile ranges (shaded areas) for AllReduce, AllGather and ReduceScatter operations.</p>

        <p>Sequence and context parallelism can help for long sequences but don’t help much if the sequence length is not the root cause of our memory issues but rather the size of the model itself. For large model (70B+), the size of the weights alone can already push past the limits of the 4-8 GPUs on a single node. We can solve this issue by summoning the fourth (and last) parallelism dimension: “pipeline parallelism”.</p>

        <p>Pipeline parallelism is a simple but powerful technique - we split our model's layers across multiple GPUs! For example, if we have 8 GPUs, we could put layers 1-4 on GPU 1, layers 5-8 on GPU 2, and so on. This way, each GPU only needs to store and process a portion of the model's layers, significantly reducing the memory requirements per GPU. Let's see the effect of Pipeline Parallelism in action on the memory usage for a 8B model:</p>

        <aside>This technique may remind you of our discussion on <a target="_self" href="#zero-redundancy-optimizer">ZeRO-3</a> where we split the model parameters across GPUs. We compare both techniques in details later in the <a target="_self" href="#5d_parallelism_in_a_nutshell">5D parallelism in a nutshell</a> section.</aside>

        <!-- <iframe class="l-body" id="plotFrame12" src="assets/data/benchmarks/pp_memoryusage.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body" id="fragment-pp_memoryusage"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame12');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->
        <!-- <p><img alt="pp_memoryusage.svg" src="/assets/images/pp_memoryusage.svg" /></p> -->

        <p>Looking at the figure above, we notice something interesting: while the model parameters are nicely split across GPUs, the activation memory remains the same on each GPU! This is because each GPU still needs to process the full batch of data, just with different layers. The activations from one GPU's layers will be sent to the next GPU to continue the forward pass.</p>

        <p>This introduces a new type of communication pattern: instead of communicating parameters like we did with ZeRO-3 in data parallelism, we're now passing activation tensors sequentially between GPUs in a "pipeline". While conceptually simple, efficiently implementing this technique is quite tricky. Let's dive right into the details!</p>

        <h3>Splitting layers on various nodes - All forward, all backward</h3>
        
        <p>So, let’s say we simply spread the layers on several devices, e.g. a first GPU will take the first few layers and a second GPU will take the second part of the models and so on. The forward pass through our model now simply involves sequentially passing the batch of data along the model and thus successively using each compute device.</p>
        
        <p>We have a direct first advantage: the required interconnect bandwidth stays quite low as we only send moderate-sized activations at a handful of location along the model depth. It can make a huge difference versus e.g. communications in Tensor Parallelism, which happens several times within each layer.</p>
        
        <p>But maybe you start feeling a glimpse of the troubles to come: <strong>“sequentially”</strong> and <strong>“successively”</strong>?!? This doesn’t sound very efficient in the world of parallel computations, especially after our discussion on computation and communication overlap.</p>

        <p>Indeed reader! The main challenge in pipeline parallelism will be how to efficiently circumvent the sequential nature of PP to keep our GPU busy at all times and avoid having one GPU computing while the others are waiting. Here is how our GPU utilization is looking when doing a naive and simple forward and backward pass through the model (here the numbers indicate the model layers):</p>

        <p><img alt="image.png" src="/assets/images/pp_afab.svg" /></p>
        <div  class="figure-legend"><p>An example of Pipeline parallelism for a model with 16 layers distributed across 4 GPUs. The numbers correspond to the layer IDs.</p>
        </div>
        <p>The remaining idle time is indicated in grey and usually called the “bubble” and the sight of this probably break your heart after we spent so much time optimizing throughput.</p>
        
        <p>We can quantify how efficient a pipeline setup is by looking at how much time we loose because of the bubble. Let’s say <d-math>t_f</d-math> and <d-math>t_b</d-math> are the times for the forward and backward pass, respectively, as measured for one microbatch and one stage of the pipeline (a simple assumption is often to have   <d-math>t_b \approx 2 \times t_f</d-math> which you can see on the above graph). If we could perfectly parallelize the ideal total time would be <d-math>t_{id}=t_f + t_b</d-math>. However, we can count on the graph that due to the pipeline bubble there is additional time of <d-math>t_{pb}=(p-1)*(t_f+t_b)</d-math> (where <d-math>p</d-math> is the degree of pipeline parallelism, i.e the number of GPU on the above graph) ie. the time each GPU is waiting while other GPUs are computing.</p>
        
        <p>We can compute the ratio of the additional bubble time over the ideal time:
        </p>
        
        <d-math block>
            r_{bubble} = \frac{(p-1)*(t_f+t_b)}{t_f+t_b} = p-1
        </d-math>

        <p>As we add more stages the bubble time thus increases and the utilization drops. As we can see, the bubble can be very large in a naive implementation!</p>
        <p>Thankfully, various pipeline parallelism schemes have been designed to <strong>reduce the size of the bubble</strong>.</p>

        <p>Let’s take a first tool out of our toolbox and think about splitting our batch into smaller bit-sized portions which can be processed in parallel or almost, like we did before in data parallel for instance. Now when the second GPU is busy processing micro-batch 1, the first GPU can already start processing micro-batch 2. Here is a schedule using 8 micro-batches:</p>
        
        <p><img alt="pp_afab2.svg" src="/assets/images/pp_afab2.svg" /></p>

        <aside>Before the numbers in the diagram indicated the layers but in all pipeline parallel plots from now including this one it indicates a microbatch. You can think of each square here to contain several layers as seen in the previous figure. </aside>

        <p>The above schedule is called the <strong><em>all-forward-all-backward (AFAB)</em></strong> schedule as we first do all forward passes and then only all-backward passes. The advantage is that forward and backward steps are still generally sequential and so we're preserving the general organization of our model training code. It makes this PP implementation one of the simplest to implement.</p>

        <p>You can find the full implementation of the AFAB pipeline in picotron:</p> 
        
        <details style="background: #f6f8fa; border: 1px solid #d0d7de; border-radius: 6px; margin: 1em 0;">
            <summary style="padding: 12px; cursor: pointer; user-select: none; background: #f3f4f6; border-bottom: 1px solid #d0d7de;">
                👉 AFAB PP implementation in Picotron (Click to expand)
            </summary>
                <div class="code-embed-container" style="margin: 0; border-radius: 0; overflow-x: scroll; width: max-content; min-width: 100%; font-size: 8px;"></div>
                    <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fpicotron%2Fblob%2F0035cce0e04afd6192763b11efe50010d8ad0f71%2Fpicotron%2Fpipeline_parallel%2Fpipeline_parallel.py%23L54-L83&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>
                </div>
        </details>

        <p>Let’s estimate the bubble in this example. The difference with our first example is that the ideal time to process <d-math>m</d-math> microbatches is now <d-math>t_{id} = m*(t_f+t_b)</d-math>:</p>

        <d-math block>
            r_{bubble} = \frac{(p-1)*(t_f+t_b)}{m*(t_f+t_b)} = \frac{p-1}{m}
        </d-math>

        <p>As we can see, we can fight some inefficiencies of pipeline stages by adding more microbatches, reducing the size of the bubble by a factor of <d-math>m</d-math>.</p>
        
        <p>However, as annoying as the bubble is the memory storage required for storing all activation. We need to keep all of the activations in memory until we reach the backward stage which lead to a quick memory explosion in these implementations of PP. Can we do better and avoid this memory explosion?</p>
        
        <p>Since the memory explosion is triggered by the activation we store for the backward pass, let’s try to see if we can start performing the backward pass while we are still performing other forward part of the computation. This will allow us to drop some of the activations we need for the backward pass as soon as possible.</p>

        <h3>One-forward-one-backward and LLama 3.1 schemes</h3>

        <p>This schedule is called <strong><em>one-forward-one-backward (1F1B)</em></strong> as the middle/steady state involves alternatively performing one forward and one backward pass. The general idea is to start performing the backward pass as soon as possible. The schedule looks like this:</p>
        
        <p><img alt="image.png" src="/assets/images/pp_1f1b.svg" /></p>

        <p>If you count carefully you'll see that the bubble still has the same size so our training efficiency is not significantly improved. However we only need to store activations for <d-math>p</d-math> micro-batches (where <d-math>p</d-math> is the degree of pipeline parallelism) instead of <d-math>m</d-math> (where <d-math>m</d-math> was the number of microbatches) which can reduce the activation memory explosion we had in the AFAB schedule. As a consequence we can add more microbatches which then will actually reduce the bubble.</p>

        <p>A major complexity of this setup, visible on the above graph is how forward and backward passes are not anymore cleanly sequential but performed in parallel across devices and interleaved. This means we will have to schedule a switch from forward to backward passes independently on each device instead of in a simple and common central training loop as usual.</p>

        <p>This is one of the reason implementing Pipeline Parallelism usually requires rather extensive modifications to training code as well as modeling code.</p>
        
        <p>You can find a full implementation of 1F1B in picotron as well:</p> 
        
        <details style="background: #f6f8fa; border: 1px solid #d0d7de; border-radius: 6px; margin: 1em 0;">
            <summary style="padding: 12px; cursor: pointer; user-select: none; background: #f3f4f6; border-bottom: 1px solid #d0d7de;">
                👉 1F1B PP implementation in Picotron (Click to expand)
            </summary>
                <div class="code-embed-container" style="margin: 0; border-radius: 0; overflow-x: scroll; width: max-content; min-width: 100%; font-size: 8px;"></div>
                    <script src="https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fpicotron%2Fblob%2F0035cce0e04afd6192763b11efe50010d8ad0f71%2Fpicotron%2Fpipeline_parallel%2Fpipeline_parallel.py%23L85-L145&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></script>
                </div>
        </details>
        
        <p>Let's take a look at how the 1F1B Pipeline Parallelism schedule scales in practice with some benchmarks on our cluster:</p>
        
        <p><img alt="Throughput scaling of Pipeline Parallelism with varying microbatch sizes" src="/assets/images/pp_1f1b_scaling.png" /></p>
        
        <p>On the left, with a number of microbatches equal to –or less than– PP degree minus one (<d-math>m = p - 1</d-math>), we see how detrimental the pipeline bubble can be - performance are low and even drops as we scale PP. The right plot shows that using many more microbatches than PP degree (<d-math>m = 32 \gg p - 1</d-math>) helps improve low-PP-degree performances while still staying limited at very large PP degree. In practice it's not possible to arbitrarly increase the number of microbatches to maintain the ratio of <d-math>m \gg p - 1</d-math> since we're ultimately constrained by the target global batch size. With a maximal possible number of microbatches as we add more PP degree, we'll ultimately have to increase the bubble size according to <d-math>r_{bubble} = \frac{p - 1}{m}</d-math>.</p>
        
        <p>Interestingly, at small number of micro-batches the performance only drops by 14% when scaling from one node (<d-math>p = 8</d-math>) to two nodes (<d-math>p = 16</d-math>) - a much better scaling than Tensor Parallelism which typically sees around 43% performance degradation in similar cross-node scenarios. This type of behavior when hitting the lower-bandwith inter-node network makes Pipeline Parallelism particularly attractive for distributed training across multiple nodes.</p>

        <p>While 1F1B significantly reduces our activation memory footprint, we see on this last graph that the pipeline bubble remains a major efficiency bottleneck. With the bubble size still proportional to the number of pipeline stages, we're leaving valuable GPU compute idle. Can we design an even smarter schedule to minimize this wasted computation time?</p>
        
        <h3>Interleaving stages</h3>
                
        <p>The 1F1B schedule has let us improved memory usage but not much the size of the idle buddle. Any way we could still push this frontier?</p>
        
        <p>Well it turns out this is possible if we are willing to bring in a few additional communication operations. Time to talk about <strong><em>interleaved stages</em></strong>.</p>
        
        <p>Up to now we’ve sliced our model naively along the model depth dimensions, hosting for instance layers 1-4 on the first GPU and layers 5-8 on the second GPU. But there are other ways we could think about slicing our layers, e.g. having odd layers 1, 3, 5, 7 on the first GPU and even layers 2, 4, 6, 8 on the second GPU.</p>
        
        <p>This can be seen in general as a kind of “looping pipeline” where a micro-batch will move in circles from one GPU to the next as it goes through the forward pass through the model. Let's take a graphical look at how this works:</p>
        
        <p><img alt="pp_1f1b_interleaved.svg" src="/assets/images/pp_1f1b_interleaved.svg" /></p>

        <div class="figure-legend"><p>An example of interleaved pipeline parallelism for a model with layers distributed across 4 GPUs. Numbers still correspond to the microbatches IDs but for clarity we've colored differently the first and the last layers of the model to illustrate how layers are spread accross GPUs.</p>
          </div>

        <p>As a consequence we see additional communications happening as the model goes several times through each GPU for the same computation that previously just took one pass. However, each forward and backward pass is divided by a factor of <d-math>v</d-math>, where <d-math>v</d-math> is the number of stages or model chunks per GPUs as we are able to better interleave forward and backward passes. </p>


        <d-math block>
            \begin{aligned}
            &t_{pb} = \frac{(p-1)*(t_f+t_b)}{v} \\
            &r_{bubble} = \frac{1}{v}\frac{(p-1)*(t_f+t_b)}{m*(t_f+t_b)} = \frac{p-1}{v*m} 
            \end{aligned}
        </d-math>


        <p>So we can now decrease the bubble by adding microbatches and interleaved stages, but note that quantitatively, the amount of communication also increases by <d-math>v</d-math> so it’s a trade off. In the following plot you can see several configurations for a PP setup with <d-math>p=8</d-math>, where the special case of <d-math>m=1, v=1</d-math> corresponds to naive pipeline parallelism and the configurations with <d-math>v=1</d-math> are AFAB or 1F1B setups and <d-math>v \neq 1</d-math> are interleaved configurations.</p>

        <!-- <iframe class="l-body" id="plotFrame23" src="assets/data/benchmarks/pp_bubblesize.html" width="90%" scrolling="no" frameborder="0"></iframe> -->
        <div class="l-body" id="fragment-pp_bubblesize"></div>
        <!-- <script>
            window.addEventListener('load', function() {
                const frame = document.getElementById('plotFrame23');
                frame.style.height = frame.contentWindow.document.documentElement.scrollHeight + 'px';
                frame.style.width = frame.contentWindow.document.documentElement.scrollWidth + 'px';
            });
        </script> -->
        <!-- <p><img alt="pp_bubblesize.png" src="/assets/images/pp_bubblesize.png" /></p> -->


        <p>Scheduling also becomes more complex here as we have to decide on a given GPU and at a given moment whether we are prioritizing earlier micro-batches going through later layers –meaning that we close the forward and backward loops as fast as possible (so called “depth-first”, i.e. prioritizing getting batches out of the model as fast as possible)– or if we prioritize to first have later micro-batches going through earlier layers (so called “breadth-first” i.e. prioritizing filling in the pipeline as much as possible). This choice is explained in detail in the nice "Breadth-Fist Pipeline" paper<d-cite bibtex-key="lamypoirier2023breadthfirstpipelineparallelism"></d-cite>.</p>
        
        <p>You now have all the elements to understand the pipeline parallelism approach in Llama 3.1 which is using a one-forward-one-backward setup with interleaved stages and a priority setting tuneable between depth-first and breadth-first.</p>

        <p><img alt="pp_llama3.1_schedule.png" src="/assets/images/pp_llama3.1_schedule.png" /></p>

        <p>However, we haven’t reached the end of possible pipeline schedules and recently some methods have been proposed to <strong>reduce the bubble to virtually zero</strong>! These techniques were for instance used in the DeepSeek V3/R1 implementation<d-cite bibtex-key="deepseekai2024deepseekv3technicalreport"></d-cite>. Peaked your curiosity? Let’s have a final quick look at these magical schedules before we leave the world of Pipeline Parallelism!</p>

        <h3>Zero Bubble and DualPipe</h3>

        <p>Even more sophisticated ways to reduce the bubble have recently been proposed which reached close to a “zero bubble” regime. The secret here is to split at an even finer-grained level the operations involved in order to interleave them in the most efficient way. For instance the pipeline implementation approach in DeepSeek V3/R1, called DualPipe, reaches close to a zero bubble regime.</p>

        <aside>Ultimate "flex" in DeepSeek V3 technical report<d-cite bibtex-key="deepseekai2024deepseekv3technicalreport"></d-cite> where the authors indicate that their setup "achiev[ed] a near-zero all-to-all communication overhead".</aside>

        <p>Let’s briefly see how this can work by summarizing the ZeroBubble<d-cite bibtex-key="qi2023zerobubblepipelineparallelism"></d-cite> work which is a precursor to DualPipe. The base observation of ZeroBubble is that the backward pass through a matrix multiplication actually involves two separated operations: backward operation for the inputs (B) and the backward operation for the weights (W):</p>
    
        <p>While the output of B, the backward pass for the input, is necessary for performing the backward pass of the lower layers, the backward pass of the weights, W, is not necessary for the rest of the backward pass and generally only needs to be performed before the optimiser step. We can see that in the following diagram: </p>

        <p><img alt="image.png" src="/assets/images/pp_zerobubble_compgraph.png" /></p>
        
        <p>This means W can be flexibly scheduled anywhere after the corresponding B of the same stage. This allows for strategic placement of W to fill the pipeline bubbles. The ZB-H2 schedule on the top right is an example of (theoretical) schedule with zero bubble taking advantage for this fine-grained decomposition.</p>

        <p><img alt="image.png" src="/assets/images/pp_zerobubble_ppschedule.png" /></p>

        <div class="figure-legend"><p>On the top (Figure 2 from the ZeroBubble paper): the classical 1F1B schedule, interleaving forward and backward pass but keeping a coarse-grained backward pass. On the bottom two graphs (Figure 3 from the ZeroBubble paper), two variantes of the ZeroBubble schedule, splitting the backward operation in a "B" and a "W" finer-grained operations. The last schedule, so-called "ZB-H2" is an example of (theoretical) schedule with zero bubble taking advantage for this fine-grained decomposition.</p>
        </div>

        <p>DeepSeek’s DualPipe introduced with its V3 technical report <d-cite bibtex-key="deepseekai2024deepseekv3technicalreport"></d-cite> an extension of this decomposed approach to the additional case of two streams propagating from both ends of the PP dimension, these streams being interleaved to minimize even further idle time in the GPUs. This schedule is displayed in the following scheduling graph and is even more complex than the previous ones:</p>

        <p><img alt="image.png" src="/assets/images/pp_zerobubble_dualpipe.png" /></p>

        <p>In general, fully optimizing such complex schedules involve carfully measuring the duration of the various fine-grained operations and solving a ILP to minimize the final bubble time. See for instance in the ZeroBubble paper<d-cite bibtex-key="qi2023zerobubblepipelineparallelism"></d-cite> for a discussion of the heuristics and algorithms to perform such a scheduling. As a result, the ZeroBubble and DualPipe schedules are too complex for us to give here code snippets but you should start to have a general idea of the concepts involved. </p>

        <p>This concludes our tour into the world of pipeline schedules and bubbles. We hope you enjoyed this guided tour!</p>
        
        <p>It's now time to turn to the last parallelism method we'll detail and which we can use to train large models efficiently: <strong>Expert parallelism</strong>.</p>
        
        <h2>Expert parallelism</h2>

        <p>This is our last parallelism method to discuss. Before tackling it, if you don't have any exposure to Mixture-of-Experts, feel free to read about them in <a href="https://huggingface.co/blog/moe">this previous, much shorter, blog post</a> we published some time ago and which should help you better understand the Mixture-of-Experts (MoE) architecture in general.</p>

        <p>Mixture-of-expert models have gained recent traction and visibility with models such as GPT-4, Mixtral<d-cite bibtex-key="jiang2024mixtralexperts"></d-cite> or more recently DeepSeek-V3/R1. The basic idea is that instead of having a single feedforward module per layer we can have several parallel modules and route tokens through one or the other to be processed differently.</p>

        <p><img alt="ep_moe.png" src="/assets/images/ep_moe.png" /></p>
        <div class="figure-legend"><p>Illustrationg of a MoE layer taken from the Switch Transformers paper<d-cite bibtex-key="fedus2022switchtransformersscalingtrillion"></d-cite></p>
        </div>
        
        <p>The design of MoE layers makes it actually easy to implement parallelism across the experts  dimension for what we will call <strong>Expert parallelism</strong> (EP). Since the feedforward layers are fully independent we can simply put each expert's feedforward layer on a different worker. Compared to TP it's much more lightweight, since we don't need to split the matrix multiplication, we just need to route the hidden states of a token to the right expert.</p>

        <p>In practice, EP will typically be used in conjunction with other forms of parallelism - for instance Data Parallelism. This is because EP only affects the MoE layers and doesn't shard the input tokens (unlike Context Parallelism which shards tokens along the sequence length dimension). This means our GPUs would be doing redundant compute for all the non-MoE blocks if we only used EP. By combining EP with DP, we can efficiently shard both the experts and the input batches across our GPUs, as we can see in the simplified diagram below:</p>

        <p><img alt="ep_schema.png" src="/assets/images/ep_schema.png" /></p>
        <div class="figure-legend"><p>Source: A Survey on Mixture of Experts<d-cite bibtex-key="cai2024surveymixtureexperts"></d-cite> </p>
        </div>
        <p>But let's not get ahead of ourselves - our <a target="_self" href="#5d_parallelism_in_a_nutshell">following section</a> will specifically talk about all the interactions between different parallelism strategies, so don't worry if you don't understand yet this last diagram.</p>

        <p>In practice, there are a few tricks to make EP work efficiently and they are closely tied to model design. For instance, DeepSeek-V3 enforces a constraint in the router, ensuring that each token is sent to at most M nodes (in their case, 4) to keep the tokens on a single node and reduce communication overhead. While Expert parallelism has been around for a while<d-cite bibtex-key="lepikhin2020gshardscalinggiantmodels"></d-cite> it is just now gaining new traction with the MoE architecture gaining more traction.</p>

        <p>We plan to add a more complete example of EP in picotron/nanotron soon, so stay tuned for more!</p>
        
        <h2>5D parallelism in a nutshell</h2>

        <p>Congratulation reader, you have now seen all 5 parallelism strategies you can use to scale model training: </p>
        <ol>
            <li>Data Parallelism (DP) – along the batch dimension</li>
            <li>Tensor Parallelism (TP) - along the hidden dimension</li>
            <li>Sequence and Context Parallelism (SP/CP) - along the sequence dimension</li>
            <li>Pipeline Parallelism (PP) - along the model layers</li>
            <li>Expert Parallelism (EP) - along the model experts</li>
        </ol>

        <p>As well as the 3 ZeRO strategies which can be combined with Data Parallelism for memory reduction: </p>
        <ol>
            <li>ZeRO-1 – sharding optimizer states among the DP replicas</li>
            <li>ZeRO-2 – sharding optimizer states and gradients among the DP replicas</li>
            <li>ZeRO-3 – sharding optimizer states, gradients and parameters among the DP replicas</li>
        </ol>

        <p>At this stage, one aspect you are probably curious about is how all these parallelism and ZeRO strategies compare to, and interact with, each other. In other words, which ones should we use and efficiently combine together, and which ones should we rather keep separated?</p>

        <p>Let’s take a look at the similarities and interplay. We'll start by comparing Pipeline parallelism are ZeRO-3 side-by-side as they have some very close similarities but also important differences.</p>
        
        <p><strong>Pipeline parallelism vs. ZeRO-3 -</strong> Both PP and ZeRO-3 are ways to partition the model weights over several GPUs and perform communication/computation along the model depth axis (for example in ZeRO-3, we prefetch the next layer while computing). This means in both cases full layer operations are computed on each device, as opposed to TP or EP for instance in which computation are performed on sub-layer units.</p>
        <aside>In the following we say “a layer” to simplify what should be in general called “a set of layer” (as the basis sharding unit of the model).</aside>
        
        <p>However, there are a few major differences between PP and ZeRO-3 approaches:</p>

        <div class="l-body">
        <table>
            <thead>
              <tr>
                <th> </th>
                <th><strong>ZeRO-3</strong></th>
                <th><strong>Pipeline Parallelism</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Each compute unit stores </td>
                <td>only a fraction of a layer</td>
                <td>a full layer</td>
              </tr>
              <tr>
                <td>Communication is used to transfer</td>
                <td>weights</td>
                <td>activations</td>
              </tr>
              <tr>
                <td>Orchestration</td>
                <td>model agnostic</td>
                <td>model agnostic</td>
              </tr>
              <tr>
                <td>Implementation challenges</td>
                <td>Complex to handle model partitioning and communications</td>
                <td>Complex to handle efficient PP schedules</td>
              </tr>
              <tr>
                <td>Scaling considerations</td>
                <td>Prefers large <d-math>mbs</d-math> and <d-math>seq\_len</d-math> to hide comms</td>
                <td>Prefers large <d-math>\text{grad\_acc}</d-math> to hide bubble</td>
              </tr>
            </tbody>
           </table>
          </div>

        <p>As you can see, ZeRO-3 and PP sove the same challenge but involve different approaches and the choice between both will depend whether you decide to focus communication either on weights or on activations. While they can be combined, it's not often done in practice as doing so requires increasing the global batch size significantly to amortize the communication costs, creating a tradeoff between global batch size, model size, network bandwidth, and training efficiency. If you decide to combine them, ZeRO-3 should be configured to keep the weights in memory during the series of PP micro-batches to minimize as much as possible un-necessary communication overhead.</p>
        
        <p>On the other hand, ZeRO-1 and ZeRO-2, which focus on optimizer states and gradients, can be easily combined with Pipeline Parallelism and are complementary to it. Combining them don't raise any particular new challenge. For instance, the training of DeepSeek-v3 used PP combined with ZeRO-1 (sic).</p>

        <p><strong>Tensor Parallelism</strong> (with Sequence Parallelism) is naturally complementary and can be combined with both Pipeline Parallelism and ZeRO-3 as it relies on the distributive property of matrix multiplications which allows weights and activations to be sharded and computed independently before being combined.</p>

          <img alt="TP & SP diagram" src="/assets/images/5d_nutshell_tp_sp.svg" style="width: 1000px; max-width: none;" />
          <!-- <p><img alt="image.png" src="/assets/images/placeholder.png" /></p> -->

          
          <p>The main reason we don't want to use TP only for parallelism is that, in practice, TP has two limitations we've discussed in the previous sections: First, since its communication operations are part of the critical path of computation, it's difficult to scale well beyond a certain point at which communication overhead begins to dominate. Second, unlike ZeRO and PP which are model-agnostic, TP requires careful handling of activation sharding - sometimes along the hidden dimension (in the TP region) and sometimes along the sequence dimension (in the SP region) - making it more cumbersome to implement correctly and requiring model-specific knowledge to ensure proper sharding patterns throughout.</p>
        
        <p>As a consequence, when combining parallelism strategies, TP will typically be kept for high-speed intra-node communications while ZeRO-3 or PP can be used for parallelism groups spanning lower speed inter-node communications as their communication patterns require less bandwidth (for PP) or can be more easily overlapped with computation (for ZeRO-3). The main consideration when combining these techniques is to organize the GPU efficiently in groups for each parallelism dimension to maximize throughput and minimize communication overhead, while being mindful of TP's scaling limitations. For instance, the groups of GPUs communicating for TP should be kept inside nodes.</p>
        
        <p><strong>Context Parallelism</strong> and <strong>Expert Parallelism</strong> also help us shard activations, and can be seen as complimentary to TP. The first one handles long sequences while the second enables distributed Mixture of Experts training and they can be combined together without any particular issue.</p>

        <p><strong>Context Parallelism (CP)</strong> specifically targets the challenge of training with very long sequences by sharding activations along the sequence dimension across GPUs. While most operations like MLPs and LayerNorm can process these sharded sequences independently, attention layers require communication since each token needs access to keys/values from the full sequence. As we saw in <a target="_self" href="#context_parallelism"> CP section</a>, this is handled efficiently through ring attention patterns that overlap computation and communication. CP is particularly valuable when scaling to extreme sequence lengths (128k+ tokens) where, even when using full activation recomputation, the memory requirements for attention would be prohibitive on a single GPU.</p>
        
        <img alt="CP diagram" src="/assets/images/5d_nutshell_cp.svg" style="width: 1000px; max-width: none;" />

        <!-- <p><img alt="image.png" src="/assets/images/placeholder.png" /></p> -->
        
        
        <p><strong>Expert Parallelism (EP)</strong> specifically targets the challenge of training Mixture of Experts (MoE) models by sharding specialized "experts" across GPUs and dynamically routing tokens to relevant experts during computation. The key communication operation in EP is the `all-to-all` operations routing tokens to their assigned experts and gathering the results back. While this operation introduces some communication overhead, it enables scaling model capacity significantly since each token is only processed during inference (and training) by a much smaller fraction of the total parameters. In terms of distributed training/inference, partitioning experts across GPUs becomes relevant when models scales to a large number of experts.</p>
        <aside>For instance DeepSeek V3 uses 256 experts.</aside>
        
        <img alt="EP diagram" src="/assets/images/5d_nutshell_ep.svg" style="width: 1000px; max-width: none;" />

        <div class="note-box">
            <p class="note-box-title">📝 Note</p>
            <div class="note-box-content">
                <p>This similarity between EP and DP in terms of input handling is why some implementations consider Expert Parallelism to be a subgroup of Data Parallelism, with the key difference being that EP uses specialized expert routing rather than having all GPUs process inputs through identical model copies.</p>
                </div>
        </div>

        <p><strong>Scope and focus</strong> Let's also quickly summarize the sub-part of the model where some of these different parallelism strategies have the most impact:</p>

        <ul>
            <li>Tensor Parallelism (and Sequence Parallelism) affects computation throughout the entire model by sharding both weights and activations.</li>
            <li>Context Parallelism primarily impacts attention layers since that's where cross-sequence communication is required, with other layers operating independently on sharded sequences.</li>
            <li>Expert Parallelism primarly affects the MoE layers (which replace standard MLP blocks), leaving attention and other components unchanged</li>
            <li>Pipeline Parallelism and ZeRO are not especially specific to any sub-module or component with the exception that modules and layers need to be balanced in Pipaline Parallelism, the first and last layers are thus often treated differently due to the additional embedding layers.</li>
        </ul>
 
        <table>
            <thead>
              <tr>
                <th><strong>Tensor + Sequence Parallel</strong></th>
                <th><strong>Context Parallel</strong></th>
                <th><strong>Expert Parallel</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>shards weights and activations along hidden/seq dim</td>
                <td>shards activations along sequence dim</td>
                <td>shards specialized expert weights and activations</td>
              </tr>
              <tr>
                <td>communication for matrix multiply operations (column/row linears)</td>
                <td>communication for attention key/values</td>
                <td>communication for token routing to experts</td>
              </tr>
              <tr>
                <td>model-specific implementation needed</td>
                <td>model-agnostic except for attention</td>
                <td>model-agnostic except for MoE layers</td>
              </tr>
              <tr>
                <td>Prefers high-bandwidth intra-node communication</td>
                <td>Prefers large sequence lengths</td>
                <td>Requires MoEs</td>
              </tr>
            </tbody>
           </table>

        <p><strong>Summarizing it all–</strong> Now what about gathering and combining all the techniques we've seen in a single diagram combining them all. Yes, we're up for the challenge!</p>
        <p>In this summary diagram, you will find illustrated activations and modules for a single transformers layer –in it's MoE variant–. We also illustrate the various directions of parallelism and the communication operations we've been discussing in all the previous sections.</p>
        
        <p><img alt="image.png" src="/assets/images/5d_full.svg" style="width: 1000px; max-width: none;"/></p>

        <p>We can also represent side-by-side a <strong>full overview</strong> of the memory savings for each one of these strategies. We'll plot them with different sequence length as well as with selective (top) and full (bottom) recomputation so you can see how they all play with activations:</p>
        
        <img alt="5Dparallelism_8Bmemoryusage.svg" src="/assets/images/5Dparallelism_8Bmemoryusage.svg" style="width: 1000px; max-width: none;"/>

        <p>Let's finish this section with a high level view at all of these techniques, their main underlying idea and major bottleneck:</p>

        <table>
            <thead>
              <tr>
                <th><strong>Method</strong></th>
                <th><strong>Memory savings applies specifically on</strong></th>
                <th><strong>Parallel/sharding dimension</strong></th>
                <th><strong>Disadvantage</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>DP</td>
                <td>Activations (reduce local batch size)</td>
                <td>Batch</td>
                <td>Limited by max batch size</td>
              </tr>
              <tr>
                <td>PP</td>
                <td>Model parameters</td>
                <td>Model layers</td>
                <td>Idle bubble and complex schedules</td>
              </tr>
              <tr>
                <td>TP/SP</td>
                <td>Model parameters and activations</td>
                <td>Hidden dimension / Sequence length</td>
                <td>Requires high bandwidth communication</td>
              </tr>
              <tr>
                <td>CP</td>
                <td>Activations</td>
                <td>Sequence length</td>
                <td>Add communication overhead in attention modules</td>
              </tr>
              <tr>
                <td>EP</td>
                <td>Experts parameters</td>
                <td>Expert dimension</td>
                <td>Requires MoE layers, add routing communication overhead</td>
              </tr>
              <tr>
                <td>ZeRO-1</td>
                <td>Optimizer states</td>
                <td>Sharded among DP replicas</td>
                <td>Params communication overhead</td>
              </tr>
              <tr>
                <td>ZeRO-2</td>
                <td>Optimizer states and gradients</td>
                <td>Sharded among DP replicas</td>
                <td>Params communication overhead</td>
              </tr>
              <tr>
                <td>ZeRO-3</td>
                <td>Optimizer states, gradients, and model parameters</td>
                <td>Sharded among DP replicas</td>
                <td>Params communication overhead</td>
              </tr>
            </tbody>
           </table>
        
        <p>Clearly, none of these techniques is a silver bullet for magical scaling we'll often have to combine them in one way or another. Can we actually come up with a few rules that help finding a good starting point to select and combine them? This will be the topic of our next section.</p>
    
        <h2>How to Find the Best Training Configuration</h2>

        <p>We’ve now covered all the parallelism techniques that are actually used to distribute and training larger models as well as how and why they can be combined together. There remain a general question: which ones should we choose in the end and how to decide on a specific combination?</p>

        <p>We touched this a little bit in the previous section but let's now walk in details through a possible decision process, step by step, keeping in mind that you'll always have to run a few experiments to find the definitive optimal setup for your compute cluster given its various physical properties, network bandwidth, GPUs per node, memory per GPU, etc.</p>
        
        <h3>Step 1: Fitting a Training Step in Memory</h3>
        
        <p>First, we need to figure out how we can fit a full model instance on our GPUs (we focus on a single instance for now - even though we may use DP for ZeRO). There are two general cases.</p>

        <p>GPU-rich case 🤑 - when you have plenty of GPUs available:</p>
        <ul>
            <li>For models under 10B parameters, you can use a single parallelism technique, e.g. Tensor Parallelism or ZeRO-3/DP with Full Recompute across 8 GPUs</li>
            <li>For models between 10B-100B parameters requiring more than 8 GPUs, you have several options:</li>
            <ul>
            <li>Combining Tensor Parallelism (TP=8) with Pipeline Parallelism</li>
            <li>Combining Tensor Parallelism (TP=8) with Data Parallelism (ZeRO-3)</li>
            <li>Using only ZeRO-3 (i.e. only pure Data Parallelism) </li>
            </ul>
            <li>At 512+ GPU scale, pure Data Parallelism/ZeRO-3 will start to becomes inefficient due to communication cost - it can be better to then combine DP with either Tensor or Pipeline Parallelism</li>
            <li>At 1024+ GPU scale, a recommended setup can be Tensor Parallelism TP=8 with Data Parallelism (ZeRO-2) and Pipeline Parallelism</li>
        </ul>

        <p>Special considerations:</p>
        <ul>
            <li>For very long sequences, you will probably want to add Context Parallelism (CP) across nodes.</li>
            <li>For Mixture of Experts architectures, you will advantageously use Expert Parallelism (EP) across nodes.</li>
        </ul>

        <p>GPU-poor case 😭 - when you might be low on GPU resources:</p>
        <ul>
            <li>You can enable full activation recomputation to trade some compute for memory (and train a bit slower).</li>
            <li>You can increase gradient accumulation to process larger batches with limited memory.
            </li>
        </ul>

        <p>Now that we have a first model instance training, we need to make sure we have the right batch size.</p>

        <h3>Step 2: Achieving Target Global Batch Size </h3>
        
        <p>Depending on where step 1 left us in terms of micro batch size and DP, our current batch size might be too small or too big. It's now time to hit our target batch size.</p>
        
        <p>To increase our current global batch size:</p>
        <ul>
            <li>We can scale up Data Parallelism or gradient accumulation steps</li>
            <li>For long sequences, we can leverage Context Parallelism</li>
        </ul>

        <p>To decrease our current global batch size:</p>
        <ul>
            <li>We can reduce Data Parallelism in favor of other parallelization strategies</li>
            <li>For long sequences, we can reduce Context Parallelism</li>
        </ul>

        <p>Ok, now we have the model running in the general configuration we want in terms of model size and batch size, but are we training it the fastest way? Let's now start to optimize throughput as much as possible.</p>

        <h3>Step 3: Optimizing Training Throughput</h3>

        <p>So we want to make sure the training is running as fast as possible so all our precious GPUs are well utilized at all times. As long as memory and communication aren't bottlenecks we can try the following:</p>

        <ul>
            <li>Scale up Tensor Parallelism (using the fast intra-node bandwidth) until we reach a degree close to the node size, so that we can reduce other parallelism</li>
            <li>Increase Data Parallelism with ZeRO-3 while keeping target batch size</li>
            <li>When Data Parallelism communication starts to become a bottleneck, transition to using Pipeline Parallelism</li>
            <li>Try scaling up different parallelisms one by one</li>
            <li>Experiment with several micro batch size (mbs) to aim for an optimal balance between max GBS, model size, compute, and communication.</li>
        </ul>

        <!-- <p>We can roughly summarize the journey to the best configuration in the following diagram:</p>

        <p><img alt="image.png" src="/assets/images/placeholder.png" /></p>
 -->

        <p>This concludes our very deep dive into 5D parallelism. However, besides scaling our model efficiently across GPUs there is another way to improve model throughput and memory management. It involves a better understanding of how GPU operate at a low level and is among the necessary knowledge to be able to take maximal advantage of large GPU clusters.</p>

        <p>Time to turn the lights off and activate CUDA mode! </p>
        
        <h2>Diving in the GPUs – fusing, threading, mixing</h2>
        
        <p>Up to now our discussion has been focused on the high-level organization of our model operations. We’ve moved around computations on various accelerators, taking into account general memory constraints and high-level scheduling of the compute units.</p>

        <p>But this ignored all the optimizations we can do at a much lower level by carefully understanding how our model operations are scheduled and performed on each GPU.</p>

        <p>This section will dive into much more details of the GPU architecture and in particular in NVIDIA’s GPU architecture but the general ideas, as often, can be reused on similar accelerator units.</p>

        <p>We’ll briefly explain how GPU are organized before covering the Flash-Attention revolution, how to efficiently schedule workload on GPU and finally explain how various precisions can be efficiently used on GPU.</p>



        <h3>A primer on GPU</h4>
        
        <p>Generally, GPUs have a very hierarchical organization. In this primer we’ll keep the discussion at the concept levels that are necessary for the rest of our presentation.</p>
    
        <p>On the compute side, GPUs consist of an array of compute units called <strong>Streaming Multiprocessors</strong> (SM). Each SM contains and controls a set of streaming processors, also known as cores. For example, an Nvidia H100 GPU has 132 SMs with 128 cores per SM, resulting in a total of 16,896 cores (see <a href="https://resources.nvidia.com/en-us-tensor-core">docs for tensor cores</a> for details), each capable of handling multiple threads simultaneously.</p>
    
        <p><img alt="image.png" src="/assets/images/diving_primergpu.svg" /></p>
        <div class="figure-legend"><p>Source: https://blog.codingconfessions.com/p/gpu-computing</p></div>
    
        <p>The memory side is also highly hierarchical with several layers of cache and memory: <strong>Registers</strong> are the smallest units and are private to the threads during executions, <strong>Shared Memory</strong> and <strong>L1 cache are</strong> shared between the threads running on a single SM, higher up is the <strong>L2 cache</strong> shared by all SMs, finally there is the <strong>Global Memory</strong> which is the largest memory on the GPU (the advertised 80 GB for a H100 for instance) but also the slowest to access and query.</p>
    
        <p><img alt="image.png" src="/assets/images/diving_primergpu2.svg" /></p>
        <div class="figure-legend"><p>Source: https://www.youtube.com/watch?v=ZQKMZIP3Fzg</p></div>

        <p>The goal of GPU will be to run as many workloads as possible, in parallel, on the GPU cores, by taking advantage of this hierarchical organization of compute/memory.</p>
    
        <p>A piece of code running on a core of the GPU is called a <strong>kernel</strong>. It can be written at a high-level in <strong>CUDA</strong> or <strong>Triton</strong> for instance, and is then compiled to Parallel Thread Execution, PTX, the low-level assembly used by NVIDIA GPUs.</p>

        <p>To run the kernel, you will also need a specific code part, called <strong>host code</strong>, which is executed on the <strong>CPU/host</strong> and will take care of preparing data allocations and loading data and code.</p>

        <div class="l-body" style="display: grid; grid-template-columns: 1fr 1fr; align-items: center;">
            <div>
                <d-code block language="python">
                    // Host code                
                    void vecAdd(float* h_A, float *h_B, float *h_c, int n) {
                        // Allocate vectors in device memory
                        int size = n * sizeof(float);
                        float *d_A, *d_B, *d_C;
                        cudaMalloc(&d_A, size);
                        cudaMalloc(&d_B, size);
                        cudaMalloc(&d_C, size);

                        // Copy vectors from host memory to device memory
                        cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
                        cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

                        // Invoke kernel
                        int threadsPerBlock = 256;
                        int blocksPerGrid =
                                (N + threadsPerBlock - 1) / threadsPerBlock;
                        VecAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);

                        // Copy result from device memory to host memory
                        // h_C contains the result in host memory
                        cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

                        // Free device memory
                        cudaFree(d_A);
                        cudaFree(d_B);
                        cudaFree(d_C);
                    }</d-code>
        <div class="figure-legend">
            <p>Host code for a CUDA kernel for adding two vectors. Adapted from https://docs.nvidia.com/cuda/cuda-c-programming-guide/ and https://blog.codingconfessions.com/p/gpu-computing</p>
        </div>
    </div>
            <div>
                <d-code block language="python">
                    // Device code
                    __global__ void VecAdd(float* A, float* B, float* C, int N)
                    {
                        int i = blockDim.x * blockIdx.x + threadIdx.x;
                        if (i < N)
                            C[i] = A[i] + B[i];
                    }
                </d-code>
            <div class="figure-legend">
                <p>Device code containing the definition of the vector addition kernel adapted from https://docs.nvidia.com/cuda/cuda-c-programming-guide/ and https://blog.codingconfessions.com/p/gpu-computing</p>

            </div>
            </div>
        </div>

        <!-- <p><img alt="image.png" src="/assets/images/placeholder.png" /></p>
        <p>Figure 5: Host code for a CUDA kernel for adding two vectors from https://blog.codingconfessions.com/p/gpu-computing</p>
        <p><img alt="image.png" src="/assets/images/placeholder.png" /></p>
 -->
        <p>Kernels are generally scheduled as follow:</p>
        
        <ul>
            <li>threads are grouped in <strong>warps</strong> of sizes of 32. All the threads in a warp are synchronized to execute instructions simultaneously but on different parts of the data.</li>
            <li><strong>warps</strong> are grouped in larger <strong>blocks</strong> of more flexible size (e.g. size 256), each block still being assigned to a single SM. An SM may run several blocks in parallel, however, depending on the resources, not all the blocks may get assigned for execution immediately, some can be waitlisted waiting for resources.</li>
        </ul>

        <p>The main thing to remember from these details is that there are various sizing and allocation constraints (size of the various memories, number of concurrent block and threads in the wraps) which need to be taken into account to use the GPU architecture in the most efficient way.</p>
        
        <p>Most of the time you don’t need to go down to this level of precision and you can luckily reuse the kernels and code prepared by other members of the community. But in any case we want to give you a primer on how to get started with kernels! </p>

        <h3>How to improve performance with Kernels ?</h3>
        

        <p>If you’re looking to add a new operation that lacks an optimized kernel or to speed up an existing PyTorch function, writing kernels from scratch might seem like the most direct route. However, creating high-performance CUDA kernels from scratch requires extensive experience and a steep learning curve. Generally a better way to get started is to leverage <code>torch.compile</code>, which dynamically optimizes PyTorch code by capturing your operations and generating lower-level, high-performance kernels in triton.</p>

        <p>Let’s suppose you want to write a kernel for an activation function called Exponential Linear Unit:</p>

        <d-math block>
            \text{ELU}(x) = \begin{cases}
            e^x - 1 & \text{if } x < 0 \\
            x & \text{if } x \geq 0
            \end{cases}
        </d-math>

        <p>You can start by a simple pytorch implementation and then just add the <code>@torch.compile</code> decorator on top:</p>

        <d-code block language="python">
            @torch.compile
            def elu(x, alpha=1.0):
                return torch.where(x < 0, alpha * (torch.exp(x) - 1), x)
        </d-code>

        <p>The distinction between the compiled and non-compiled versions is striking, especially given that we only added a single decorator. This remarkable difference is illustrated in the graph below (N is the number of columns):</p>

        <p><img alt="image.png" src="/assets/images/torch-compile-triton.png" /></p>

        <!-- <p><img alt="image.png" src="/assets/images/dp_scaling.svg"/></p> -->

        <p>However, if this performance increase is insufficient, you can consider implementing Triton kernels. As a starting point, you can take a look at the triton kernel generated by @torch.compile . To do so, you simply need to set the environment variable <code>TORCH_LOGS</code> to <code>"output_code"</code>:</p>

        <d-code block language="bash">
            export TORCH_LOGS="output_code"
        </d-code>

        <p>Once you run the Python script with the <code>@torch.compile</code> decorator, it will generate and output the corresponding Triton kernel, which, in this case, is:</p>

        <d-code block language="python">
            @triton.jit
            def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
                xnumel = 100000000
                xoffset = tl.program_id(0) * XBLOCK
                xindex = xoffset + tl.arange(0, XBLOCK)[:]
                xmask = xindex < xnumel
                x0 = xindex
                tmp0 = tl.load(in_ptr0 + (x0), xmask)
                tmp1 = 0.0
                tmp2 = tmp0 < tmp1
                tmp3 = tl_math.exp(tmp0)
                tmp4 = 1.0
                tmp5 = tmp3 - tmp4
                tmp6 = tl.where(tmp2, tmp5, tmp0)
                tl.store(out_ptr0 + (x0), tmp6, xmask)
        </d-code>

        <p>To enhance readability, we can modify the variable names, add comments, and make slight adjustments (or ask an LLM to do it for us), as demonstrated below:</p>
        
        <d-code block language="python">
            @triton.jit
            def elu_kernel(input_ptr, output_ptr, num_elements, BLOCK_SIZE: tl.constexpr):
                # Calculate the starting index for this block
                block_start = tl.program_id(0) * BLOCK_SIZE
                # Create an array of indices for this block
                block_indices = block_start + tl.arange(0, BLOCK_SIZE)[:]
                # Create a mask to ensure only valid indices are processed
                valid_mask = block_indices < num_elements
                # Load input values from the input pointer based on valid indices
                input_values = tl.load(input_ptr + block_indices, valid_mask)
                # Define the ELU parameters
                zero_value = 0.0  # Threshold for ELU activation
                negative_mask = input_values < zero_value
                exp_values = tl.math.exp(input_values)
                # Define the ELU output shift
                one_value = 1.0
                shifted_exp_values = exp_values - one_value

                output_values = tl.where(negative_mask, shifted_exp_values, input_values)

                # Store the computed output values back to the output pointer
                tl.store(output_ptr + block_indices, output_values, valid_mask)
        </d-code>
        
        <p>Here, <code>tl.program_id(0)</code> provides a unique block ID, that we use to determine which section of data that block will process. Using this block ID, <code>block_start</code> calculates the starting index for each block’s section, while <code>block_indices</code> specifies the range of indices within that section. A <code>valid_mask</code> ensures that only indices within <code>num_elements</code> are processed, safely loading the data with <code>tl.load</code>. The ELU function is then applied, modifying values based on whether they're negative, and results are written back to memory with <code>tl.store</code>.</p>
        
        <p>When we benchmark the generated kernel using <code>triton.testing.Benchmark</code> we have the following performance:</p>

        <p><img alt="image.png" src="/assets/images/torch-compile-triton-kernel.png" /></p>

        <p>This standalone kernel even demonstrates superior performance with smaller sizes compared to <code>@torch.compile</code> but this is likely just an artifact of the compilation time of <code>torch.compile</code>. In any case, instead of starting from scratch, remember that you can start from such generated kernels and focus your attention to optimizing its performance, saving you a lot of time in the process. </p>
        
        <p>Even in Triton, sometimes, we cannot fully achieve the peak performance of the device due to the language limitations to handle low level details like shared memory and scheduling within streaming multiprocessors (SMs). Triton capabilities are restricted to blocks and scheduling of blocks across SMs. To gain an even deeper control, you will need to implement kernels directly in CUDA, where you will have access to all the underlying low-level details.</p>

        <p>Moving down to CUDA, various techniques can be employed to improve the efficiency of kernels. We will just cover a few here: optimizing memory access patterns to reduce latency, using shared memory to store frequently accessed data, and managing thread workloads to minimize idle times.</p>
        
        <p> Before we dive deeper in CUDA examples, let's summarize the tools we've seen that let us write kernel code to execute instructions on the GPU:</p>

        <ol>
            <li>Pytorch: easy but slow</li>
            <li>torch.compile: easy, fast, but not flexible</li>
            <li>triton: harder, faster, and more flexible</li>
            <li>CUDA: hardest, fastest, and flexiblest (if you get it right)</li>

        </ol>

        <p>Let’s talk about one of the most frequent technique we can use in CUDA: optimizing memory access. The global memory in GPUs (the largest memory in our above graph) has a long latency and low bandwidth in comparison to the cache which often creates a major bottleneck for most applications. Efficiently accessing data from global memory can improve a lot the performance.</p>

        <h4>Memory Coalescing</h4>
        
        <p>To effectively utilize the bandwidth of global memory, it is essential to understand its architecture. In CUDA devices, global memory is implemented using DRAM.</p>

        <p>Memory coalescing takes advantage of how DRAM delivers data in bursts, or ranges of consecutive memory locations, whenever a memory address is accessed. Each time a DRAM location is accessed, a sequence of consecutive locations, including the requested one, is read in parallel by multiple sensors in the DRAM chip. Once read, this data can then be quickly transferred to the processor as a burst. In CUDA, coalescing uses this burst behavior to maximize memory access efficiency by ensuring that threads in a warp—32 threads that execute the same instruction in lockstep (SIMD)—access consecutive memory locations. For instance, if thread 0 accesses location M, thread 1 accesses M + 1, thread 2 accesses M + 2, and so forth, the GPU hardware coalesces or combines these requests into one large, efficient access request for the DRAM burst, rather than handling each access individually. </p>
        
        <p>Let’s take the example of matrix multiplication. A simple, straightforward implementation would have each thread compute a single element of the output matrix, like this:</p>

        <d-code block language="clike">
            __global__ void matmul_naive(int M, int N, int K, const float *A, const float *B, float *C) {
                const uint x = blockIdx.x * blockDim.x + threadIdx.x;
                const uint y = blockIdx.y * blockDim.y + threadIdx.y;
            
                if (x < M && y < N) {
                    float tmp = 0.0;
                    for (int i = 0; i < K; ++i) {
                        tmp += A[x * K + i] * B[i * N + y];
                    }
                    C[x * N + y] = tmp;
                }
            }
        </d-code>

        <p>Here’s an excellent visualization of the kernel from this <a href="https://siboehm.com/articles/22/CUDA-MMM">fantastic blogpost</a>: </p> 
        
        <p><img alt="image.png" src="/assets/images/memorycoalescing.png" /></p>

        <p>However, when profiling this kernel with a tool like <code>ncu</code>, we can see issues, including low memory throughput and uncoalesced memory accesses.</p>

        <div class="large-image-background">
            <img width="1200px" alt="image.png" src="/assets/images/memorycoalescing2.png" />
        </div>
        <div class="large-image-background">
            <img width="1200px" alt="image.png" src="/assets/images/memorycoalescing3.png" />
        </div>


        <p>The reason for this is that in this kernel, two threads in the same block with Thread IDs <code>(0, 0)</code> and <code>(1, 0)</code> (which will end up in the same warp) will both load from the same column of matrix <code>B</code> but different rows of matrix <code>A</code>. Since matrix elements are stored in row-major order (meaning each row's elements are in consecutive memory addresses, as shown in the figure below), in the first iteration with <code>i = 0</code>, thread <code>(0, 0)</code> will load <d-math>A_{0,0}</d-math>, and thread <code>(1, 0)</code> will load <d-math>A_{1,0}</d-math>. These elements are not stored close to each other in memory, and this misalignment repeats across all iterations along the shared dimension, preventing memory accesses from being coalesced.</p>
        
        <p><img alt="image.png" src="/assets/images/memorycoalescing4.png" /></p>

        
        <p>To improve our kernel we can change the way the coordinates x and y are calculated like the following : </p>
        
        <d-code block language="clike">
            const int x = blockIdx.x * BLOCKSIZE + (threadIdx.x / BLOCKSIZE);
            const int y = blockIdx.y * BLOCKSIZE + (threadIdx.x % BLOCKSIZE);

            if (x < M && y < N) {
            float tmp = 0.0;
            for (int i = 0; i < K; ++i) {
                tmp += A[x * K + i] * B[i * N + y];
            }
            C[x * N + y] = tmp;
            }
        </d-code>
        
        <p>Instead of using a 2D block, we switch to a 1D block and redefine how we determine the values of <code>x</code> and <code>y</code>. In this new method, threads within the same warp (which have close <code>threadIdx.x</code> values) will share the same <code>x</code> value but have different <code>y</code> values. This means that they will load the same row of matrix <code>A</code> but different columns of matrix <code>B</code>. As a result, memory accesses can be coalesced for a row-major matrix.</p>

        <p>When we profile our new kernel, we notice that the warning about uncoalesced memory accesses has disappeared, and <strong>the GPU's memory throughput has increased by approximately 10 times</strong>.</p>
        
        <p><img alt="image.png" src="/assets/images/memorycoalescing5.png" /></p>


        <p>We also notice that the execution time of the kernel <strong>decreases by 10x</strong>!</p>
        <p>Let’s cover another technique you will often see mentioned in the litterature: tiling.</p>
        

        <h4>Tiling</h4>

        
        <p>Tiling is a technique that leverages <em>shared memory</em> to optimize memory access patterns. As we mentioned above, the shared memory is a small, fast memory accessible by all threads within a block. It allows data to be reused by multiple threads, reducing the need to repeatedly load data from slower global memory.</p>

        <p>In matrix multiplication for example, each thread in a block may need elements from two matrices, say A and B. If each thread independently loads the row and column it needs from global memory, we end up with many redundant loads, as multiple threads in a block will access overlapping data. Instead, we can use tiling to load a block (or tile) of A and B into shared memory just once, allowing all threads in that block to reuse the same shared data.</p>
        
        <p>In the tiling approach, each iteration involves all threads within a block cooperatively loading two tiles—one from matrix A and another from matrix B —into shared memory. Specifically, threads load a tile of matrix A (of size <code>BLOCK_SIZE_M</code> by <code>BLOCK_SIZE_K</code>) and a tile of matrix B (of size <code>BLOCK_SIZE_K</code> by <code>BLOCK_SIZE_N</code>). Once the tiles are in shared memory, the threads perform matrix multiplication on these tiles, enabling efficient computation since all necessary data is quickly accessible. The results of the tile multiplication are stored in an accumulation matrix that holds intermediate results. After each iteration, the results from the current tile multiplication are added to this accumulation matrix, continuing until all tiles from both matrices have been processed.</p>
        
        <p><img alt="image.png" src="/assets/images/tiling.png" /></p>
        <p>From https://cnugteren.github.io/tutorial/pages/page4.html</p>

        <p>The important parts to understand the implementation are below (for simplicity we consider a square shaped tile) : </p>
        
        <d-code block language="clike">
            // Set pointers to the starting elements
            A += blockRow * TILE_SIZE * K; // Start at row = blockRow, column = 0
            B += blockCol * TILE_SIZE; // Start at row = 0, column = blockCol
            C += blockRow * TILE_SIZE * N + blockCol * TILE_SIZE; // Start at row = blockRow, column = blockCol
            float sum = 0.0;
            // The outer loop moves through tiles of A (across columns) and B (down rows)
            for (int tileIdx = 0; tileIdx < K; tileIdx += TILE_SIZE) {
            sharedA[localRow * TILE_SIZE + localCol] = A[localRow * K + localCol];
            sharedB[localRow * TILE_SIZE + localCol] = B[localRow * N + localCol];

            // Ensure all threads in the block have completed data loading
            __syncthreads();

            // Shift pointers to the next tile
            A += TILE_SIZE;
            B += TILE_SIZE * N;

            // Compute the partial dot product for this tile
            for (int i = 0; i < TILE_SIZE; ++i) {
                sum += sharedA[localRow * TILE_SIZE + i] * sharedB[i * TILE_SIZE + localCol];
            }
            // Synchronize again to prevent any thread from loading new data
            // into shared memory before others have completed their calculations
            __syncthreads();
            }
            C[localRow * N + localCol] = sum;
        </d-code>

        <p>Each thread begins by loading one element from both <strong>Matrix A</strong> and <strong>Matrix B</strong> into shared memory. In this scenario, achieving coalesced memory access is straightforward, by assigning <code>threadIdx.x</code> as the <strong>local column index (localCol)</strong>, threads within the same warp will access adjacent elements of both matrices. After each thread in the block completes loading its elements into shared memory (ensured by calling <code>__syncthreads()</code>), they proceed to compute the dot product of the two tiles. Once the threads have iterated through all the tiles—horizontally for <strong>Matrix A</strong> and vertically for <strong>Matrix B</strong>—the resulting sum is stored in the corresponding location of <strong>Matrix C</strong>.</p>
        
        <p>When benchmarking this kernel using ncu, we noticed that the memory throughput increased to 410 Gb / s, and the kernel execution time decreased by ~43% achieving a ~6.6 TFLOPs performance</p>



        <h4>Thread Coarsening</h4>
        

        <p>The tiling technique has significantly improved the performance of our kernel. However, when analyzing the warp states which quantify how many cycles were spent in each state, we observe the following:</p>

        <p><img alt="image.png" src="/assets/images/threadcoarsening.png" /></p>


        <p>The meaning of the states can be found in the <a href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference">Profiling Guide</a>, specifically in the <strong>Warp Stall Reasons</strong> section. There we can read that:</p>

        <p><em><code>"smsp__pcsamp_warps_issue_stalled_mio_throttle</code>: Warp was stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure."</em></p>
        
        <p>So it seems warps are stalling waiting for shared memory accesses to return ! To resolve this issue we can apply the <strong>Thread Coarsening</strong> technique by merging several threads into a single coarsened thread, we can significantly reduce shared memory accesses because each coarsened thread can handle multiple output elements which would increase the arithmetic intensity of the kernel.</p>

        <h4>Minimizing Control Divergence</h4>
        
        <p>A Streaming Multiprocessor (SM) is built to execute all threads in a warp using the Single Instruction, Multiple Data (SIMD) model. This means that at any given moment, one instruction is fetched and executed simultaneously for all threads within the warp. When a warp is executed, the threads within it operate on different segments of the data but follow the same instruction, hence the name Single Instruction, Multiple Data. The primary advantage of SIMD is its efficiency; the control hardware responsible for instruction fetching and dispatching is shared among multiple execution units. This design minimizes the hardware overhead associated with control functions, allowing a greater portion of the hardware to focus on improving arithmetic throughput.</p>

        <p>Control divergence occurs when threads within the same warp take different execution paths. For instance, if a conditional statement (like an <code>if</code> statement) leads to some threads executing one block of code while others execute a different block, the warp must serialize these executions, resulting in idle threads waiting for others to complete.  To minimize control divergence, we need to design kernels to ensure that threads within the same warp follow the same execution path. This can be achieved by restructuring code to reduce branching, using data structures that ensure all threads follow similar execution paths, or employing techniques such as predication.</p>

        <p>We have covered some of the main considerations when writing custom kernels and improving the performance and memory footprint of GPU operations. But there’s one more important concept before moving to a real example which is “fusing kernels”.</p>

        <h3>Fused Kernels</h3>
        
        <p>In several places now we’ve mentioned how GPU and CPU operation can be asynchronous. In particular, the host code on the CPU can schedule workload on the GPU in a non-blocking way.</p>
        
        <p>Non-blocking can be useful for overlapping communication and computation as we saw at several part along this blog post but can be extended to the more general idea of trying to avoid at all cost going back and forth between host and GPU kernel commands. This is beautifully illustrated by <a href="https://horace.io/brrr_intro.html">Horace He</a> in these diagrams:</p>
        <div style="display: flex; gap: 20px; align-items: flex-start;">
            <div style="width: 50%;">
                <img alt="image.png" src="/assets/images/fused_kernels1.png" style="width: 100%;" />
                <p>A sequence of kernels requiring back and forth between global memory and compute units</p>
            </div>
            <div style="width: 50%;">
                <img alt="image.png" src="/assets/images/fused_kernels2.png" style="width: 100%;" />
                <p>Instead of sending our triangle back to global memory just to read it back again, we instead just do all of our operations in one go.</p>
            </div>
        </div>

        <p>How can we avoid this back and forth? Well the best way is to make our GPU as autonomous as possible. This is achieved by packing as many successive compute operations together in a single kernel for the GPU to run, called a “Fused Kernel”.</p>


        <p>Fused kernel are especially efficient and simple to write for succession of point-like operations which are performed independently of each other on each input tokens. In this case, there is no point in bringing back computed values in Global Memory before moving them to SM memory and spinning up a new kernel. It’s much more efficient to keep all values local until the succession of computation has been performed.</p>

        <p>What are many places in a Transformer model were this can be advantageous, for instance when. a succession of point-wise operations is performed, e.g. in the computation involved in the Layer norms.</p>

        <p>We now have all the understanding necessary to marvel at a true masterpiece of kernel engineering: <strong><em>Flash Attention</em></strong></p>

        <h3>Flash Attention 1-3</h3>
        
        <p>Flash attention is a technique pioneered by <a href="https://tridao.me">Tri Dao</a> that optimizes the attention computations by writing custom CUDA kernels to make it much faster *and* more memory efficient. The idea behind Flash Attention is to make efficient use of the various memories of the GPU to avoid using too much the slowest global memory of the GPU (confusingly called the High Bandwidth Memory, HBM 🫠)</p>

        <p>A basic implementation of the attention mechanism involve a lot of transfer between memory and workers. It requires materializing the S and P matrices in HBM which means that the results need to be sent to HBM and then back to SRAM for the next computations:</p>

        <p style="text-align: center"><img alt="image.png" src="/assets/images/flashattn.png" style="width: 500px" /></p>
        
        <p>Since bandwidth is much lower in HBM this introduces a severe bottleneck in the attention computation. Can we do better? Tri Dao says yes!</p>

        <p>The key element is to compute the S matrices in small pieces which can fit in the smaller shared memory of the SM. But we can do even better and avoid materializing the very large S matrix all together in favor of keeping only the necessary statistics for computing the normalization factor of the softmax. So we can compute part of <d-math>O</d-math> directly in one computation in SRAM rather than moving intermediate results back and forth. In this case, not even do we make use of the shared memory but we also release the memory bottleneck resulting from materializing one of the largest activation matrices in the model (at long context length), the attention matrix.</p>

        <p><img alt="image.png" src="/assets/images/flashattn2.png" /></p>
        <div class="figure-legend"><p>Source: FlashAttention paper<d-cite bibtex-key="dao2022flashattention"></d-cite></p></div>

        <p>The idea of flash attention resolves so many bottlenecks in model training that it has quickly become the default way to perform attention in all transformers:</p>
        <ul>
            <li>By avoiding to materialize the S matrix we <strong>reduce the memory burden of attention</strong></li>
            <li>We also remove a large part of the <strong>naive impact of the S^2 cost of attention</strong></li>
        </ul>

        <p>As a result as well, all variants of linear attention and sub-quadratic approaches to approximate attention –developed shortly after the invention of the transformers architecture– have been mostly put aside in favor of this exact and fast flash attention implementation and mechanism.</p>

        <p>Following Flash-attention 1, two successive improved versions have been released by the same lab: Flash-attention 2 and 3. In comparison to Flash-attention 1, the improvements in Flash-attention 2 and 3 are less about the general attention mechanism than about tailoring its low level implementation more specifically to the GPU by (1) reducing the number of non-matmul operations as much as possible (2) partitioning carefully the workload among wraps and thread blocks (for Flash Attention 2) and carefully optimizing for FP8 and Tensor Core support on the latest Hopper (H100) architecture for Flash Attention 3.</p>

        <aside>Flash attention puts some restrictions on which attention patterns can be sped up. Check out <a href="https://pytorch.org/blog/flexattention/">FlexAttention</a> which is a fast <em>and</em> flexible variant.</aside>

        <p>Flash-Attention is a master demonstration of the breakthrough improvements that can come when you take into account the internal memory/compute design of current GPU accelerators.</p>

        <p>The techniques described so far in this section require specific modeling code changes and writing custom kernels for certain operations in order to speed up training. In this section we take a look at a range of methods that are agnostic to the modeling code and can be used for any model!</p>
        
        <h3>Mixed Precision Training</h3>
        
        <p>Mixed Precision Training, as the name suggests, involves mixing different precisions when training. The default numerical precision of PyTorch tensors is single-precision floating point format or also called FP32 or float32 which means that every number stored takes up 32 bits or 4 bytes. The available bits to represent a number are divided into 3 parts:</p>

        <ul>
            <li>Sign: the first bit determines if the number is positive or negative</li>
            <li>Mantissa: determines the significant figures of a number</li>
            <li>Exponent: controls the magnitude of the number</li>
        </ul>

        <p>The principle of floating point numbers can be easily illustrated by recalling the scientific notation of numbers, e.g. <d-math>- 5.734 \times 10^{7}</d-math>, where we first have the sign, followed by the mantissa an the exponent. As such we can represent numbers across a wide range of magnitudes with an adaptive precision. Although float32 is the default there is a range of floating point formats available in PyTorch:</p>

        <p></p>

        <table>
            <thead>
              <tr>
                <th><strong>Format</strong></th>
                <th><strong>Total bits</strong></th>
                <th><strong>Sign</strong></th>
                <th><strong>Mantissa</strong></th>
                <th><strong>Exponent</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>float32</td>
                <td>32</td>
                <td>1</td>
                <td>23</td>
                <td>8</td>
              </tr>
              <tr>
                <td>float16</td>
                <td>16</td>
                <td>1</td>
                <td>10</td>
                <td>5</td>
              </tr>
              <tr>
                <td>bfloat16</td>
                <td>16</td>
                <td>1</td>
                <td>7</td>
                <td>8</td>
              </tr>
              <tr>
                <td>float8 (e4m3)</td>
                <td>8</td>
                <td>1</td>
                <td>3</td>
                <td>4</td>
              </tr>
              <tr>
                <td>float8 (e5m2)</td>
                <td>8</td>
                <td>1</td>
                <td>2</td>
                <td>5</td>
              </tr>
            </tbody>
           </table>

        <aside>Note: You might be wondering where the “b” in bfloat16 comes from. The format was developed at Google Brain and thus the “b” stands for “brain”. </aside>

        <p>Reducing the total number of bits comes at a price (no free lunch here either), but we have some control over how to pay. Either we can sacrifice more bits on the mantissa or exponent. For this reason there exist also two float8 formats, named according to exponent and mantissa, to flexibly choose the most appropriate format. We can look at the possible range of numbers for each format:</p>

        <p><img alt="image.png" src="/assets/images/mixedprecision.png" /></p>


        <p>We can see that float32 spans 80 orders of magnitude and float16 sacrifices a lot of range while bfloat16 maintains the full range. The two float8 formats reduce the range even further where e5e2 can maintain float16 range and e4m3 has an even smaller ranger.</p>

        <p>How come some format are able to maintain the range and other not? Let’s investigate the resolution by plotting 10,000 points between 1 and 2. Each point will be rounded to the nearest representable number in each format:</p>

        <p><img alt="image.png" src="/assets/images/mixedprecision_2.png" /></p>

        <p>We can see here that bfloat16 maintained the range of float32 over float16 but did this with the cost of sacrificing more precision. In case of float8 the situation is even more dire as e4m3 can represent 7 and e5m2 only 3 number on the interval 1-2.</p>

        <p>A common metric to measure a formats resolution is epsilon: the first representable number after 1.00. We can see that for the float32 format $10^{-4}$  is an upper bound (it’s actually <d-math>1.19^{-7}</d-math>). For float16 it is <d-math>\tilde 10^{-3}</d-math> and for bfloat 10x higher still.</p>

        <p>The idea of mixed precision training is to use some of these lower precisions formats while maintaining the performance of full precision training. It turns out we <strong>can’t</strong> totally abandon float32 and usually will need to maintain some parts in full precision.</p>

        <p>This is why lower precision training is usually called <strong><em>mixed precision</em></strong> training. </p>

        <p>Let’s now take a look at training models with 16 bits and then see if we can take it a step further all the way down to 8 bits.</p>

        
        
        <h4>FP16 and BF16 training</h4>

        <p>Naively switching all the tensors and operations to float16 unfortunately doesn’t work and the result is usually diverging losses. However, the original mixed precision training paper<d-cite bibtex-key="micikevicius2018mixedprecisiontraining"></d-cite> came up with three tricks to match float32 trainings:</p>

        <ol>
            <li><strong>FP32 copy of weights</strong>: There are two possible issues with float16 weights. During training some of the weights can become very small and will be rounded to 0. However, even if the weights themselves are not close to zero, if the updates are very small the difference in magnitude can cause the weights to underflow during the addition. Once the weights are zero they will remain 0 for the rest of training as there is no gradient signal coming through anymore.</li>
            <li><strong>Loss scaling</strong>: We have a similar issue with the gradients as well as gradients tend to be much smaller than 1 and are thus at risk to underflow. A simple, yet effective, strategy is to scale the loss before the backward pass and unscale the gradients after the backward pass. This ensures that there is no underflow during the backward pass and the scaling is not affecting training as we unscale before processing the gradients further (e.g. clipping) and the optimization step.  </li>
            <li><strong>Accumulation</strong>: Finally, when performing arithmetic operations in float16 such as in dot products, we can also face under or overflows. Does targeting certain types of arithmetic operations to accumulate the intermediate results in float32 during the operation and then casting the accumulated result back to fp16. For the same reason gradients are also accumulated in float32.</li>
        </ol>
        
        <p>With these techniques, you get consistently stable training while benefitting from higher throughput due to the faster, lower precision operations. Naturally, as the curious reader you are and by now slightly addicted to maximizing the throughput, you ask the question: can we go further and faster? </p>

        <p>Maybe!</p>
        
        <h4>FP8 pretraining</h4>
        
        <p>Even if we perfectly overlap communication with computation, we always eventually run into the low level theoretical FLOPS limit of the hardware itself, i.e. the efficiency of each individual operation on our hardware. This is where numerical precision becomes crucial. For instance, on NVIDIA's H100 GPU, FP8 matrix multiplications (GEMM operations) achieve twice the theoretical FLOPS of bfloat16, making lower-precision training an attractive path for further optimization.</p>
        
        <p>Recent research - including  FP8-LM<d-cite bibtex-key="peng2023fp8lmtrainingfp8large"></d-cite>, torchao<d-cite bibtex-key="torchao"></d-cite>, and DeepSeek-V3<d-cite bibtex-key="deepseekai2024deepseekv3technicalreport"></d-cite> - has demonstrated the potential of FP8 training for large-scale models. Still, FP8 pretraining introduces a significant challenge: stability. At lower precision, numerical instability often leads to loss divergence, making it difficult to match the accuracy of higher-precision training.</p>
        
        <p>We know that instability increases as learning rates rise for a fixed model size<d-cite bibtex-key="wortsman2023smallscaleproxieslargescaletransformer"></d-cite>, making FP8 pretraining particularly tricky.</p>
        
        <iframe class="l-body-outset" id="plotFP8Loss" src="/assets/data/fp8/fp8_training_loss_curves.html" height="520" width="1000" scrolling="no" frameborder="0"></iframe>
        <!-- Hynek uncomment this once it's added to -->
        <!-- <div class="l-body-outset" id="fragment-fp8_training_loss_curves"></div> -->

        <p>The first, successful, very large scale training with FP8 mixed precision was publicly reported on DeepSeek-V3. The authors carefully analyzed each operation of the forward pass (Fprop) as well as the activation (Dgrad) and weight (Wgrad) backward pass. Similar to BF16 mixed precision training, some aggregation and master weights are kept in higher precision while the operations themselves are performed in FP8.  </p>
        
        <p><img alt="image.png" src="/assets/images/fp8_diagram.png" /></p>

        <p>In order to switch from high precision (e.g. FP32 or BF16) to lower precision (e.g. FP16 or FP8) with smaller range, we need to normalize the range of values by computing the absolute maximum. DeepSeek-V3 also introduces a quantization scheme, where the ranges are normalized per tile: 1x128 for inputs/activations and 128x128 for weights and scale elements. This makes the normalization less susceptible to outliers. There is a number of additional tricks they deploy to also reduce the memory and communication footprint which you can follow in section 3.3. of the DeepSeek-V3 technical report<d-cite bibtex-key="deepseekai2024deepseekv3technicalreport"></d-cite>. </p>

        <p>Here’s a summary of a few known approaches to FP8 training:</p>

        <table>
            <thead>
              <tr>
                <th></th>
                <th>GEMM's precision</th>
                <th>Master model weights</th>
                <th>Accumulated gradients</th>
                <th>Model weights</th>
                <th>Gradients</th>
                <th>Optimizer States</th>
                <th>Total Memory</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>bfloat16 with fp32 mixed precision baseline</td>
                <td>bf16</td>
                <td>fp32</td>
                <td>fp32</td>
                <td>bf16</td>
                <td>bf16</td>
                <td>fp32 + fp32</td>
                <td>4 + 4 + 2 + 2 + 4 + 4 = 20 bytes</td>
              </tr>
              <tr>
                <td>Above without FP32 grad accumulation</td>
                <td>bf16</td>
                <td>fp32</td>
                <td>n/a</td>
                <td>bf16</td>
                <td>bf16</td>
                <td>fp32 + fp32</td>
                <td>4 + 2 + 2 + 4 + 4 = 16 bytes</td>
              </tr>
              <tr>
                <td>Transformer Engine</td>
                <td>fp8</td>
                <td>n/a</td>
                <td>n/a</td>
                <td>fp32</td>
                <td>fp32</td>
                <td>fp32 + fp32</td>
                <td>4 + 4 + 4 + 4 = 16 bytes (20% reduction)</td>
              </tr>
              <tr>
                <td>FP8-LM's O3 level</td>
                <td>fp8</td>
                <td>fp16</td>
                <td>fp16</td>
                <td>fp8</td>
                <td>fp8</td>
                <td>fp8 + fp16</td>
                <td>2 + 2 + 1 + 1 + 1 + 2 = 9 bytes (55%)</td>
              </tr>
              <tr>
                <td>DeepSeek-V3</td>
                <td>fp8</td>
                <td>fp32</td>
                <td>fp32</td>
                <td>fp8</td>
                <td>bf16</td>
                <td>bf16 + bf16</td>
                <td>4+4+1+2+2+2 = 15 (25%)</td>
              </tr>
              <tr>
                <td>nanotron's FP8</td>
                <td>fp8</td>
                <td>bf16</td>
                <td>fp32</td>
                <td>fp8</td>
                <td>fp8</td>
                <td>fp8 + fp8</td>
                <td>2 + 4 + 1 + 1 + 1 + 1 = 10 bytes (50%)</td>
              </tr>
            </tbody>
           </table>

        <p>Overall, FP8 is still an experimental technique and methods are evolving, but will likely become the standard soon replacing bf16 mixed-precision. To follow a public implementations of this, please head to the nanotron’s implementation in <a href="https://github.com/huggingface/nanotron/pull/70">this PR</a>. </p>

        <p>In the future, Blackwell, the next generation of NVIDIA chips, <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/">have been announced </a> to support FP4 training, further speeding up training but without a doubt also introducing a new training stability challenge.</p>

        <p>We now arrived at the end of the distributed training journey. Let’s take a step back and conclude.</p>

        <h2>Conclusion</h2>
        
        
        <p>Congratulations, dear reader, you made it to the end! We've completed quite a journey: we started from understanding how to train a simple model on a single GPU, all the way to mastering all the intricate techniques used to efficiently train massive language models like Llama-405B and DeepSeek-V3 on thousands of GPUs. By now, you can read a diagram, like Llama-3's 4D parallel setup, with ease:</p>
        
        <p><img alt="image.png" src="/assets/images/conclusion_llama3_parallelism.png" /></p>

        <p>Orchestrating large clusters of GPUs to train LLMs efficiently is no easy feat. We learned how to optimize computations and communications between GPUs such that they run with maximum utilization at all times. It involves choosing the right parallelization strategy for a given model and cluster size, overlapping communication and computation where possible, and writing custom kernels that take into account the hardware layout to perform an operation as fast as possible on the GPU.</p>

        <p>You might still believe that this knowledge is a bit niche and only concerns the small set of people that pretrain LLMs. Historically, that mayb be true, but as models are growing rapidly even people who want to fine-tune models require distributd training setups. So diving deeper into all things distributed might prove very timely.</p>

        <p>This has been a long learning journey, but not just for you! Running thousands of benchmarks on a GPU cluster was more challenging than we anticipated and we want to share a few highlights of our learning experience.</p>

        <h3>What we learned</h3>

        <p>Our goal for this blogpost was not only to discuss theory and implementations but provide actual data points as well. So the plan was simple: lets run every possible distributed configuration for every model and a number of cluster sizes (namely 1-64 nodes of 8xH100s). Even after excluding impossible configuration we still needed to run thousands of experiments. </p>

        <aside>We want to take this opportunity to apologize to our co-workers for blocking most of the science cluster and in turn forgive any threats that may have been whispered.</aside>

        <p>   
        On paper this sounds easy enough: we can easily launch big arrays of jobs on our cluster. However, when we launched the first batches is when the troubles began:
        </p>

        <ul>
            <li>PyTorch processes would sometimes fail to clean up properly</li>
            <li>Slurm job manager would forcefully terminate jobs, leading to node failures </li>
            <li>Simple benchmarks that should take minutes would stretch into hours</li>
            <li>Some jobs would hang indefinitely</li>
        </ul>

        <p>So in order to run all experiments in a finite amount of time required some additional engineering. In particular we spent a significant amount of time on the following:</p>

        <ul>
            <li>Minimizing cluster restart times and optimize idle time</li>
            <li>Analyzing detailed NCCL debug logs</li>
            <li>Understand memory usage patterns and CUDA memory allocator behaviors</li>
            <li>Improving pipeline parallelism performance on multi-node</li>
        </ul>
        
        <p>These challenges deserve their own story, but they taught us valuable lessons about the complexities of distributed training infrastructure. What looks simple in theory often requires careful attention to many moving parts in practice.</p>

        <!--
        <p>Let's analyze the results of our benchmarks and understand how different configurations affect each other. All benchmarks were run with a sequence length of 4096 and a global batch size of 1M tokens. We'll look at two key visualizations that help illustrate our findings.
        </p>
        
        <p>First, let's examine this heatmap visualization:</p>

        <p><img alt="image.png" src="/assets/images/what_we_learnt_heatmap.svg" /></p>
        <p>Heatmap visualization showing the optimal training configurations across different model sizes and compute node counts. For each combination, the configuration details include Data Parallelism (DP), Tensor Parallelism (TP), Pipeline Parallelism (PP), Gradient Accumulation Steps (GAS), Micro Batch Size (MBS), and ZeRO optimization stage. The color intensity indicates the Model FLOPs Utilization (MFU), with brighter colors representing higher efficiency.</p>

        <p>To complement this, let's look at the relationships between different parameters:</p>
        
        <iframe id="plotFrame" src="/assets/images/what_we_learnt_parallel_coordinates.html" height="540" width="1000" scrolling="no" frameborder="0"></iframe>

        <p>Parallel coordinates plot showing the relationship between different model parallelism configurations (Data Parallel degree, Tensor Parallel degree, Pipeline Parallel degree), training hyperparameters (gradient accumulation steps, micro batch size), ZeRO stage and the resulting Model FLOPs Utilization (MFU). Each line represents a different training configuration, with colors indicating the MFU value - warmer colors show higher efficiency.</p>

        <p>From these visualizations, we can draw several important insights:
        </p>

        <ol>
            <li>As we increase the number of nodes (higher parallelism), we observe a decrease in efficiency. This effect is particularly pronounced for smaller models, which have a lower compute-to-model-size ratio. While we might typically compensate for small model size by increasing the batch size, we're constrained by our global batch size limit of 1M.
            </li>
            <li>Larger models present a different challenge. As model size increases, memory requirements grow substantially. This creates two scenarios with fewer nodes: either the model doesn't fit at all, or it barely fits but runs inefficiently due to operating near the GPU memory limits.</li>
            <li>Our benchmarks demonstrate how performance heavily depends on implementation quality. When we first implemented both parallelism strategies, Tensor Parallelism (TP) outperformed Pipeline Parallelism (PP). After optimizing our PP code, it became the faster option. Now that we're improving the communication overlap in our TP implementation, we expect it to regain the performance lead.</li>
        </ol>
        -->
        <p>Reproducing theoretical results in practice is challenging, especially given the limited availability of production training code. Through open-source projects like picotron and nanotron, we hope to make these distributed training techniques more accessible and foster collaboration on simpler, more efficient codebases that help researchers and practitioners make the most of their hardware resources.</p>

        <h3>So, what’s next?</h3>

        <p>You now have good overview of the main distributed training concepts but at the same time we just scratched to surface of on some aspects. There are many ways to dive deep into a subject but here are some steps that we recommend:</p>
        <ul>
            <li>Carefully read some of the landmark or very recent papers. You can find a list of some of the most impactful papers in <a target="_self" href="#references" class="">References</a>.</li>
            <li>Start from scratch and implement an algorithm yourself. Often a method only fully “clicks” if you implemented it yourself.</li>
            <li>Dive into one of the widely used frameworks and start contributing: fix bugs, answer issues, or implement a new feature. That’s the best way to get in any ML field!</li>
        </ul>

        <p>We hope this book helps you get started in distributed training and that you will train the next generation of awesome models to the hum of your GPU cluster!</p>
        
        <h3>Acknowledgements</h3>

        <p>We thank <a href="https://huggingface.co/eliebak">Elie</a> for conducting thorough reviews and creating the audio components using NotebookLM. Special thanks to <a href="https://huggingface.co/hynky">Hynek</a> for optimizing the frontend performance. We also thank <a href="https://huggingface.co/sbrandeis">Simon</a> for resolving some issues on the hub.</p>


        <h2>References</h2>
        
        <h3>Landmark LLM Scaling Papers</h3>

        <div>
            <a href="https://arxiv.org/abs/1909.08053"><strong>Megatron-LM</strong></a>
            <p>Introduces tensor parallelism and efficient model parallelism techniques for training large language models.</p>
        </div>

        <div>
            <a href="https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/"><strong>Megatron-Turing NLG 530B</strong></a>
            <p>Describes the training of a 530B parameter model using a combination of DeepSpeed and Megatron-LM frameworks.</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/2204.02311"><strong>PaLM</strong></a>
            <p>Introduces Google's Pathways Language Model, demonstrating strong performance across hundreds of language tasks and reasoning capabilities.</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/2312.11805"><strong>Gemini</strong></a>
            <p>Presents Google's multimodal model architecture capable of processing text, images, audio, and video inputs.</p>
        </div>
              
        <div>
            <a href="https://arxiv.org/abs/2407.21783"><strong>Llama 3</strong></a>
            <p>The Llama 3 Herd of Models</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/2412.19437v1"><strong>DeepSeek-V3</strong></a>
            <p>DeepSeek's report on architecture and training of the DeepSeek-V3 model.</p>
        </div>


        <h3>Training Frameworks</h3>
        <div>
            <a href="https://github.com/facebookresearch/fairscale/tree/main"><strong>FairScale</strong></a>
            <p>PyTorch extension library for large-scale training, offering various parallelism and optimization techniques.</p>
        </div>

        <div>
            <a href="https://github.com/NVIDIA/Megatron-LM"><strong>Megatron-LM</strong></a>
            <p>NVIDIA's framework for training large language models with model and data parallelism.</p>
        </div>

        <div>
            <a href="https://www.deepspeed.ai/"><strong>DeepSpeed</strong></a>
            <p>Microsoft's deep learning optimization library featuring ZeRO optimization stages and various parallelism techniques.</p>
        </div>

        <div>
            <a href="https://colossalai.org/"><strong>ColossalAI</strong></a>
            <p>Integrated large-scale model training system with various optimization techniques.</p>
        </div>

        <div>
            <a href="https://github.com/pytorch/torchtitan"><strong>torchtitan</strong></a>
            <p>A PyTorch native library for large model training.</p>
        </div>

        <div>
            <a href="https://github.com/EleutherAI/gpt-neox"><strong>GPT-NeoX</strong></a>
            <p>EleutherAI's framework for training large language models, used to train GPT-NeoX-20B.</p>
        </div>

        <div>
            <a href="https://github.com/Lightning-AI/litgpt"><strong>LitGPT</strong></a>
            <p>Lightning AI's implementation of state-of-the-art open-source LLMs with focus on reproducibility.</p>
        </div>

        <div>
            <a href="https://github.com/PrimeIntellect-ai/OpenDiLoCo"><strong>DiLoco</strong></a>
            <p>Training language models across compute clusters with DiLoCo.</p>
        </div>

        <div>
            <a href="https://github.com/kakaobrain/torchgpipe"><strong>torchgpipe</strong></a>
            <p>A GPipe implementation in PyTorch.</p>
        </div>
        
        <div>
            <a href="https://github.com/EleutherAI/oslo"><strong>OSLO</strong></a>
            <p>OSLO: Open Source for Large-scale Optimization.</p>
        </div>

        <h3>Debugging</h3>

        <div>
            <a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html"><strong>Speed profiling</strong></a>
            <p>Official PyTorch tutorial on using the profiler to analyze model performance and bottlenecks.</p>
        </div>

        <div>
            <a href="https://pytorch.org/blog/understanding-gpu-memory-1/"><strong>Memory profiling</strong></a>
            <p>Comprehensive guide to understanding and optimizing GPU memory usage in PyTorch.</p>
        </div>

        <div>
            <a href="https://huggingface.co/blog/train_memory"><strong>Memory profiling walkthrough on a simple example</strong></a>
            <p>Visualize and understand GPU memory in PyTorch.</p>
        </div>

        <div>
            <a href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html"><strong>TensorBoard Profiler Tutorial</strong></a>
            <p>Guide to using TensorBoard's profiling tools for PyTorch models.</p>
        </div>

        <h3>Distribution Techniques</h3>

        <div>
            <a href="https://siboehm.com/articles/22/data-parallel-training"><strong>Data parallelism</strong></a>
            <p>Comprehensive explanation of data parallel training in deep learning.</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/1910.02054"><strong>ZeRO</strong></a>
            <p>Introduces Zero Redundancy Optimizer for training large models with memory optimization.</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/2304.11277"><strong>FSDP</strong></a>
            <p>Fully Sharded Data Parallel training implementation in PyTorch.</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/2205.05198"><strong>Tensor and Sequence Parallelism + Selective Recomputation</strong></a>
            <p>Advanced techniques for efficient large-scale model training combining different parallelism strategies.</p>
        </div>

        <div>
            <a href="https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/#pipeline_parallelism"><strong>Pipeline parallelism</strong></a>
            <p>NVIDIA's guide to implementing pipeline parallelism for large model training.</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/2211.05953"><strong>Breadth first Pipeline Parallelism</strong></a>
            <p>Includes broad discussions around PP schedules.</p>
        </div>

        <div>
            <a href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/"><strong>All-reduce</strong></a>
            <p>Detailed explanation of the ring all-reduce algorithm used in distributed training.</p>
        </div>

        <div>
            <a href="https://github.com/zhuzilin/ring-flash-attention"><strong>Ring-flash-attention</strong></a>
            <p>Implementation of ring attention mechanism combined with flash attention for efficient training.</p>
        </div>

        <div>
            <a href="https://coconut-mode.com/posts/ring-attention/"><strong>Ring attention tutorial</strong></a>
            <p>Tutorial explaining the concepts and implementation of ring attention.</p>
        </div>

        <div>
            <a href="https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/#understanding-performance-tradeoff-between-zero-and-3d-parallelism"><strong>ZeRO and 3D</strong></a>
            <p>DeepSpeed's guide to understanding tradeoffs between ZeRO and 3D parallelism strategies.</p>
        </div>

        <div>
            <a href="https://arxiv.org/abs/1710.03740"><strong>Mixed precision training</strong></a>
            <p>Introduces mixed precision training techniques for deep learning models.</p>
        </div>
              
        <div>
            <a href="https://main-horse.github.io/posts/visualizing-6d/"><strong>@main_horse blog</strong></a>
            <p>Visualizing 6D Mesh Parallelism</p>
        </div>

        <h3>Hardware</h3>

        <div>
            <a href="https://www.arxiv.org/abs/2408.14158"><strong>Fire-Flyer - a 10,000 PCI chips cluster</strong></a>
            <p>DeepSeek's report on designing a cluster with 10k PCI GPUs.</p>
        </div>

        <div>
            <a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/"><strong>Meta's 24k H100 Pods</strong></a>
            <p>Meta's detailed overview of their massive AI infrastructure built with NVIDIA H100 GPUs.</p>
        </div>

        <div>
            <a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network"><strong>Semianalysis - 100k H100 cluster</strong></a>
            <p>Analysis of large-scale H100 GPU clusters and their implications for AI infrastructure.</p>
        </div>
              
        <div>
            <a href="https://modal.com/gpu-glossary/readme"><strong>Modal GPU Glossary </strong></a>
            <p>CUDA docs for human</p>
        </div>

        <h3>Others</h3>

        <div>
            <a href="https://github.com/stas00/ml-engineering"><strong>Stas Bekman's Handbook</strong></a>
            <p>Comprehensive handbook covering various aspects of training LLMs.</p>
        </div>

        <div>
            <a href="https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md"><strong>Bloom training chronicles</strong></a>
            <p>Detailed documentation of the BLOOM model training process and challenges.</p>
        </div>

        <div>
            <a href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf"><strong>OPT logbook</strong></a>
            <p>Meta's detailed logbook documenting the training process of the OPT-175B model.</p>
        </div>

        <div>
            <a href="https://www.harmdevries.com/post/model-size-vs-compute-overhead/"><strong>Harm's law for training smol models longer</strong></a>
            <p>Investigation into the relationship between model size and training overhead.</p>
        </div>

        <div>
            <a href="https://www.harmdevries.com/post/context-length/"><strong>Harm's blog for long context</strong></a>
            <p>Investigation into long context training in terms of data and training cost.</p>
        </div>
              
        <div>
            <a href="https://www.youtube.com/@GPUMODE/videos"><strong>GPU Mode</strong></a>
            <p>A GPU reading group and community.</p>
        </div>
              
        <div>
            <a href="https://youtube.com/playlist?list=PLvtrkEledFjqOLuDB_9FWL3dgivYqc6-3&si=fKWPotx8BflLAUkf"><strong>EleutherAI Youtube channel</strong></a>
            <p>ML Scalability & Performance Reading Group</p>
        </div>
              
        <div>
            <a href="https://jax-ml.github.io/scaling-book/"><strong>Google Jax Scaling book</strong></a>
            <p>How to Scale Your Model</p>
        </div>
              
        <div>
            <a href="https://github.com/facebookresearch/capi/blob/main/fsdp.py"><strong>@fvsmassa & @TimDarcet FSDP</strong></a>
            <p>Standalone ~500 LoC FSDP implementation</p>
        </div>

        <div>
            <a href="https://www.thonking.ai/"><strong>thonking.ai</strong></a>
            <p>Some of Horace He's blogposts</p>
        </div>
              
        <div>
            <a href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad"><strong>Aleksa's ELI5 Flash Attention</strong></a>
            <p>Easy explanation of Flash Attention</p>
        </div>        
        <div>
            <a href="https://github.com/tunib-ai/large-scale-lm-tutorials"><strong>TunibAI's 3D parallelism tutorial</strong></a>
            <p>Large-scale language modeling tutorials with PyTorch.</p>
        </div>
        
        <h2>Appendix</h2>
        
        <h3>A0: Parallel Programming Crash Course</h3>
        

        <p>Throughout the blogpost we  scale LLM training from one to hundreds of GPUs. This will require the communication and synchronization of weights, gradients, and data between all the machines. There’s a set of distributed patterns to achieve exactly that called <strong><em>collective operations</em></strong>. In this section we’ll do a small crash course of all the operations like <em>Broadcast, AllReduce, Scatter</em> and more. Let’s dive in!</p>
        
        <p>The general setup is that we have a number of independent nodes which could be CPU cores, GPUs, or compute nodes. Each performs some computation and then we want to communicate the result or parts of it to the other nodes for the next computation step (t+1).</p>
        
        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_general.png" style="width: 400px" /></p>


        <p>Maybe we need to send the result from one node to all other nodes, or we need to sum all the intermediate results from each node to report the overall result. Usually, there is one node with an elevated status that plays a central role, here denoted with <code>root</code> that is the target or source of some operations. Let’s start with one of the simplest primitives: a broadcast operation.</p>
        
 

        <h4>Broadcast</h4>

        <p>A very common pattern is that you have some data on Node 1 and you want to share it with all the other nodes so they can do some computation with the data. The broadcast operation does just that:</p>
        
        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_broadcast.png" style="width: 400px" /></p>


        <p>Collective operations are natively provided by PyTorch so we can easily write a small example that demonstrates how broadcasting works. We first need to initialize a process group with <code>dist.initi_process_group</code> which sets up the communication backend (we’ll talk about NCCL later), it determines how many workers (aka nodes) exists and assigns a rank to each one (which we can get with <code>dist.get_rank</code>). Finally, it establishes a connection between the workers.</p>

        <p>To showcase the <code>dist.broadcast</code> operation, let's create a tensor with non-zero values on <code>rank=0</code> and tensors full of zeros on the other workers. We then distribute the <code>rank=0</code> tensor to all other ranks with <code>dist.broadcast(tensor, src=0)</code> :</p>
        
        <d-code block language="python">
            import torch
            import torch.distributed as dist

            def init_process():
                dist.init_process_group(backend='nccl')
                torch.cuda.set_device(dist.get_rank())
                
            def example_broadcast():
                if dist.get_rank() == 0:
                    tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32).cuda()
                else:
                    tensor = torch.zeros(5, dtype=torch.float32).cuda()
                print(f"Before broadcast on rank {dist.get_rank()}: {tensor}")
                dist.broadcast(tensor, src=0)
                print(f"After broadcast on rank {dist.get_rank()}: {tensor}")
                
            init_process()
            example_broadcats()
        </d-code>


        <p>You can run the above script with <code>torchrun --nproc_per_node=3 dist_op.py</code> (you’ll need 3 GPUs for this or change <code>nproc_per_node</code> accordingly) and you should see the following output:</p>

        <d-code block language="python">
            Before broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
            Before broadcast on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
            Before broadcast on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

            After broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
            After broadcast on rank 1: tensor([1., 2., 3., 4., 5.], device='cuda:1')
            After broadcast on rank 2: tensor([1., 2., 3., 4., 5.], device='cuda:2')
        </d-code>

        <p>Great, seems like it works as expected. Note that the rank messages can be printed out of order as we have no control over which print statement is executed first (we ordered them here for readability). Now let’s move on to the Reduce and AllReduce patterns! </p>
        


        <h4>Reduce & AllReduce</h4>
        
        <p>Reduce patterns are among the most fundamental patterns in distributed data processing. The idea is that you want to combine the data present on each node through a function <code>f()</code> which can be for instance summation or averaging. In the Reduce paradigm the result is sent to the root node only, whereas in the AllReduce case the result is broadcasted to all nodes:</p>
        
        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_reduce_allreduce.png" style="width: 1000px" /></p>


        <p>Of course no magic “free flying” node that can perform this operation and generally each node does a partial computation in a ring or tree structure of the nodes. Here is a simple example: let’s say we need to compute a sum of numbers on each nodes and our nodes are connected in a ring pattern. The first node sends its number to a neighbour which adds its number to the received number before forwarding it to the next neighbour. At the end of a round along the ring of nodes, the first node will receive the total sum.</p>

        <p>Here’s the code to run a simple Reduce operation summing the tensors, we specify the operation to use with <code>op=dist.ReduceOp.SUM</code> (you can find more information on the supported operations in the <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp">Pytorch docs</a>):</p>
        
        <d-code block language="python">
            def example_reduce():
                tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
                print(f"Before reduce on rank {dist.get_rank()}: {tensor}")
                dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)
                print(f"After reduce on rank {rank}: {tensor}")
    
            init_process()
            example_reduce()
        </d-code>

        <p>Note that in the Reduce operation only the tensor on the <code>dst</code> node is updated:</p>

        <d-code block language="python">
            Before reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
            Before reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
            Before reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

            After reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
            After reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
            After reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
        </d-code>

        <p>Similarly we can perform an AllReduce (we don’t need to specify a destination in this case):</p>

        <d-code block language="python">
            def example_all_reduce():
                tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
                print(f"Before all_reduce on rank {dist.get_rank()}: {tensor}")
                dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
                print(f"After all_reduce on rank {dist.get_rank()}: {tensor}")
                
            init_process()
            example_all_reduce()
        </d-code>

        <p>In this case the result is available on all nodes:</p>
        
        <d-code block language="python">
            Before all_reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
            Before all_reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
            Before all_reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
            
            After all_reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
            After all_reduce on rank 1: tensor([6., 6., 6., 6., 6.], device='cuda:1')
            After all_reduce on rank 2: tensor([6., 6., 6., 6., 6.], device='cuda:2')
        </d-code>

        <p>Now let’s turn to our next distributed communication operation. In many real cases, each node individually perform many complex computations and we need to share the final results among nodes. Gather and AllGather are the operations we want to use in this case. Let’s take a look!</p>

        <h4>Gather & AllGather </h4>
        
        <p>Gather and AllGather are quite similar to the Broadcast in that they allow distributing data among node without modification. The main difference to Broadcast is that there is not one value we need to share from one node to all other nodes but each node has an individual chunk of data that we want to either gather all data on one node (in case of Gather) or gather all data on all nodes (in the case of AllGather). A picture being worth 1000 words, let’s take a look:</p>

        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_gather_allgather.png" style="width: 1000px" /></p>

        <p>Note that the dashed lines indicate that some data actually doesn’t move at all (since it’s already present on the node).</p>

        <p>In the case of the gather operation we need to prepare a container objects where the gathered tensors can be stored in this example the <code>gather_list</code>:</p>

        <d-code block language="python">
            def example_gather():
                tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
                if dist.get_rank() == 0:
                    gather_list = [
                        torch.zeros(5, dtype=torch.float32).cuda()
                        for _ in range(dist.get_world_size())
                        ]
                else:
                    gather_list = None
                print(f"Before gather on rank {dist.get_rank()}: {tensor}")
                dist.gather(tensor, gather_list, dst=0)
                if dist.get_rank() == 0:
                    print(f"After gather on rank 0: {gather_list}")
                
            init_process()
            example_gather()
        </d-code>

        <p>And we see that the `gather_list` indeed contains the tensors of all ranks:</p>

        <d-code block language="python">
            Before gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
            Before gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
            Before gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

            After gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                                     tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                                     tensor([3., 3., 3., 3., 3.], device='cuda:0')]
        </d-code>

        <p>The only thing we need to change for the AllGather example is that every node will need a placeholder for the results:</p>

        <d-code block language="python">
            def example_all_gather():
                tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
                gather_list = [
                    torch.zeros(5, dtype=torch.float32).cuda()
                    for _ in range(dist.get_world_size())
                    ]
                print(f"Before all_gather on rank {dist.get_rank()}: {tensor}")
                dist.all_gather(gather_list, tensor)
                print(f"After all_gather on rank {dist.get_rank()}: {gather_list}")
                
            init_process()
            example_all_gather()
        </d-code>

        <p>And indeed we can see that now each node has all the data:</p>

        <d-code block language="python">
            Before all_gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
            Before all_gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
            Before all_gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

            After all_gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                                         tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                                         tensor([3., 3., 3., 3., 3.], device='cuda:0')]
            After all_gather on rank 1: [tensor([1., 1., 1., 1., 1.], device='cuda:1'),
                                         tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                                         tensor([3., 3., 3., 3., 3.], device='cuda:0')]
            After all_gather on rank 2: [tensor([1., 1., 1., 1., 1.], device='cuda:2'),
                                         tensor([2., 2., 2., 2., 2.], device='cuda:2'),
                                         tensor([3., 3., 3., 3., 3.], device='cuda:2')]
        </d-code>

        <p>Now what about the inverse of a gather? In this case we would have all the data on one node and want to distribute/slice it among node, possibly with some intermediate processing? We can use the Scatter, or in the case of an operation in between a Reduce Scatter pattern:</p>

        <h4>Scatter & ReduceScatter</h4>
        
        <p>As the name subtly suggests, the goal of the Scatter operation is to take data on one node and distribute slices of it to all other nodes. It’s thus different from the Broadcast operation which copy data without slicing and it’s the logical the inverse of the Gather operation.</p>

        <p>The ReduceScatter pattern is slightly more complex: imagine you apply an operation like in the Reduce case but instead of moving the result to just one node we also distribute it evenly to all nodes:</p>

        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_scatter_reducescatter.png" style="width: 1000px" /></p>

        <p>The Scatter operation is written in code as the opposite of the Gather: instead of preparing a list of tensors as target we prepare the source data as a list of tensors we want to distribute. We also need to specify the <code>src</code>:</p>

        <d-code block language="python">
            def example_scatter():
                if dist.get_rank() == 0:
                    scatter_list = [
                        torch.tensor([i + 1] * 5, dtype=torch.float32).cuda()
                        for i in range(dist.get_world_size())
                        ]
                    print(f"Rank 0: Tensor to scatter: {scatter_list}")
                else:
                    scatter_list = None
                tensor = torch.zeros(5, dtype=torch.float32).cuda()
                print(f"Before scatter on rank {dist.get_rank()}: {tensor}")
                dist.scatter(tensor, scatter_list, src=0)
                print(f"After scatter on rank {dist.get_rank()}: {tensor}")
                
            init_process()
            example_scatter()
        </d-code>

        <p>As a result we can see how the empty tensors got filled with the contents of the <code>scatter_list</code></p>

        <d-code block language="python">
            Rank 0: Tensor to scatter: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                                        tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                                        tensor([3., 3., 3., 3., 3.], device='cuda:0')]
            Before scatter on rank 0: tensor([0., 0., 0., 0., 0.], device='cuda:0')
            Before scatter on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
            Before scatter on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

            After scatter on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
            After scatter on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
            After scatter on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
        </d-code>

        <p>Let’s create more interesting data to demonstrate the ReduceScatter logic: on each node we create a list of 2-elements vector on each node with a power exponent and an offset function of the node rank (it’s a bit hard to imagine so just look below for an example): </p>

        <d-code block language="python">
            def example_reduce_scatter():
                rank = dist.get_rank()
                world_size = dist.get_world_size()
                input_tensor = [
                    torch.tensor([(rank + 1) * i for i in range(1, 3)], dtype=torch.float32).cuda()**(j+1) 
                    for j in range(world_size)
                    ]
                output_tensor = torch.zeros(2, dtype=torch.float32).cuda()
                print(f"Before ReduceScatter on rank {rank}: {input_tensor}")
                dist.reduce_scatter(output_tensor, input_tensor, op=dist.ReduceOp.SUM)
                print(f"After ReduceScatter on rank {rank}: {output_tensor}")    
    
            init_process()
            example_reduce_scatter()
        </d-code>

        <p>Let’s print the pattern of data that we created. We also immediately see the ReduceScatter pattern: the first rank received the sum of the first tensor from each node, and the second rank contains the sum of the second tensor on each node and so on:</p>

        <d-code block language="python">
            Before ReduceScatter on rank 0: [tensor([1., 2.], device='cuda:0'),
											 tensor([1., 4.], device='cuda:0'),
											 tensor([1., 8.], device='cuda:0')]
            Before ReduceScatter on rank 1: [tensor([2., 4.], device='cuda:1'),
                                             tensor([ 4., 16.], device='cuda:1'),
                                             tensor([ 8., 64.], device='cuda:1')]
            Before ReduceScatter on rank 2: [tensor([3., 6.], device='cuda:2'),
                                             tensor([ 9., 36.], device='cuda:2'),
                                             tensor([ 27., 216.], device='cuda:2')]

            After ReduceScatter on rank 0: tensor([ 6., 12.], device='cuda:0')
            After ReduceScatter on rank 1: tensor([14., 56.], device='cuda:1')
            After ReduceScatter on rank 2: tensor([ 36., 288.], device='cuda:2')
        </d-code>


        <p>Let's have a quick look at a common implementation of AllReduce that uses ReduceScatter and AllGather: Ring AllReduce.</p>

        <h4>A quick focus on Ring AllReduce</h4>
        
        <p><strong><em>Ring AllReduce</em></strong> is one specific implementation of AllReduce, optimized for scalability. Rather than all devices communicating with each other directly, which could create communication bottlenecks, Ring All-Reduce can be broken down into two key steps: ReduceScatter and AllGather. Here's how it works:</p>

        <ol>
            <li><strong>ReduceScatter</strong></li>
            <ul>
                <li>Each device splits its data (e.g., gradients) into chunks and sends one chunk to its neighbour. Simultaneously, each device receives a chunk from its other neighbour.</li>
                <li>As each device receives a chunk, it adds (reduces) its corresponding chunk to the received one.</li>
                <li>This process continues around the ring until each device holds a partially reduced chunk, representing a sum of the gradients across all devices for that chunk.</li>
            </ul>
            <li><strong>AllGather</strong></li>
            <ul>
                <li>Now, each device needs to collect the fully reduced chunks from other devices.</li>
                <li>The devices start sending their reduced chunks to neighbours.</li>
                <li>Each device forwards the chunks it receives until every device has all the fully reduced chunks, giving each device the complete, summed-up gradient.</li>
            </ul>
        </ol>

        <p>Let’s illustrate this with the following gifs, where we have 5 GPUs, each with a tensor of length 5. The first animation shows the ReduceScatter step, where, at the end, each GPU receives the reduced results for a specific chunk of data (orange rectangle).</p>

        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_reduce_scatter.gif" style="width: 400px" /></p>

        <p>The next animation shows the AllGather step, where, at the end, each GPU obtains the full results of the AllReduce operation:</p>

        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_all_gather.gif" style="width: 400px" /></p>

        <p>You may have noticed that each of the <d-math>N</d-math> GPUs sends and receives values <d-math>N-1</d-math> times during both the reduce-scatter and all-gather steps. Each GPU sends <d-math>\frac{K}{N}</d-math> values per transfer, where <d-math>K</d-math> is the total number of values in the array being summed across the GPUs. Therefore, the total amount of data transferred to and from each GPU is  <d-math>2 \times (N-1) \times \frac{K}{N}</d-math>. When <d-math>N</d-math>  (the number of GPUs) is large, the total amount of data transferred to and from each GPU is approximately <d-math>2 \times K</d-math>, where <d-math>K</d-math> is the total number of parameters.</p>


        <p><strong>There are two key things to keep in mind for AllReduce:</strong></p>
        <ol>
            <li>The communication cost for AllReduce is approximately <d-math>2xK</d-math> when  <d-math>N</d-math> (the number of GPUs) is large.</li>
            <li>An AllReduce operation can be broken down into a reduce-scatter followed by an all-gather. The communication cost for these two operations is half that of the AllReduce, which is approximately <d-math>K</d-math>.</li>
        </ol>

        <p>As we can see this implementation can make efficient use of even a limited bandwidth between nodes.</p>

        <p>We now have seen the main building block of distributed operations but before we see them in action let’s have a look at a special operation used for synchronization: the Barrier.</p>
        
        <h4>Barrier</h4>
        
        <p>The Barrier is a simple operation to synchronize all nodes. A barrier is not lifted until all nodes have reached it. Then only are they allowed to continue with further computations:</p>

        <p style="text-align: center"><img alt="image.png" src="/assets/images/a0_barrier.png" style="width: 400px" /></p>

        <p>We can easily simulate delayed nodes by setting up a different sleep time on each node and see how long it takes for all of them to pass the barrier:</p>

        <d-code block language="python">
            def example_barrier():
                rank = dist.get_rank()
                t_start = time.time()
                print(f"Rank {rank} sleeps {rank} seconds.")
                time.sleep(rank)  # Simulate different processing times
                dist.barrier()
                print(f"Rank {rank} after barrier time delta: {time.time()-t_start:.4f}")
    
            init_process()
            example_barrier()
        </d-code>

        <p>We can see that although the first rank didn’t sleep at all it also took it 2sec to pass the barrier:</p>

        <d-code block language="python">
            Rank 0 sleeps 0 seconds.
            Rank 1 sleeps 1 seconds.
            Rank 2 sleeps 2 seconds.

            Rank 0 after barrier time delta: 2.0025
            Rank 1 after barrier time delta: 2.0025
            Rank 2 after barrier time delta: 2.0024
        </d-code>

        <p>We need to be careful with synchronizing all nodes like this, as this defeat the purpose of parallel independent operations and might thus slow down the whole processing. In many situations it can be just fine if a fast node already starts processing the next job as the fast node could be slower in a next iteration therefore evening out the delay over the whole process.</p>

        <p>Before turning to practical distributed training implementations, let’s first solve a mystery: what the heck is NCCL?</p>

        <h4>NCCL: NVIDIA Collective Communications Library</h4>

        <p>When training large models on many GPUs we may sometimes strike gold but we will always encounter nickel (or NCCL 🥁)! What’s is that?</p>

        <p>There are several libraries that implement collective communication and are support by PyTorch: there’s the classic <strong><em>MPI</em></strong> (Message Passing Interface), there’s <strong><em>Gloo</em></strong> by Meta, and finally there is `NCCL` (NVIDIA Collective Communications Library). They all provide similar functionality in terms of collective communication patterns but are optimized for different hardware setups; NCCL is designed to serve GPU-GPU communication efficiently while MPI and Gloo are setup for CPU-CPU or CPU-GPU communication. PyTorch provides a <a href="https://pytorch.org/docs/stable/distributed.html#which-backend-to-use">great guide</a> to decide which one to use:</p>

        <ul>
            <li>GPU training: use NCCL</li>
            <li>CPU training: use Gloo</li>
        </ul>

        <p>There are a few finer points in the decision tree that we leave to the reader to explore in the PyTorch guide referenced above.</p>

        <p>Now that we covered the fundamental operations for distributed training and when you should should be ready to follow the blog post easily.</p>
        
        <h3>A1: Distributed Training Profiling</h3>

        <h3>A2: Math for Compute/Comms Overlap</h3>

    
    </d-article>

    <d-appendix>
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <style>
            d-appendix .citation {
                font-size: 11px;
                line-height: 15px;
                border-left: 1px solid rgba(0, 0, 0, 0.1);
                padding-left: 18px;
                border: 1px solid rgba(0, 0, 0, 0.1);
                background: rgba(0, 0, 0, 0.02);
                padding: 10px 18px;
                border-radius: 3px;
                color: rgba(150, 150, 150, 1);
                overflow: hidden;
                margin-top: -12px;
                white-space: pre-wrap;
                word-wrap: break-word;
            }
        </style>

        <h3 id="citation">Citation</h3>
        <p>For attribution in academic contexts, please cite this work as</p>
        <pre
            class="citation short">XXX, et al., "The Ultra-Scale Playbook: Training LLMs on GPU Clusterse", 2025.</pre>
        <p>BibTeX citation</p>
        <pre class="citation long">@misc{TODO,
      title={The Ultra-Scale Playbook: Training LLMs on GPU Clusters},
      author={TODO},
      year={2025},
}</pre>
    </d-appendix>
    <script>
      function toggleTOC() {
          const content = document.querySelector('.toc-content');
          const icon = document.querySelector('.toggle-icon');
          
          content.classList.toggle('collapsed');
          icon.classList.toggle('collapsed');
      }
    </script>

    <script>
        const article = document.querySelector('d-article');
        const toc = document.querySelector('d-contents');
        if (toc) {
            const headings = article.querySelectorAll('h2, h3, h4');
            // let ToC = `<nav role="navigation" class="l-text figcaption"><h3>Table of contents</h3>`;
            let ToC = `<nav role="navigation" class="l-text figcaption"><div class="toc-header" onclick="toggleTOC()">
            <span class="toc-title">Table of Contents</span>
            <span class="toggle-icon">▼</span>
            </div><div class="toc-content">`;
            let prevLevel = 0;

            for (const el of headings) {
                // should element be included in TOC?
                const isInTitle = el.parentElement.tagName == 'D-TITLE';
                const isException = el.getAttribute('no-toc');
                if (isInTitle || isException) continue;
                el.setAttribute('id', el.textContent.toLowerCase().replaceAll(" ", "_"))
                const link = '<a target="_self" href="' + '#' + el.getAttribute('id') + '">' + el.textContent + '</a>';

                const level = el.tagName === 'H2' ? 0 : (el.tagName === 'H3' ? 1 : 2);
                while (prevLevel < level) {
                    ToC += '<ul>'
                    prevLevel++;
                }
                while (prevLevel > level) {
                    ToC += '</ul>'
                    prevLevel--;
                }
                if (level === 0)
                    ToC += '<div>' + link + '</div>';
                else if (level === 1)
                    ToC += '<li>' + link + '</li>';
            }

            while (prevLevel > 0) {
                ToC += '</ul>'
                prevLevel--;
            }
            ToC += '</div></nav>';
            toc.innerHTML = ToC;
            toc.setAttribute('prerendered', 'true');
            const toc_links = document.querySelectorAll('d-contents > nav div a');

            window.addEventListener('scroll', (_event) => {
                if (typeof (headings) != 'undefined' && headings != null && typeof (toc_links) != 'undefined' && toc_links != null) {
                    find_active: {
                        for (let i = headings.length - 1; i >= 0; i--) {
                            const heading = headings[i];
                            // Skip headings that shouldn't be in TOC
                            if (heading.parentElement.tagName == 'D-TITLE' || heading.getAttribute('no-toc')) {
                                continue;
                            }
                            
                            if (heading.getBoundingClientRect().top - 50 <= 0) {
                                // Find matching TOC link by href
                                const headingId = heading.getAttribute('id');
                                const activeLink = Array.from(toc_links).find(link => 
                                    link.getAttribute('href') === '#' + headingId
                                );
                                
                                if (activeLink && !activeLink.classList.contains("active")) {
                                    toc_links.forEach(link => link.classList.remove("active"));
                                    activeLink.classList.add('active');
                                }
                                break find_active;
                            }
                        }
                        toc_links.forEach(link => link.classList.remove("active"));
                    }
                }
            });
        }
    </script>

</body>

</html>
